{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.io as pio\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 데이터가 많아 plotly로 시각화시 인터렉션 활성화시 팅김 주의!!!! 특히 test데이터는 더더욱\n",
    "config = {\n",
    "    'staticPlot': True,  # 인터랙션 비활성화 (호버, 확대/축소 등)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"C:/Users/lim/Desktop/upstage AI/AD_project/data\")\n",
    "train_data = pd.read_csv(DATA_PATH / \"train.csv\")\n",
    "test_data = pd.read_csv(DATA_PATH / \"test.csv\")\n",
    "def process_data(df) -> pd.DataFrame:\n",
    "    numeric_cols = [\n",
    "        'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6',\n",
    "        'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12',\n",
    "        'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18',\n",
    "        'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24',\n",
    "        'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30',\n",
    "        'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36',\n",
    "        'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1',\n",
    "        'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8',\n",
    "        'xmv_9', 'xmv_10', 'xmv_11','simulationRun'\n",
    "    ]\n",
    "    return df[numeric_cols]\n",
    "train_a = process_data(train_data)\n",
    "test_a = process_data(test_data)\n",
    "\n",
    "df_cleaned = train_a.drop(columns=['simulationRun'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1316</th>\n",
       "      <th>1317</th>\n",
       "      <th>1318</th>\n",
       "      <th>1319</th>\n",
       "      <th>1320</th>\n",
       "      <th>1321</th>\n",
       "      <th>1322</th>\n",
       "      <th>1323</th>\n",
       "      <th>1324</th>\n",
       "      <th>1325</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020591</td>\n",
       "      <td>-0.260661</td>\n",
       "      <td>-0.176951</td>\n",
       "      <td>0.030020</td>\n",
       "      <td>0.032671</td>\n",
       "      <td>-0.236612</td>\n",
       "      <td>-0.032780</td>\n",
       "      <td>0.134138</td>\n",
       "      <td>-0.028196</td>\n",
       "      <td>0.160339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003579</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>0.032646</td>\n",
       "      <td>-0.048934</td>\n",
       "      <td>-0.032728</td>\n",
       "      <td>0.073266</td>\n",
       "      <td>0.031077</td>\n",
       "      <td>-0.041114</td>\n",
       "      <td>0.044858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078139</td>\n",
       "      <td>-0.278459</td>\n",
       "      <td>-0.200471</td>\n",
       "      <td>0.054274</td>\n",
       "      <td>0.077768</td>\n",
       "      <td>-0.462644</td>\n",
       "      <td>-0.018672</td>\n",
       "      <td>0.136028</td>\n",
       "      <td>0.126637</td>\n",
       "      <td>0.414842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>-0.032789</td>\n",
       "      <td>-0.004751</td>\n",
       "      <td>-0.004982</td>\n",
       "      <td>-0.055407</td>\n",
       "      <td>-0.057435</td>\n",
       "      <td>-0.029346</td>\n",
       "      <td>0.144913</td>\n",
       "      <td>-0.030144</td>\n",
       "      <td>0.032246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.100890</td>\n",
       "      <td>-0.170232</td>\n",
       "      <td>-0.154391</td>\n",
       "      <td>0.012444</td>\n",
       "      <td>0.094113</td>\n",
       "      <td>-0.214063</td>\n",
       "      <td>-0.037470</td>\n",
       "      <td>0.108339</td>\n",
       "      <td>0.109032</td>\n",
       "      <td>0.191369</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008171</td>\n",
       "      <td>-0.011791</td>\n",
       "      <td>-0.046247</td>\n",
       "      <td>0.042561</td>\n",
       "      <td>0.029119</td>\n",
       "      <td>-0.031643</td>\n",
       "      <td>-0.027008</td>\n",
       "      <td>0.118832</td>\n",
       "      <td>-0.029250</td>\n",
       "      <td>0.011395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.040025</td>\n",
       "      <td>-0.172660</td>\n",
       "      <td>-0.117603</td>\n",
       "      <td>0.114720</td>\n",
       "      <td>0.096880</td>\n",
       "      <td>-0.181544</td>\n",
       "      <td>-0.079530</td>\n",
       "      <td>0.130010</td>\n",
       "      <td>-0.112081</td>\n",
       "      <td>0.245221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.031534</td>\n",
       "      <td>0.090385</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.097693</td>\n",
       "      <td>-0.018807</td>\n",
       "      <td>0.079889</td>\n",
       "      <td>-0.043916</td>\n",
       "      <td>-0.023085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.045265</td>\n",
       "      <td>-0.153692</td>\n",
       "      <td>-0.090107</td>\n",
       "      <td>-0.047188</td>\n",
       "      <td>0.109109</td>\n",
       "      <td>-0.241844</td>\n",
       "      <td>-0.058974</td>\n",
       "      <td>0.178490</td>\n",
       "      <td>0.027738</td>\n",
       "      <td>0.278298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.037174</td>\n",
       "      <td>-0.062884</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>-0.049896</td>\n",
       "      <td>-0.061816</td>\n",
       "      <td>0.012691</td>\n",
       "      <td>0.121101</td>\n",
       "      <td>-0.021045</td>\n",
       "      <td>0.005566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-0.047545</td>\n",
       "      <td>-0.238838</td>\n",
       "      <td>-0.093243</td>\n",
       "      <td>-0.006759</td>\n",
       "      <td>0.105480</td>\n",
       "      <td>-0.141812</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.123374</td>\n",
       "      <td>0.054046</td>\n",
       "      <td>0.163194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.019705</td>\n",
       "      <td>0.021680</td>\n",
       "      <td>0.012125</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>-0.053741</td>\n",
       "      <td>-0.052038</td>\n",
       "      <td>0.092733</td>\n",
       "      <td>-0.004530</td>\n",
       "      <td>-0.036385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.099218</td>\n",
       "      <td>-0.027499</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.188598</td>\n",
       "      <td>0.031447</td>\n",
       "      <td>0.031058</td>\n",
       "      <td>0.106059</td>\n",
       "      <td>0.120738</td>\n",
       "      <td>0.074067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037796</td>\n",
       "      <td>0.038915</td>\n",
       "      <td>-0.081774</td>\n",
       "      <td>-0.003518</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.013241</td>\n",
       "      <td>-0.070348</td>\n",
       "      <td>0.059713</td>\n",
       "      <td>-0.006511</td>\n",
       "      <td>0.034448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.044097</td>\n",
       "      <td>-0.041517</td>\n",
       "      <td>-0.084305</td>\n",
       "      <td>-0.072219</td>\n",
       "      <td>0.119722</td>\n",
       "      <td>-0.067360</td>\n",
       "      <td>0.143825</td>\n",
       "      <td>0.162899</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.126713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104324</td>\n",
       "      <td>-0.064259</td>\n",
       "      <td>-0.074763</td>\n",
       "      <td>0.034755</td>\n",
       "      <td>-0.056325</td>\n",
       "      <td>-0.099593</td>\n",
       "      <td>-0.080487</td>\n",
       "      <td>0.064682</td>\n",
       "      <td>-0.046970</td>\n",
       "      <td>-0.024776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-0.179437</td>\n",
       "      <td>-0.033423</td>\n",
       "      <td>-0.055022</td>\n",
       "      <td>0.021946</td>\n",
       "      <td>0.092321</td>\n",
       "      <td>-0.091678</td>\n",
       "      <td>-0.064811</td>\n",
       "      <td>0.043981</td>\n",
       "      <td>-0.039920</td>\n",
       "      <td>0.104806</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003084</td>\n",
       "      <td>0.037942</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.056788</td>\n",
       "      <td>-0.035899</td>\n",
       "      <td>0.017817</td>\n",
       "      <td>-0.039523</td>\n",
       "      <td>0.130114</td>\n",
       "      <td>-0.051412</td>\n",
       "      <td>0.026306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.000588</td>\n",
       "      <td>-0.189917</td>\n",
       "      <td>-0.046273</td>\n",
       "      <td>0.012764</td>\n",
       "      <td>0.087047</td>\n",
       "      <td>-0.202811</td>\n",
       "      <td>0.017435</td>\n",
       "      <td>0.085272</td>\n",
       "      <td>0.036885</td>\n",
       "      <td>0.209933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010812</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.015720</td>\n",
       "      <td>-0.012597</td>\n",
       "      <td>0.039930</td>\n",
       "      <td>0.047449</td>\n",
       "      <td>0.088248</td>\n",
       "      <td>0.018051</td>\n",
       "      <td>0.029481</td>\n",
       "      <td>-0.009792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1326 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.020591 -0.260661 -0.176951  0.030020  0.032671 -0.236612 -0.032780   \n",
       "1    0.078139 -0.278459 -0.200471  0.054274  0.077768 -0.462644 -0.018672   \n",
       "2   -0.100890 -0.170232 -0.154391  0.012444  0.094113 -0.214063 -0.037470   \n",
       "3   -0.040025 -0.172660 -0.117603  0.114720  0.096880 -0.181544 -0.079530   \n",
       "4   -0.045265 -0.153692 -0.090107 -0.047188  0.109109 -0.241844 -0.058974   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "495 -0.047545 -0.238838 -0.093243 -0.006759  0.105480 -0.141812  0.000510   \n",
       "496  0.099218 -0.027499  0.014118  0.019531  0.188598  0.031447  0.031058   \n",
       "497  0.044097 -0.041517 -0.084305 -0.072219  0.119722 -0.067360  0.143825   \n",
       "498 -0.179437 -0.033423 -0.055022  0.021946  0.092321 -0.091678 -0.064811   \n",
       "499  0.000588 -0.189917 -0.046273  0.012764  0.087047 -0.202811  0.017435   \n",
       "\n",
       "         7         8         9     ...      1316      1317      1318  \\\n",
       "0    0.134138 -0.028196  0.160339  ...  0.003579  0.003319  0.010667   \n",
       "1    0.136028  0.126637  0.414842  ...  0.069824 -0.032789 -0.004751   \n",
       "2    0.108339  0.109032  0.191369  ... -0.008171 -0.011791 -0.046247   \n",
       "3    0.130010 -0.112081  0.245221  ...  0.045537  0.002909  0.031534   \n",
       "4    0.178490  0.027738  0.278298  ...  0.003031 -0.037174 -0.062884   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "495  0.123374  0.054046  0.163194  ...  0.007365  0.019705  0.021680   \n",
       "496  0.106059  0.120738  0.074067  ... -0.037796  0.038915 -0.081774   \n",
       "497  0.162899  0.010877  0.126713  ...  0.104324 -0.064259 -0.074763   \n",
       "498  0.043981 -0.039920  0.104806  ... -0.003084  0.037942  0.000393   \n",
       "499  0.085272  0.036885  0.209933  ...  0.010812  0.001425  0.015720   \n",
       "\n",
       "         1319      1320      1321      1322      1323      1324      1325  \n",
       "0    0.032646 -0.048934 -0.032728  0.073266  0.031077 -0.041114  0.044858  \n",
       "1   -0.004982 -0.055407 -0.057435 -0.029346  0.144913 -0.030144  0.032246  \n",
       "2    0.042561  0.029119 -0.031643 -0.027008  0.118832 -0.029250  0.011395  \n",
       "3    0.090385  0.000218  0.097693 -0.018807  0.079889 -0.043916 -0.023085  \n",
       "4    0.005423 -0.049896 -0.061816  0.012691  0.121101 -0.021045  0.005566  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "495  0.012125  0.016923 -0.053741 -0.052038  0.092733 -0.004530 -0.036385  \n",
       "496 -0.003518  0.003784  0.013241 -0.070348  0.059713 -0.006511  0.034448  \n",
       "497  0.034755 -0.056325 -0.099593 -0.080487  0.064682 -0.046970 -0.024776  \n",
       "498  0.056788 -0.035899  0.017817 -0.039523  0.130114 -0.051412  0.026306  \n",
       "499 -0.012597  0.039930  0.047449  0.088248  0.018051  0.029481 -0.009792  \n",
       "\n",
       "[500 rows x 1326 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_groups_train = train_a.groupby('simulationRun')\n",
    "\n",
    "# 최종적으로 상관관계 결과를 저장할 빈 데이터프레임\n",
    "input_vector_train = pd.DataFrame()\n",
    "\n",
    "# 시뮬레이션 별로 상관관계 계산\n",
    "for sim_id, group in simulation_groups_train:\n",
    "    # 시뮬레이션 번호 컬럼은 제거 (상관관계 계산에는 필요 없음)\n",
    "    group_cleaned = group.drop(columns=['simulationRun'])\n",
    "    \n",
    "    # 상관관계 행렬 계산\n",
    "    sim_corr = group_cleaned.corr()\n",
    "    \n",
    "    # 상삼각 행렬(upper triangle)만 추출 (대각선 위의 값들)\n",
    "    upper_triangle = sim_corr.where(np.triu(np.ones(sim_corr.shape), k=1).astype(bool))\n",
    "    \n",
    "    # 상삼각 행렬을 1차원으로 펼쳐서 데이터프레임으로 변환\n",
    "    # stack()으로 상삼각 행렬에서 값만 추출한 후 reset_index로 인덱스를 리셋\n",
    "    correlation_df = upper_triangle.stack().reset_index().rename(columns={0: \"correlation_value\"})\n",
    "    \n",
    "    # 상관계수 값만 추출\n",
    "    correlation_values = correlation_df['correlation_value']\n",
    "    \n",
    "    # 상관계수 값을 1행으로 변환 (전치 작업)\n",
    "    df_transposed = pd.DataFrame(correlation_values.values).T\n",
    "    \n",
    "    # 해당 시뮬레이션 번호를 새로운 컬럼에 추가\n",
    "    df_transposed['simulationRun'] = sim_id\n",
    "    \n",
    "    # 최종 데이터프레임에 시뮬레이션 결과를 이어붙이기 (concat)\n",
    "    input_vector_train = pd.concat([input_vector_train, df_transposed], ignore_index=True)\n",
    "\n",
    "# 최종 결과 출력\n",
    "input_vector_train.drop(axis=1,columns='simulationRun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1316</th>\n",
       "      <th>1317</th>\n",
       "      <th>1318</th>\n",
       "      <th>1319</th>\n",
       "      <th>1320</th>\n",
       "      <th>1321</th>\n",
       "      <th>1322</th>\n",
       "      <th>1323</th>\n",
       "      <th>1324</th>\n",
       "      <th>1325</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.051253</td>\n",
       "      <td>-0.173687</td>\n",
       "      <td>-0.146285</td>\n",
       "      <td>0.007504</td>\n",
       "      <td>0.060541</td>\n",
       "      <td>-0.242854</td>\n",
       "      <td>-0.015613</td>\n",
       "      <td>0.099555</td>\n",
       "      <td>0.065974</td>\n",
       "      <td>0.211078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003934</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>-0.016742</td>\n",
       "      <td>0.008616</td>\n",
       "      <td>-0.015082</td>\n",
       "      <td>-0.014496</td>\n",
       "      <td>-0.022616</td>\n",
       "      <td>0.102121</td>\n",
       "      <td>-0.019225</td>\n",
       "      <td>0.004588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1326 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0 -0.051253 -0.173687 -0.146285  0.007504  0.060541 -0.242854 -0.015613   \n",
       "\n",
       "       7         8         9     ...      1316      1317      1318      1319  \\\n",
       "0  0.099555  0.065974  0.211078  ... -0.003934 -0.000861 -0.016742  0.008616   \n",
       "\n",
       "       1320      1321      1322      1323      1324      1325  \n",
       "0 -0.015082 -0.014496 -0.022616  0.102121 -0.019225  0.004588  \n",
       "\n",
       "[1 rows x 1326 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cleaned = train_a.drop(columns=['simulationRun'])\n",
    "sim_corr = group_cleaned.corr()\n",
    "    \n",
    "# 상삼각 행렬(upper triangle)만 추출 (대각선 위의 값들)\n",
    "upper_triangle = sim_corr.where(np.triu(np.ones(sim_corr.shape), k=1).astype(bool))\n",
    "\n",
    "# 상삼각 행렬을 1차원으로 펼쳐서 데이터프레임으로 변환\n",
    "# stack()으로 상삼각 행렬에서 값만 추출한 후 reset_index로 인덱스를 리셋\n",
    "correlation_df = upper_triangle.stack().reset_index().rename(columns={0: \"correlation_value\"})\n",
    "\n",
    "# 상관계수 값만 추출\n",
    "correlation_values = correlation_df['correlation_value']\n",
    "\n",
    "# 상관계수 값을 1행으로 변환 (전치 작업)\n",
    "df_transposed = pd.DataFrame(correlation_values.values).T\n",
    "    \n",
    "df_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1316</th>\n",
       "      <th>1317</th>\n",
       "      <th>1318</th>\n",
       "      <th>1319</th>\n",
       "      <th>1320</th>\n",
       "      <th>1321</th>\n",
       "      <th>1322</th>\n",
       "      <th>1323</th>\n",
       "      <th>1324</th>\n",
       "      <th>1325</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020591</td>\n",
       "      <td>-0.260661</td>\n",
       "      <td>-0.176951</td>\n",
       "      <td>0.030020</td>\n",
       "      <td>0.032671</td>\n",
       "      <td>-0.236612</td>\n",
       "      <td>-0.032780</td>\n",
       "      <td>0.134138</td>\n",
       "      <td>-0.028196</td>\n",
       "      <td>0.160339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003579</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>0.032646</td>\n",
       "      <td>-0.048934</td>\n",
       "      <td>-0.032728</td>\n",
       "      <td>0.073266</td>\n",
       "      <td>0.031077</td>\n",
       "      <td>-0.041114</td>\n",
       "      <td>0.044858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078139</td>\n",
       "      <td>-0.278459</td>\n",
       "      <td>-0.200471</td>\n",
       "      <td>0.054274</td>\n",
       "      <td>0.077768</td>\n",
       "      <td>-0.462644</td>\n",
       "      <td>-0.018672</td>\n",
       "      <td>0.136028</td>\n",
       "      <td>0.126637</td>\n",
       "      <td>0.414842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>-0.032789</td>\n",
       "      <td>-0.004751</td>\n",
       "      <td>-0.004982</td>\n",
       "      <td>-0.055407</td>\n",
       "      <td>-0.057435</td>\n",
       "      <td>-0.029346</td>\n",
       "      <td>0.144913</td>\n",
       "      <td>-0.030144</td>\n",
       "      <td>0.032246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.100890</td>\n",
       "      <td>-0.170232</td>\n",
       "      <td>-0.154391</td>\n",
       "      <td>0.012444</td>\n",
       "      <td>0.094113</td>\n",
       "      <td>-0.214063</td>\n",
       "      <td>-0.037470</td>\n",
       "      <td>0.108339</td>\n",
       "      <td>0.109032</td>\n",
       "      <td>0.191369</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008171</td>\n",
       "      <td>-0.011791</td>\n",
       "      <td>-0.046247</td>\n",
       "      <td>0.042561</td>\n",
       "      <td>0.029119</td>\n",
       "      <td>-0.031643</td>\n",
       "      <td>-0.027008</td>\n",
       "      <td>0.118832</td>\n",
       "      <td>-0.029250</td>\n",
       "      <td>0.011395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.040025</td>\n",
       "      <td>-0.172660</td>\n",
       "      <td>-0.117603</td>\n",
       "      <td>0.114720</td>\n",
       "      <td>0.096880</td>\n",
       "      <td>-0.181544</td>\n",
       "      <td>-0.079530</td>\n",
       "      <td>0.130010</td>\n",
       "      <td>-0.112081</td>\n",
       "      <td>0.245221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.031534</td>\n",
       "      <td>0.090385</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.097693</td>\n",
       "      <td>-0.018807</td>\n",
       "      <td>0.079889</td>\n",
       "      <td>-0.043916</td>\n",
       "      <td>-0.023085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.045265</td>\n",
       "      <td>-0.153692</td>\n",
       "      <td>-0.090107</td>\n",
       "      <td>-0.047188</td>\n",
       "      <td>0.109109</td>\n",
       "      <td>-0.241844</td>\n",
       "      <td>-0.058974</td>\n",
       "      <td>0.178490</td>\n",
       "      <td>0.027738</td>\n",
       "      <td>0.278298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.037174</td>\n",
       "      <td>-0.062884</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>-0.049896</td>\n",
       "      <td>-0.061816</td>\n",
       "      <td>0.012691</td>\n",
       "      <td>0.121101</td>\n",
       "      <td>-0.021045</td>\n",
       "      <td>0.005566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-0.047545</td>\n",
       "      <td>-0.238838</td>\n",
       "      <td>-0.093243</td>\n",
       "      <td>-0.006759</td>\n",
       "      <td>0.105480</td>\n",
       "      <td>-0.141812</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.123374</td>\n",
       "      <td>0.054046</td>\n",
       "      <td>0.163194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.019705</td>\n",
       "      <td>0.021680</td>\n",
       "      <td>0.012125</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>-0.053741</td>\n",
       "      <td>-0.052038</td>\n",
       "      <td>0.092733</td>\n",
       "      <td>-0.004530</td>\n",
       "      <td>-0.036385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.099218</td>\n",
       "      <td>-0.027499</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.188598</td>\n",
       "      <td>0.031447</td>\n",
       "      <td>0.031058</td>\n",
       "      <td>0.106059</td>\n",
       "      <td>0.120738</td>\n",
       "      <td>0.074067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037796</td>\n",
       "      <td>0.038915</td>\n",
       "      <td>-0.081774</td>\n",
       "      <td>-0.003518</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.013241</td>\n",
       "      <td>-0.070348</td>\n",
       "      <td>0.059713</td>\n",
       "      <td>-0.006511</td>\n",
       "      <td>0.034448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.044097</td>\n",
       "      <td>-0.041517</td>\n",
       "      <td>-0.084305</td>\n",
       "      <td>-0.072219</td>\n",
       "      <td>0.119722</td>\n",
       "      <td>-0.067360</td>\n",
       "      <td>0.143825</td>\n",
       "      <td>0.162899</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.126713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104324</td>\n",
       "      <td>-0.064259</td>\n",
       "      <td>-0.074763</td>\n",
       "      <td>0.034755</td>\n",
       "      <td>-0.056325</td>\n",
       "      <td>-0.099593</td>\n",
       "      <td>-0.080487</td>\n",
       "      <td>0.064682</td>\n",
       "      <td>-0.046970</td>\n",
       "      <td>-0.024776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-0.179437</td>\n",
       "      <td>-0.033423</td>\n",
       "      <td>-0.055022</td>\n",
       "      <td>0.021946</td>\n",
       "      <td>0.092321</td>\n",
       "      <td>-0.091678</td>\n",
       "      <td>-0.064811</td>\n",
       "      <td>0.043981</td>\n",
       "      <td>-0.039920</td>\n",
       "      <td>0.104806</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003084</td>\n",
       "      <td>0.037942</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.056788</td>\n",
       "      <td>-0.035899</td>\n",
       "      <td>0.017817</td>\n",
       "      <td>-0.039523</td>\n",
       "      <td>0.130114</td>\n",
       "      <td>-0.051412</td>\n",
       "      <td>0.026306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.000588</td>\n",
       "      <td>-0.189917</td>\n",
       "      <td>-0.046273</td>\n",
       "      <td>0.012764</td>\n",
       "      <td>0.087047</td>\n",
       "      <td>-0.202811</td>\n",
       "      <td>0.017435</td>\n",
       "      <td>0.085272</td>\n",
       "      <td>0.036885</td>\n",
       "      <td>0.209933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010812</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.015720</td>\n",
       "      <td>-0.012597</td>\n",
       "      <td>0.039930</td>\n",
       "      <td>0.047449</td>\n",
       "      <td>0.088248</td>\n",
       "      <td>0.018051</td>\n",
       "      <td>0.029481</td>\n",
       "      <td>-0.009792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1326 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.020591 -0.260661 -0.176951  0.030020  0.032671 -0.236612 -0.032780   \n",
       "1    0.078139 -0.278459 -0.200471  0.054274  0.077768 -0.462644 -0.018672   \n",
       "2   -0.100890 -0.170232 -0.154391  0.012444  0.094113 -0.214063 -0.037470   \n",
       "3   -0.040025 -0.172660 -0.117603  0.114720  0.096880 -0.181544 -0.079530   \n",
       "4   -0.045265 -0.153692 -0.090107 -0.047188  0.109109 -0.241844 -0.058974   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "495 -0.047545 -0.238838 -0.093243 -0.006759  0.105480 -0.141812  0.000510   \n",
       "496  0.099218 -0.027499  0.014118  0.019531  0.188598  0.031447  0.031058   \n",
       "497  0.044097 -0.041517 -0.084305 -0.072219  0.119722 -0.067360  0.143825   \n",
       "498 -0.179437 -0.033423 -0.055022  0.021946  0.092321 -0.091678 -0.064811   \n",
       "499  0.000588 -0.189917 -0.046273  0.012764  0.087047 -0.202811  0.017435   \n",
       "\n",
       "         7         8         9     ...      1316      1317      1318  \\\n",
       "0    0.134138 -0.028196  0.160339  ...  0.003579  0.003319  0.010667   \n",
       "1    0.136028  0.126637  0.414842  ...  0.069824 -0.032789 -0.004751   \n",
       "2    0.108339  0.109032  0.191369  ... -0.008171 -0.011791 -0.046247   \n",
       "3    0.130010 -0.112081  0.245221  ...  0.045537  0.002909  0.031534   \n",
       "4    0.178490  0.027738  0.278298  ...  0.003031 -0.037174 -0.062884   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "495  0.123374  0.054046  0.163194  ...  0.007365  0.019705  0.021680   \n",
       "496  0.106059  0.120738  0.074067  ... -0.037796  0.038915 -0.081774   \n",
       "497  0.162899  0.010877  0.126713  ...  0.104324 -0.064259 -0.074763   \n",
       "498  0.043981 -0.039920  0.104806  ... -0.003084  0.037942  0.000393   \n",
       "499  0.085272  0.036885  0.209933  ...  0.010812  0.001425  0.015720   \n",
       "\n",
       "         1319      1320      1321      1322      1323      1324      1325  \n",
       "0    0.032646 -0.048934 -0.032728  0.073266  0.031077 -0.041114  0.044858  \n",
       "1   -0.004982 -0.055407 -0.057435 -0.029346  0.144913 -0.030144  0.032246  \n",
       "2    0.042561  0.029119 -0.031643 -0.027008  0.118832 -0.029250  0.011395  \n",
       "3    0.090385  0.000218  0.097693 -0.018807  0.079889 -0.043916 -0.023085  \n",
       "4    0.005423 -0.049896 -0.061816  0.012691  0.121101 -0.021045  0.005566  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "495  0.012125  0.016923 -0.053741 -0.052038  0.092733 -0.004530 -0.036385  \n",
       "496 -0.003518  0.003784  0.013241 -0.070348  0.059713 -0.006511  0.034448  \n",
       "497  0.034755 -0.056325 -0.099593 -0.080487  0.064682 -0.046970 -0.024776  \n",
       "498  0.056788 -0.035899  0.017817 -0.039523  0.130114 -0.051412  0.026306  \n",
       "499 -0.012597  0.039930  0.047449  0.088248  0.018051  0.029481 -0.009792  \n",
       "\n",
       "[500 rows x 1326 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector_train = input_vector_train.drop(axis=1,columns='simulationRun')\n",
    "input_vector_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6  \\\n",
      "0   -0.083705 -0.146291 -0.147316  0.043904  0.100317 -0.229497 -0.015868   \n",
      "1   -0.030525 -0.098300 -0.070935  0.015439  0.074234 -0.082444  0.008479   \n",
      "2   -0.061090 -0.183311 -0.206996 -0.021754 -0.025341 -0.317265 -0.027436   \n",
      "3   -0.094754 -0.129334 -0.157846  0.098175  0.048454 -0.238174  0.023942   \n",
      "4   -0.029432 -0.099604 -0.133493  0.064296  0.107419 -0.155940  0.035465   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "735 -0.019815 -0.105710 -0.168113 -0.028623  0.012427 -0.227943 -0.025613   \n",
      "736 -0.064367 -0.150520 -0.101398  0.027810  0.060473 -0.166544 -0.007591   \n",
      "737  0.052814  0.212196  0.190459  0.053032  0.244777 -0.192522 -0.019539   \n",
      "738 -0.570429 -0.687159 -0.420220  0.041096 -0.134033 -0.615365 -0.189209   \n",
      "739 -0.031507 -0.167382 -0.241901 -0.005488  0.004184 -0.290302  0.034723   \n",
      "\n",
      "            7         8         9  ...      1317      1318      1319  \\\n",
      "0    0.104999  0.135875  0.255291  ... -0.036390 -0.074461 -0.001122   \n",
      "1    0.094404  0.057404  0.083904  ...  0.006979 -0.027061  0.034229   \n",
      "2    0.104302  0.105783  0.210912  ... -0.052163 -0.036866 -0.029417   \n",
      "3    0.095521 -0.035370  0.193542  ...  0.029152 -0.030187 -0.008234   \n",
      "4    0.110237  0.051370  0.098302  ... -0.006853  0.016900  0.039125   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "735  0.090517  0.047136  0.155466  ... -0.017124 -0.012875  0.056727   \n",
      "736  0.119012  0.037587  0.153394  ...  0.020799  0.003292  0.009433   \n",
      "737  0.075527  0.381025  0.356624  ...  0.072279 -0.081167  0.020842   \n",
      "738  0.389638  0.214121  0.555281  ...  0.087307 -0.117938  0.109074   \n",
      "739  0.122054  0.071275  0.233685  ...  0.026083 -0.061186  0.032631   \n",
      "\n",
      "         1320      1321      1322      1323      1324      1325  simulationRun  \n",
      "0   -0.001292  0.016187 -0.011120  0.119724 -0.015386  0.020772              0  \n",
      "1   -0.018317 -0.003150 -0.024807  0.079372  0.024705  0.063742              1  \n",
      "2   -0.032821 -0.021177 -0.051575  0.144394 -0.029293 -0.004971              2  \n",
      "3   -0.013529 -0.086892 -0.033384  0.095301 -0.056766  0.013498              3  \n",
      "4    0.037337  0.018794 -0.065423  0.049548 -0.019722  0.063039              4  \n",
      "..        ...       ...       ...       ...       ...       ...            ...  \n",
      "735 -0.040235 -0.037193 -0.003857  0.087729  0.007210  0.058281            735  \n",
      "736  0.052629 -0.021794 -0.025866  0.039963  0.011522  0.059293            736  \n",
      "737  0.051065 -0.031010 -0.041723 -0.222852  0.305756 -0.086334            737  \n",
      "738 -0.030487 -0.117519 -0.015182  0.233117  0.208013 -0.201244            738  \n",
      "739 -0.014527  0.058131 -0.033069  0.139873 -0.025251 -0.011274            739  \n",
      "\n",
      "[740 rows x 1327 columns]\n"
     ]
    }
   ],
   "source": [
    "simulation_groups_test = test_a.groupby('simulationRun')\n",
    "\n",
    "# 최종적으로 상관관계 결과를 저장할 빈 데이터프레임\n",
    "input_vector_test = pd.DataFrame()\n",
    "\n",
    "# 시뮬레이션 별로 상관관계 계산\n",
    "for sim_id, group in simulation_groups_test:\n",
    "    group_cleaned = group.drop(columns=['simulationRun'])\n",
    "    sim_corr = group_cleaned.corr()\n",
    "    upper_triangle = sim_corr.where(np.triu(np.ones(sim_corr.shape), k=1).astype(bool))\n",
    "    correlation_df = upper_triangle.stack().reset_index().rename(columns={0: \"correlation_value\"})\n",
    "    correlation_values = correlation_df['correlation_value']\n",
    "    df_transposed = pd.DataFrame(correlation_values.values).T\n",
    "    df_transposed['simulationRun'] = sim_id\n",
    "    input_vector_test = pd.concat([input_vector_test, df_transposed], ignore_index=True)\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(input_vector_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1316</th>\n",
       "      <th>1317</th>\n",
       "      <th>1318</th>\n",
       "      <th>1319</th>\n",
       "      <th>1320</th>\n",
       "      <th>1321</th>\n",
       "      <th>1322</th>\n",
       "      <th>1323</th>\n",
       "      <th>1324</th>\n",
       "      <th>1325</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.083705</td>\n",
       "      <td>-0.146291</td>\n",
       "      <td>-0.147316</td>\n",
       "      <td>0.043904</td>\n",
       "      <td>0.100317</td>\n",
       "      <td>-0.229497</td>\n",
       "      <td>-0.015868</td>\n",
       "      <td>0.104999</td>\n",
       "      <td>0.135875</td>\n",
       "      <td>0.255291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013692</td>\n",
       "      <td>-0.036390</td>\n",
       "      <td>-0.074461</td>\n",
       "      <td>-0.001122</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>0.016187</td>\n",
       "      <td>-0.011120</td>\n",
       "      <td>0.119724</td>\n",
       "      <td>-0.015386</td>\n",
       "      <td>0.020772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.030525</td>\n",
       "      <td>-0.098300</td>\n",
       "      <td>-0.070935</td>\n",
       "      <td>0.015439</td>\n",
       "      <td>0.074234</td>\n",
       "      <td>-0.082444</td>\n",
       "      <td>0.008479</td>\n",
       "      <td>0.094404</td>\n",
       "      <td>0.057404</td>\n",
       "      <td>0.083904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028225</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>-0.027061</td>\n",
       "      <td>0.034229</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>-0.003150</td>\n",
       "      <td>-0.024807</td>\n",
       "      <td>0.079372</td>\n",
       "      <td>0.024705</td>\n",
       "      <td>0.063742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.061090</td>\n",
       "      <td>-0.183311</td>\n",
       "      <td>-0.206996</td>\n",
       "      <td>-0.021754</td>\n",
       "      <td>-0.025341</td>\n",
       "      <td>-0.317265</td>\n",
       "      <td>-0.027436</td>\n",
       "      <td>0.104302</td>\n",
       "      <td>0.105783</td>\n",
       "      <td>0.210912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022292</td>\n",
       "      <td>-0.052163</td>\n",
       "      <td>-0.036866</td>\n",
       "      <td>-0.029417</td>\n",
       "      <td>-0.032821</td>\n",
       "      <td>-0.021177</td>\n",
       "      <td>-0.051575</td>\n",
       "      <td>0.144394</td>\n",
       "      <td>-0.029293</td>\n",
       "      <td>-0.004971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.094754</td>\n",
       "      <td>-0.129334</td>\n",
       "      <td>-0.157846</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>0.048454</td>\n",
       "      <td>-0.238174</td>\n",
       "      <td>0.023942</td>\n",
       "      <td>0.095521</td>\n",
       "      <td>-0.035370</td>\n",
       "      <td>0.193542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007996</td>\n",
       "      <td>0.029152</td>\n",
       "      <td>-0.030187</td>\n",
       "      <td>-0.008234</td>\n",
       "      <td>-0.013529</td>\n",
       "      <td>-0.086892</td>\n",
       "      <td>-0.033384</td>\n",
       "      <td>0.095301</td>\n",
       "      <td>-0.056766</td>\n",
       "      <td>0.013498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.029432</td>\n",
       "      <td>-0.099604</td>\n",
       "      <td>-0.133493</td>\n",
       "      <td>0.064296</td>\n",
       "      <td>0.107419</td>\n",
       "      <td>-0.155940</td>\n",
       "      <td>0.035465</td>\n",
       "      <td>0.110237</td>\n",
       "      <td>0.051370</td>\n",
       "      <td>0.098302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031441</td>\n",
       "      <td>-0.006853</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.039125</td>\n",
       "      <td>0.037337</td>\n",
       "      <td>0.018794</td>\n",
       "      <td>-0.065423</td>\n",
       "      <td>0.049548</td>\n",
       "      <td>-0.019722</td>\n",
       "      <td>0.063039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>-0.019815</td>\n",
       "      <td>-0.105710</td>\n",
       "      <td>-0.168113</td>\n",
       "      <td>-0.028623</td>\n",
       "      <td>0.012427</td>\n",
       "      <td>-0.227943</td>\n",
       "      <td>-0.025613</td>\n",
       "      <td>0.090517</td>\n",
       "      <td>0.047136</td>\n",
       "      <td>0.155466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048633</td>\n",
       "      <td>-0.017124</td>\n",
       "      <td>-0.012875</td>\n",
       "      <td>0.056727</td>\n",
       "      <td>-0.040235</td>\n",
       "      <td>-0.037193</td>\n",
       "      <td>-0.003857</td>\n",
       "      <td>0.087729</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.058281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>-0.064367</td>\n",
       "      <td>-0.150520</td>\n",
       "      <td>-0.101398</td>\n",
       "      <td>0.027810</td>\n",
       "      <td>0.060473</td>\n",
       "      <td>-0.166544</td>\n",
       "      <td>-0.007591</td>\n",
       "      <td>0.119012</td>\n",
       "      <td>0.037587</td>\n",
       "      <td>0.153394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036876</td>\n",
       "      <td>0.020799</td>\n",
       "      <td>0.003292</td>\n",
       "      <td>0.009433</td>\n",
       "      <td>0.052629</td>\n",
       "      <td>-0.021794</td>\n",
       "      <td>-0.025866</td>\n",
       "      <td>0.039963</td>\n",
       "      <td>0.011522</td>\n",
       "      <td>0.059293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0.052814</td>\n",
       "      <td>0.212196</td>\n",
       "      <td>0.190459</td>\n",
       "      <td>0.053032</td>\n",
       "      <td>0.244777</td>\n",
       "      <td>-0.192522</td>\n",
       "      <td>-0.019539</td>\n",
       "      <td>0.075527</td>\n",
       "      <td>0.381025</td>\n",
       "      <td>0.356624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008340</td>\n",
       "      <td>0.072279</td>\n",
       "      <td>-0.081167</td>\n",
       "      <td>0.020842</td>\n",
       "      <td>0.051065</td>\n",
       "      <td>-0.031010</td>\n",
       "      <td>-0.041723</td>\n",
       "      <td>-0.222852</td>\n",
       "      <td>0.305756</td>\n",
       "      <td>-0.086334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>-0.570429</td>\n",
       "      <td>-0.687159</td>\n",
       "      <td>-0.420220</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>-0.134033</td>\n",
       "      <td>-0.615365</td>\n",
       "      <td>-0.189209</td>\n",
       "      <td>0.389638</td>\n",
       "      <td>0.214121</td>\n",
       "      <td>0.555281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019895</td>\n",
       "      <td>0.087307</td>\n",
       "      <td>-0.117938</td>\n",
       "      <td>0.109074</td>\n",
       "      <td>-0.030487</td>\n",
       "      <td>-0.117519</td>\n",
       "      <td>-0.015182</td>\n",
       "      <td>0.233117</td>\n",
       "      <td>0.208013</td>\n",
       "      <td>-0.201244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>-0.031507</td>\n",
       "      <td>-0.167382</td>\n",
       "      <td>-0.241901</td>\n",
       "      <td>-0.005488</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>-0.290302</td>\n",
       "      <td>0.034723</td>\n",
       "      <td>0.122054</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>0.233685</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024010</td>\n",
       "      <td>0.026083</td>\n",
       "      <td>-0.061186</td>\n",
       "      <td>0.032631</td>\n",
       "      <td>-0.014527</td>\n",
       "      <td>0.058131</td>\n",
       "      <td>-0.033069</td>\n",
       "      <td>0.139873</td>\n",
       "      <td>-0.025251</td>\n",
       "      <td>-0.011274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>740 rows × 1326 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0   -0.083705 -0.146291 -0.147316  0.043904  0.100317 -0.229497 -0.015868   \n",
       "1   -0.030525 -0.098300 -0.070935  0.015439  0.074234 -0.082444  0.008479   \n",
       "2   -0.061090 -0.183311 -0.206996 -0.021754 -0.025341 -0.317265 -0.027436   \n",
       "3   -0.094754 -0.129334 -0.157846  0.098175  0.048454 -0.238174  0.023942   \n",
       "4   -0.029432 -0.099604 -0.133493  0.064296  0.107419 -0.155940  0.035465   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "735 -0.019815 -0.105710 -0.168113 -0.028623  0.012427 -0.227943 -0.025613   \n",
       "736 -0.064367 -0.150520 -0.101398  0.027810  0.060473 -0.166544 -0.007591   \n",
       "737  0.052814  0.212196  0.190459  0.053032  0.244777 -0.192522 -0.019539   \n",
       "738 -0.570429 -0.687159 -0.420220  0.041096 -0.134033 -0.615365 -0.189209   \n",
       "739 -0.031507 -0.167382 -0.241901 -0.005488  0.004184 -0.290302  0.034723   \n",
       "\n",
       "         7         8         9     ...      1316      1317      1318  \\\n",
       "0    0.104999  0.135875  0.255291  ... -0.013692 -0.036390 -0.074461   \n",
       "1    0.094404  0.057404  0.083904  ...  0.028225  0.006979 -0.027061   \n",
       "2    0.104302  0.105783  0.210912  ...  0.022292 -0.052163 -0.036866   \n",
       "3    0.095521 -0.035370  0.193542  ...  0.007996  0.029152 -0.030187   \n",
       "4    0.110237  0.051370  0.098302  ... -0.031441 -0.006853  0.016900   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "735  0.090517  0.047136  0.155466  ... -0.048633 -0.017124 -0.012875   \n",
       "736  0.119012  0.037587  0.153394  ...  0.036876  0.020799  0.003292   \n",
       "737  0.075527  0.381025  0.356624  ... -0.008340  0.072279 -0.081167   \n",
       "738  0.389638  0.214121  0.555281  ...  0.019895  0.087307 -0.117938   \n",
       "739  0.122054  0.071275  0.233685  ... -0.024010  0.026083 -0.061186   \n",
       "\n",
       "         1319      1320      1321      1322      1323      1324      1325  \n",
       "0   -0.001122 -0.001292  0.016187 -0.011120  0.119724 -0.015386  0.020772  \n",
       "1    0.034229 -0.018317 -0.003150 -0.024807  0.079372  0.024705  0.063742  \n",
       "2   -0.029417 -0.032821 -0.021177 -0.051575  0.144394 -0.029293 -0.004971  \n",
       "3   -0.008234 -0.013529 -0.086892 -0.033384  0.095301 -0.056766  0.013498  \n",
       "4    0.039125  0.037337  0.018794 -0.065423  0.049548 -0.019722  0.063039  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "735  0.056727 -0.040235 -0.037193 -0.003857  0.087729  0.007210  0.058281  \n",
       "736  0.009433  0.052629 -0.021794 -0.025866  0.039963  0.011522  0.059293  \n",
       "737  0.020842  0.051065 -0.031010 -0.041723 -0.222852  0.305756 -0.086334  \n",
       "738  0.109074 -0.030487 -0.117519 -0.015182  0.233117  0.208013 -0.201244  \n",
       "739  0.032631 -0.014527  0.058131 -0.033069  0.139873 -0.025251 -0.011274  \n",
       "\n",
       "[740 rows x 1326 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector_test = input_vector_test.drop(axis=1,columns='simulationRun')\n",
    "input_vector_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 1 at dim 0 (got 1326)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # 데이터 스케일러 인스턴스 생성(데이터 표준화)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# scaler =StandardScaler() \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# # 학습 데이터셋에 대해 fit과 transform 수행: train 기준 정보 계산 및 데이터 변환\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# PyTorch Tensor로 변환 \u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(df_transposed)\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[0;32m     10\u001b[0m X_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(input_vector_test)\u001b[38;5;241m.\u001b[39mto(device)  \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# DataLoader 설정 \u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 1 at dim 0 (got 1326)"
     ]
    }
   ],
   "source": [
    "# # 데이터 스케일러 인스턴스 생성(데이터 표준화)\n",
    "# scaler =StandardScaler() \n",
    "# # 학습 데이터셋에 대해 fit과 transform 수행: train 기준 정보 계산 및 데이터 변환\n",
    "# X_train_scaled = scaler.fit_transform(input_vector_train) \n",
    "# # 테스트 데이터셋에 대해서는 transform만 수행: 학습 데이터셋의 기준 정보를 사용하여 데이터 변환\n",
    "# X_test_scaled = scaler.transform(input_vector_test)  \n",
    "\n",
    "# PyTorch Tensor로 변환 \n",
    "X_train_tensor = torch.FloatTensor(input_vector_train).to(device) \n",
    "X_test_tensor = torch.FloatTensor(input_vector_test).to(device)  \n",
    "\n",
    "# DataLoader 설정 \n",
    "train_dataset = TensorDataset(X_train_tensor, X_train_tensor) # 입력과 타겟이 같음 \n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector_train_np = df_transposed.to_numpy()\n",
    "input_vector_test_np = input_vector_test.to_numpy()\n",
    "X_train_tensor = torch.tensor(input_vector_train_np, dtype=torch.float32).to(device)  \n",
    "X_test_tensor = torch.tensor(input_vector_test_np, dtype=torch.float32).to(device)  \n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2805110812187195\n",
      "Epoch 2, Loss: 0.27911385893821716\n",
      "Epoch 3, Loss: 0.2782140374183655\n",
      "Epoch 4, Loss: 0.2777785360813141\n",
      "Epoch 5, Loss: 0.277641624212265\n",
      "Epoch 6, Loss: 0.2775997817516327\n",
      "Epoch 7, Loss: 0.2775762677192688\n",
      "Epoch 8, Loss: 0.2775520086288452\n",
      "Epoch 9, Loss: 0.277524471282959\n",
      "Epoch 10, Loss: 0.27749666571617126\n",
      "Epoch 11, Loss: 0.27746713161468506\n",
      "Epoch 12, Loss: 0.2774345576763153\n",
      "Epoch 13, Loss: 0.27740025520324707\n",
      "Epoch 14, Loss: 0.2773599326610565\n",
      "Epoch 15, Loss: 0.27731746435165405\n",
      "Epoch 16, Loss: 0.27727213501930237\n",
      "Epoch 17, Loss: 0.277224063873291\n",
      "Epoch 18, Loss: 0.2771708071231842\n",
      "Epoch 19, Loss: 0.2771216034889221\n",
      "Epoch 20, Loss: 0.2770688235759735\n",
      "Epoch 21, Loss: 0.2770254611968994\n",
      "Epoch 22, Loss: 0.2769869565963745\n",
      "Epoch 23, Loss: 0.2769606411457062\n",
      "Epoch 24, Loss: 0.27694061398506165\n",
      "Epoch 25, Loss: 0.27693289518356323\n",
      "Epoch 26, Loss: 0.27692675590515137\n",
      "Epoch 27, Loss: 0.2769232392311096\n",
      "Epoch 28, Loss: 0.2769194543361664\n",
      "Epoch 29, Loss: 0.2769164741039276\n",
      "Epoch 30, Loss: 0.2769099175930023\n",
      "Epoch 31, Loss: 0.2769050598144531\n",
      "Epoch 32, Loss: 0.2768974304199219\n",
      "Epoch 33, Loss: 0.27689096331596375\n",
      "Epoch 34, Loss: 0.27688726782798767\n",
      "Epoch 35, Loss: 0.2768843472003937\n",
      "Epoch 36, Loss: 0.27688315510749817\n",
      "Epoch 37, Loss: 0.27688390016555786\n",
      "Epoch 38, Loss: 0.2768857479095459\n",
      "Epoch 39, Loss: 0.2768864929676056\n",
      "Epoch 40, Loss: 0.2768864929676056\n",
      "Epoch 41, Loss: 0.27688515186309814\n",
      "Epoch 42, Loss: 0.2768833637237549\n",
      "Epoch 43, Loss: 0.27688056230545044\n",
      "Epoch 44, Loss: 0.27687788009643555\n",
      "Epoch 45, Loss: 0.27687525749206543\n",
      "Epoch 46, Loss: 0.27687323093414307\n",
      "Epoch 47, Loss: 0.2768727242946625\n",
      "Epoch 48, Loss: 0.2768712639808655\n",
      "Epoch 49, Loss: 0.2768712639808655\n",
      "Epoch 50, Loss: 0.2768717110157013\n",
      "Epoch 51, Loss: 0.2768724262714386\n",
      "Epoch 52, Loss: 0.2768732011318207\n",
      "Epoch 53, Loss: 0.27687376737594604\n",
      "Epoch 54, Loss: 0.27687403559684753\n",
      "Epoch 55, Loss: 0.2768702507019043\n",
      "Epoch 56, Loss: 0.27686554193496704\n",
      "Epoch 57, Loss: 0.27686136960983276\n",
      "Epoch 58, Loss: 0.27685821056365967\n",
      "Epoch 59, Loss: 0.2768564522266388\n",
      "Epoch 60, Loss: 0.27685603499412537\n",
      "Epoch 61, Loss: 0.2768566608428955\n",
      "Epoch 62, Loss: 0.2768574655056\n",
      "Epoch 63, Loss: 0.27685874700546265\n",
      "Epoch 64, Loss: 0.2768588364124298\n",
      "Epoch 65, Loss: 0.27685901522636414\n",
      "Epoch 66, Loss: 0.27685874700546265\n",
      "Epoch 67, Loss: 0.27684369683265686\n",
      "Epoch 68, Loss: 0.276824027299881\n",
      "Epoch 69, Loss: 0.2768026888370514\n",
      "Epoch 70, Loss: 0.27678173780441284\n",
      "Epoch 71, Loss: 0.2767627239227295\n",
      "Epoch 72, Loss: 0.27674785256385803\n",
      "Epoch 73, Loss: 0.2767326533794403\n",
      "Epoch 74, Loss: 0.27672258019447327\n",
      "Epoch 75, Loss: 0.27671587467193604\n",
      "Epoch 76, Loss: 0.27671104669570923\n",
      "Epoch 77, Loss: 0.2767079174518585\n",
      "Epoch 78, Loss: 0.2767062783241272\n",
      "Epoch 79, Loss: 0.27670517563819885\n",
      "Epoch 80, Loss: 0.27670541405677795\n",
      "Epoch 81, Loss: 0.27670395374298096\n",
      "Epoch 82, Loss: 0.2767035663127899\n",
      "Epoch 83, Loss: 0.27670326828956604\n",
      "Epoch 84, Loss: 0.27670368552207947\n",
      "Epoch 85, Loss: 0.27670320868492126\n",
      "Epoch 86, Loss: 0.27670398354530334\n",
      "Epoch 87, Loss: 0.2767038643360138\n",
      "Epoch 88, Loss: 0.27670392394065857\n",
      "Epoch 89, Loss: 0.27670398354530334\n",
      "Epoch 90, Loss: 0.27670395374298096\n",
      "Epoch 91, Loss: 0.2767038643360138\n",
      "Epoch 92, Loss: 0.27670374512672424\n",
      "Epoch 93, Loss: 0.2767036557197571\n",
      "Epoch 94, Loss: 0.2767038643360138\n",
      "Epoch 95, Loss: 0.276703804731369\n",
      "Epoch 96, Loss: 0.27670547366142273\n",
      "Epoch 97, Loss: 0.27670472860336304\n",
      "Epoch 98, Loss: 0.2767043113708496\n",
      "Epoch 99, Loss: 0.27670377492904663\n",
      "Epoch 100, Loss: 0.27670377492904663\n",
      "Epoch 101, Loss: 0.2767038941383362\n",
      "Epoch 102, Loss: 0.2766953408718109\n",
      "Epoch 103, Loss: 0.2766869366168976\n",
      "Epoch 104, Loss: 0.27668142318725586\n",
      "Epoch 105, Loss: 0.2766788899898529\n",
      "Epoch 106, Loss: 0.27667951583862305\n",
      "Epoch 107, Loss: 0.2766815721988678\n",
      "Epoch 108, Loss: 0.2766837179660797\n",
      "Epoch 109, Loss: 0.2766851782798767\n",
      "Epoch 110, Loss: 0.2766856551170349\n",
      "Epoch 111, Loss: 0.27668529748916626\n",
      "Epoch 112, Loss: 0.2766840159893036\n",
      "Epoch 113, Loss: 0.2766824960708618\n",
      "Epoch 114, Loss: 0.27668097615242004\n",
      "Epoch 115, Loss: 0.27667954564094543\n",
      "Epoch 116, Loss: 0.2766787111759186\n",
      "Epoch 117, Loss: 0.2766783833503723\n",
      "Epoch 118, Loss: 0.27667859196662903\n",
      "Epoch 119, Loss: 0.2766790986061096\n",
      "Epoch 120, Loss: 0.2766796052455902\n",
      "Epoch 121, Loss: 0.27667996287345886\n",
      "Epoch 122, Loss: 0.2766801118850708\n",
      "Epoch 123, Loss: 0.2766799032688141\n",
      "Epoch 124, Loss: 0.2766796052455902\n",
      "Epoch 125, Loss: 0.276679128408432\n",
      "Epoch 126, Loss: 0.2766787111759186\n",
      "Epoch 127, Loss: 0.2766783833503723\n",
      "Epoch 128, Loss: 0.27667826414108276\n",
      "Epoch 129, Loss: 0.27667826414108276\n",
      "Epoch 130, Loss: 0.2766783833503723\n",
      "Epoch 131, Loss: 0.2766784429550171\n",
      "Epoch 132, Loss: 0.27667853236198425\n",
      "Epoch 133, Loss: 0.2766786217689514\n",
      "Epoch 134, Loss: 0.2766786217689514\n",
      "Epoch 135, Loss: 0.2766786515712738\n",
      "Epoch 136, Loss: 0.27667853236198425\n",
      "Epoch 137, Loss: 0.2766784429550171\n",
      "Epoch 138, Loss: 0.2766783535480499\n",
      "Epoch 139, Loss: 0.27667826414108276\n",
      "Epoch 140, Loss: 0.276678204536438\n",
      "Epoch 141, Loss: 0.276678204536438\n",
      "Epoch 142, Loss: 0.276678204536438\n",
      "Epoch 143, Loss: 0.276678204536438\n",
      "Epoch 144, Loss: 0.2766782343387604\n",
      "Epoch 145, Loss: 0.27667826414108276\n",
      "Epoch 146, Loss: 0.27667826414108276\n",
      "Epoch 147, Loss: 0.27667826414108276\n",
      "Epoch 148, Loss: 0.2766782343387604\n",
      "Epoch 149, Loss: 0.276678204536438\n",
      "Epoch 150, Loss: 0.2766781747341156\n",
      "Epoch 151, Loss: 0.2766781747341156\n",
      "Epoch 152, Loss: 0.2766781151294708\n",
      "Epoch 153, Loss: 0.2766781151294708\n",
      "Epoch 154, Loss: 0.2766781151294708\n",
      "Epoch 155, Loss: 0.2766781151294708\n",
      "Epoch 156, Loss: 0.2766781151294708\n",
      "Epoch 157, Loss: 0.2766781151294708\n",
      "Epoch 158, Loss: 0.2766781151294708\n",
      "Epoch 159, Loss: 0.2766781151294708\n",
      "Epoch 160, Loss: 0.2766781151294708\n",
      "Epoch 161, Loss: 0.27667808532714844\n",
      "Epoch 162, Loss: 0.27667808532714844\n",
      "Epoch 163, Loss: 0.27667808532714844\n",
      "Epoch 164, Loss: 0.27667808532714844\n",
      "Epoch 165, Loss: 0.27667808532714844\n",
      "Epoch 166, Loss: 0.27667808532714844\n",
      "Epoch 167, Loss: 0.27667808532714844\n",
      "Epoch 168, Loss: 0.27667808532714844\n",
      "Epoch 169, Loss: 0.27667805552482605\n",
      "Epoch 170, Loss: 0.27667805552482605\n",
      "Epoch 171, Loss: 0.27667805552482605\n",
      "Epoch 172, Loss: 0.27667802572250366\n",
      "Epoch 173, Loss: 0.27667802572250366\n",
      "Epoch 174, Loss: 0.27667802572250366\n",
      "Epoch 175, Loss: 0.27667802572250366\n",
      "Epoch 176, Loss: 0.27667802572250366\n",
      "Epoch 177, Loss: 0.27667802572250366\n",
      "Epoch 178, Loss: 0.27667802572250366\n",
      "Epoch 179, Loss: 0.2766779959201813\n",
      "Epoch 180, Loss: 0.2766779959201813\n",
      "Epoch 181, Loss: 0.2766779959201813\n",
      "Epoch 182, Loss: 0.2766779959201813\n",
      "Epoch 183, Loss: 0.2766779959201813\n",
      "Epoch 184, Loss: 0.2766779959201813\n",
      "Epoch 185, Loss: 0.2766779959201813\n",
      "Epoch 186, Loss: 0.2766779959201813\n",
      "Epoch 187, Loss: 0.2766779959201813\n",
      "Epoch 188, Loss: 0.2766779959201813\n",
      "Epoch 189, Loss: 0.2766779959201813\n",
      "Epoch 190, Loss: 0.2766779959201813\n",
      "Epoch 191, Loss: 0.2766779959201813\n",
      "Epoch 192, Loss: 0.2766779959201813\n",
      "Epoch 193, Loss: 0.2766779959201813\n",
      "Epoch 194, Loss: 0.2766779959201813\n",
      "Epoch 195, Loss: 0.2766779959201813\n",
      "Epoch 196, Loss: 0.2766779959201813\n",
      "Epoch 197, Loss: 0.2766779959201813\n",
      "Epoch 198, Loss: 0.2766779661178589\n",
      "Epoch 199, Loss: 0.2766779661178589\n",
      "Epoch 200, Loss: 0.2766779661178589\n",
      "Epoch 201, Loss: 0.2766779661178589\n",
      "Epoch 202, Loss: 0.2766779363155365\n",
      "Epoch 203, Loss: 0.2766779363155365\n",
      "Epoch 204, Loss: 0.2766779363155365\n",
      "Epoch 205, Loss: 0.2766779363155365\n",
      "Epoch 206, Loss: 0.2766779363155365\n",
      "Epoch 207, Loss: 0.2766779363155365\n",
      "Epoch 208, Loss: 0.2766779363155365\n",
      "Epoch 209, Loss: 0.2766779363155365\n",
      "Epoch 210, Loss: 0.2766779363155365\n",
      "Epoch 211, Loss: 0.2766779363155365\n",
      "Epoch 212, Loss: 0.2766779363155365\n",
      "Epoch 213, Loss: 0.2766779363155365\n",
      "Epoch 214, Loss: 0.2766779363155365\n",
      "Epoch 215, Loss: 0.2766779363155365\n",
      "Epoch 216, Loss: 0.2766779363155365\n",
      "Epoch 217, Loss: 0.2766779363155365\n",
      "Epoch 218, Loss: 0.2766779363155365\n",
      "Epoch 219, Loss: 0.2766779363155365\n",
      "Epoch 220, Loss: 0.2766779363155365\n",
      "Epoch 221, Loss: 0.2766779065132141\n",
      "Epoch 222, Loss: 0.2766779065132141\n",
      "Epoch 223, Loss: 0.2766779065132141\n",
      "Epoch 224, Loss: 0.2766779065132141\n",
      "Epoch 225, Loss: 0.2766779065132141\n",
      "Epoch 226, Loss: 0.2766779065132141\n",
      "Epoch 227, Loss: 0.2766779065132141\n",
      "Epoch 228, Loss: 0.2766779065132141\n",
      "Epoch 229, Loss: 0.2766779065132141\n",
      "Epoch 230, Loss: 0.2766779065132141\n",
      "Epoch 231, Loss: 0.2766779065132141\n",
      "Epoch 232, Loss: 0.2766779065132141\n",
      "Epoch 233, Loss: 0.2766779065132141\n",
      "Epoch 234, Loss: 0.2766779065132141\n",
      "Epoch 235, Loss: 0.2766779065132141\n",
      "Epoch 236, Loss: 0.2766779065132141\n",
      "Epoch 237, Loss: 0.2766779065132141\n",
      "Epoch 238, Loss: 0.2766779065132141\n",
      "Epoch 239, Loss: 0.2766779065132141\n",
      "Epoch 240, Loss: 0.2766779065132141\n",
      "Epoch 241, Loss: 0.2766779065132141\n",
      "Epoch 242, Loss: 0.2766779065132141\n",
      "Epoch 243, Loss: 0.2766779065132141\n",
      "Epoch 244, Loss: 0.2766779065132141\n",
      "Epoch 245, Loss: 0.2766779065132141\n",
      "Epoch 246, Loss: 0.2766779065132141\n",
      "Epoch 247, Loss: 0.2766779065132141\n",
      "Epoch 248, Loss: 0.2766779065132141\n",
      "Epoch 249, Loss: 0.2766779065132141\n",
      "Epoch 250, Loss: 0.2766778767108917\n",
      "Epoch 251, Loss: 0.2766778767108917\n",
      "Epoch 252, Loss: 0.2766778767108917\n",
      "Epoch 253, Loss: 0.2766778767108917\n",
      "Epoch 254, Loss: 0.2766778767108917\n",
      "Epoch 255, Loss: 0.2766778767108917\n",
      "Epoch 256, Loss: 0.2766778767108917\n",
      "Epoch 257, Loss: 0.2766778767108917\n",
      "Epoch 258, Loss: 0.2766778767108917\n",
      "Epoch 259, Loss: 0.2766778767108917\n",
      "Epoch 260, Loss: 0.2766778767108917\n",
      "Epoch 261, Loss: 0.2766778767108917\n",
      "Epoch 262, Loss: 0.2766778767108917\n",
      "Epoch 263, Loss: 0.2766778767108917\n",
      "Epoch 264, Loss: 0.2766778767108917\n",
      "Epoch 265, Loss: 0.2766778767108917\n",
      "Epoch 266, Loss: 0.2766778767108917\n",
      "Epoch 267, Loss: 0.2766778767108917\n",
      "Epoch 268, Loss: 0.2766778767108917\n",
      "Epoch 269, Loss: 0.2766778767108917\n",
      "Epoch 270, Loss: 0.2766778767108917\n",
      "Epoch 271, Loss: 0.2766778767108917\n",
      "Epoch 272, Loss: 0.2766778767108917\n",
      "Epoch 273, Loss: 0.2766778767108917\n",
      "Epoch 274, Loss: 0.2766778767108917\n",
      "Epoch 275, Loss: 0.27667784690856934\n",
      "Epoch 276, Loss: 0.27667784690856934\n",
      "Epoch 277, Loss: 0.27667784690856934\n",
      "Epoch 278, Loss: 0.27667784690856934\n",
      "Epoch 279, Loss: 0.27667784690856934\n",
      "Epoch 280, Loss: 0.27667784690856934\n",
      "Epoch 281, Loss: 0.27667784690856934\n",
      "Epoch 282, Loss: 0.27667784690856934\n",
      "Epoch 283, Loss: 0.27667784690856934\n",
      "Epoch 284, Loss: 0.27667784690856934\n",
      "Epoch 285, Loss: 0.27667784690856934\n",
      "Epoch 286, Loss: 0.27667784690856934\n",
      "Epoch 287, Loss: 0.27667784690856934\n",
      "Epoch 288, Loss: 0.27667784690856934\n",
      "Epoch 289, Loss: 0.27667784690856934\n",
      "Epoch 290, Loss: 0.27667784690856934\n",
      "Epoch 291, Loss: 0.27667784690856934\n",
      "Epoch 292, Loss: 0.27667784690856934\n",
      "Epoch 293, Loss: 0.27667784690856934\n",
      "Epoch 294, Loss: 0.27667784690856934\n",
      "Epoch 295, Loss: 0.27667784690856934\n",
      "Epoch 296, Loss: 0.27667784690856934\n",
      "Epoch 297, Loss: 0.27667784690856934\n",
      "Epoch 298, Loss: 0.27667784690856934\n",
      "Epoch 299, Loss: 0.27667784690856934\n",
      "Epoch 300, Loss: 0.27667784690856934\n",
      "Epoch 301, Loss: 0.27667784690856934\n",
      "Epoch 302, Loss: 0.27667784690856934\n",
      "Epoch 303, Loss: 0.27667784690856934\n",
      "Epoch 304, Loss: 0.27667784690856934\n",
      "Epoch 305, Loss: 0.27667784690856934\n",
      "Epoch 306, Loss: 0.27667784690856934\n",
      "Epoch 307, Loss: 0.27667784690856934\n",
      "Epoch 308, Loss: 0.27667784690856934\n",
      "Epoch 309, Loss: 0.27667784690856934\n",
      "Epoch 310, Loss: 0.27667784690856934\n",
      "Epoch 311, Loss: 0.27667784690856934\n",
      "Epoch 312, Loss: 0.27667784690856934\n",
      "Epoch 313, Loss: 0.27667784690856934\n",
      "Epoch 314, Loss: 0.27667784690856934\n",
      "Epoch 315, Loss: 0.27667784690856934\n",
      "Epoch 316, Loss: 0.27667784690856934\n",
      "Epoch 317, Loss: 0.27667784690856934\n",
      "Epoch 318, Loss: 0.27667784690856934\n",
      "Epoch 319, Loss: 0.27667781710624695\n",
      "Epoch 320, Loss: 0.27667781710624695\n",
      "Epoch 321, Loss: 0.27667781710624695\n",
      "Epoch 322, Loss: 0.27667781710624695\n",
      "Epoch 323, Loss: 0.27667781710624695\n",
      "Epoch 324, Loss: 0.27667781710624695\n",
      "Epoch 325, Loss: 0.27667781710624695\n",
      "Epoch 326, Loss: 0.27667781710624695\n",
      "Epoch 327, Loss: 0.27667781710624695\n",
      "Epoch 328, Loss: 0.27667781710624695\n",
      "Epoch 329, Loss: 0.27667781710624695\n",
      "Epoch 330, Loss: 0.27667781710624695\n",
      "Epoch 331, Loss: 0.27667781710624695\n",
      "Epoch 332, Loss: 0.27667781710624695\n",
      "Epoch 333, Loss: 0.27667781710624695\n",
      "Epoch 334, Loss: 0.27667781710624695\n",
      "Epoch 335, Loss: 0.27667781710624695\n",
      "Epoch 336, Loss: 0.27667781710624695\n",
      "Epoch 337, Loss: 0.27667781710624695\n",
      "Epoch 338, Loss: 0.27667781710624695\n",
      "Epoch 339, Loss: 0.27667781710624695\n",
      "Epoch 340, Loss: 0.27667781710624695\n",
      "Epoch 341, Loss: 0.27667781710624695\n",
      "Epoch 342, Loss: 0.27667781710624695\n",
      "Epoch 343, Loss: 0.27667781710624695\n",
      "Epoch 344, Loss: 0.27667781710624695\n",
      "Epoch 345, Loss: 0.27667781710624695\n",
      "Epoch 346, Loss: 0.27667781710624695\n",
      "Epoch 347, Loss: 0.27667781710624695\n",
      "Epoch 348, Loss: 0.27667781710624695\n",
      "Epoch 349, Loss: 0.27667781710624695\n",
      "Epoch 350, Loss: 0.27667781710624695\n",
      "Epoch 351, Loss: 0.27667781710624695\n",
      "Epoch 352, Loss: 0.27667781710624695\n",
      "Epoch 353, Loss: 0.27667781710624695\n",
      "Epoch 354, Loss: 0.27667781710624695\n",
      "Epoch 355, Loss: 0.27667781710624695\n",
      "Epoch 356, Loss: 0.27667781710624695\n",
      "Epoch 357, Loss: 0.27667781710624695\n",
      "Epoch 358, Loss: 0.27667781710624695\n",
      "Epoch 359, Loss: 0.27667781710624695\n",
      "Epoch 360, Loss: 0.27667781710624695\n",
      "Epoch 361, Loss: 0.27667781710624695\n",
      "Epoch 362, Loss: 0.27667781710624695\n",
      "Epoch 363, Loss: 0.27667781710624695\n",
      "Epoch 364, Loss: 0.27667781710624695\n",
      "Epoch 365, Loss: 0.27667781710624695\n",
      "Epoch 366, Loss: 0.27667781710624695\n",
      "Epoch 367, Loss: 0.27667781710624695\n",
      "Epoch 368, Loss: 0.27667781710624695\n",
      "Epoch 369, Loss: 0.27667781710624695\n",
      "Epoch 370, Loss: 0.27667781710624695\n",
      "Epoch 371, Loss: 0.27667778730392456\n",
      "Epoch 372, Loss: 0.27667778730392456\n",
      "Epoch 373, Loss: 0.27667778730392456\n",
      "Epoch 374, Loss: 0.27667778730392456\n",
      "Epoch 375, Loss: 0.27667778730392456\n",
      "Epoch 376, Loss: 0.27667778730392456\n",
      "Epoch 377, Loss: 0.27667778730392456\n",
      "Epoch 378, Loss: 0.27667778730392456\n",
      "Epoch 379, Loss: 0.27667778730392456\n",
      "Epoch 380, Loss: 0.27667778730392456\n",
      "Epoch 381, Loss: 0.27667778730392456\n",
      "Epoch 382, Loss: 0.27667778730392456\n",
      "Epoch 383, Loss: 0.27667778730392456\n",
      "Epoch 384, Loss: 0.27667778730392456\n",
      "Epoch 385, Loss: 0.27667778730392456\n",
      "Epoch 386, Loss: 0.27667778730392456\n",
      "Epoch 387, Loss: 0.27667778730392456\n",
      "Epoch 388, Loss: 0.27667778730392456\n",
      "Epoch 389, Loss: 0.27667778730392456\n",
      "Epoch 390, Loss: 0.27667778730392456\n",
      "Epoch 391, Loss: 0.27667778730392456\n",
      "Epoch 392, Loss: 0.27667778730392456\n",
      "Epoch 393, Loss: 0.27667778730392456\n",
      "Epoch 394, Loss: 0.27667778730392456\n",
      "Epoch 395, Loss: 0.27667778730392456\n",
      "Epoch 396, Loss: 0.27667778730392456\n",
      "Epoch 397, Loss: 0.27667778730392456\n",
      "Epoch 398, Loss: 0.27667778730392456\n",
      "Epoch 399, Loss: 0.27667778730392456\n",
      "Epoch 400, Loss: 0.27667778730392456\n",
      "Epoch 401, Loss: 0.27667778730392456\n",
      "Epoch 402, Loss: 0.27667778730392456\n",
      "Epoch 403, Loss: 0.27667778730392456\n",
      "Epoch 404, Loss: 0.27667778730392456\n",
      "Epoch 405, Loss: 0.27667778730392456\n",
      "Epoch 406, Loss: 0.27667778730392456\n",
      "Epoch 407, Loss: 0.27667778730392456\n",
      "Epoch 408, Loss: 0.27667778730392456\n",
      "Epoch 409, Loss: 0.27667778730392456\n",
      "Epoch 410, Loss: 0.27667778730392456\n",
      "Epoch 411, Loss: 0.27667778730392456\n",
      "Epoch 412, Loss: 0.27667778730392456\n",
      "Epoch 413, Loss: 0.27667778730392456\n",
      "Epoch 414, Loss: 0.27667778730392456\n",
      "Epoch 415, Loss: 0.27667778730392456\n",
      "Epoch 416, Loss: 0.27667778730392456\n",
      "Epoch 417, Loss: 0.27667778730392456\n",
      "Epoch 418, Loss: 0.27667778730392456\n",
      "Epoch 419, Loss: 0.27667778730392456\n",
      "Epoch 420, Loss: 0.27667778730392456\n",
      "Epoch 421, Loss: 0.27667778730392456\n",
      "Epoch 422, Loss: 0.27667778730392456\n",
      "Epoch 423, Loss: 0.27667778730392456\n",
      "Epoch 424, Loss: 0.27667778730392456\n",
      "Epoch 425, Loss: 0.27667778730392456\n",
      "Epoch 426, Loss: 0.27667778730392456\n",
      "Epoch 427, Loss: 0.27667778730392456\n",
      "Epoch 428, Loss: 0.27667778730392456\n",
      "Epoch 429, Loss: 0.27667781710624695\n",
      "Epoch 430, Loss: 0.27667778730392456\n",
      "Epoch 431, Loss: 0.27667778730392456\n",
      "Epoch 432, Loss: 0.27667778730392456\n",
      "Epoch 433, Loss: 0.27667778730392456\n",
      "Epoch 434, Loss: 0.27667778730392456\n",
      "Epoch 435, Loss: 0.27667778730392456\n",
      "Epoch 436, Loss: 0.27667778730392456\n",
      "Epoch 437, Loss: 0.27667778730392456\n",
      "Epoch 438, Loss: 0.27667778730392456\n",
      "Epoch 439, Loss: 0.27667778730392456\n",
      "Epoch 440, Loss: 0.27667778730392456\n",
      "Epoch 441, Loss: 0.27667778730392456\n",
      "Epoch 442, Loss: 0.27667778730392456\n",
      "Epoch 443, Loss: 0.27667778730392456\n",
      "Epoch 444, Loss: 0.27667778730392456\n",
      "Epoch 445, Loss: 0.27667778730392456\n",
      "Epoch 446, Loss: 0.27667778730392456\n",
      "Epoch 447, Loss: 0.27667778730392456\n",
      "Epoch 448, Loss: 0.27667778730392456\n",
      "Epoch 449, Loss: 0.27667778730392456\n",
      "Epoch 450, Loss: 0.27667778730392456\n",
      "Epoch 451, Loss: 0.27667778730392456\n",
      "Epoch 452, Loss: 0.27667778730392456\n",
      "Epoch 453, Loss: 0.27667778730392456\n",
      "Epoch 454, Loss: 0.27667778730392456\n",
      "Epoch 455, Loss: 0.27667778730392456\n",
      "Epoch 456, Loss: 0.27667778730392456\n",
      "Epoch 457, Loss: 0.27667778730392456\n",
      "Epoch 458, Loss: 0.27667778730392456\n",
      "Epoch 459, Loss: 0.27667778730392456\n",
      "Epoch 460, Loss: 0.27667778730392456\n",
      "Epoch 461, Loss: 0.27667778730392456\n",
      "Epoch 462, Loss: 0.27667778730392456\n",
      "Epoch 463, Loss: 0.27667778730392456\n",
      "Epoch 464, Loss: 0.27667778730392456\n",
      "Epoch 465, Loss: 0.27667778730392456\n",
      "Epoch 466, Loss: 0.27667778730392456\n",
      "Epoch 467, Loss: 0.27667778730392456\n",
      "Epoch 468, Loss: 0.27667778730392456\n",
      "Epoch 469, Loss: 0.27667778730392456\n",
      "Epoch 470, Loss: 0.27667778730392456\n",
      "Epoch 471, Loss: 0.27667778730392456\n",
      "Epoch 472, Loss: 0.27667778730392456\n",
      "Epoch 473, Loss: 0.27667778730392456\n",
      "Epoch 474, Loss: 0.27667778730392456\n",
      "Epoch 475, Loss: 0.27667778730392456\n",
      "Epoch 476, Loss: 0.27667778730392456\n",
      "Epoch 477, Loss: 0.27667778730392456\n",
      "Epoch 478, Loss: 0.27667778730392456\n",
      "Epoch 479, Loss: 0.27667778730392456\n",
      "Epoch 480, Loss: 0.27667778730392456\n",
      "Epoch 481, Loss: 0.27667778730392456\n",
      "Epoch 482, Loss: 0.27667778730392456\n",
      "Epoch 483, Loss: 0.27667778730392456\n",
      "Epoch 484, Loss: 0.27667778730392456\n",
      "Epoch 485, Loss: 0.27667778730392456\n",
      "Epoch 486, Loss: 0.27667778730392456\n",
      "Epoch 487, Loss: 0.27667778730392456\n",
      "Epoch 488, Loss: 0.27667778730392456\n",
      "Epoch 489, Loss: 0.27667778730392456\n",
      "Epoch 490, Loss: 0.27667778730392456\n",
      "Epoch 491, Loss: 0.27667778730392456\n",
      "Epoch 492, Loss: 0.27667778730392456\n",
      "Epoch 493, Loss: 0.27667778730392456\n",
      "Epoch 494, Loss: 0.27667778730392456\n",
      "Epoch 495, Loss: 0.27667778730392456\n",
      "Epoch 496, Loss: 0.27667778730392456\n",
      "Epoch 497, Loss: 0.27667778730392456\n",
      "Epoch 498, Loss: 0.27667778730392456\n",
      "Epoch 499, Loss: 0.27667778730392456\n",
      "Epoch 500, Loss: 0.27667778730392456\n",
      "Epoch 501, Loss: 0.27667778730392456\n",
      "Epoch 502, Loss: 0.27667778730392456\n",
      "Epoch 503, Loss: 0.27667778730392456\n",
      "Epoch 504, Loss: 0.27667778730392456\n",
      "Epoch 505, Loss: 0.27667778730392456\n",
      "Epoch 506, Loss: 0.27667778730392456\n",
      "Epoch 507, Loss: 0.27667778730392456\n",
      "Epoch 508, Loss: 0.27667778730392456\n",
      "Epoch 509, Loss: 0.27667778730392456\n",
      "Epoch 510, Loss: 0.27667778730392456\n",
      "Epoch 511, Loss: 0.27667778730392456\n",
      "Epoch 512, Loss: 0.27667778730392456\n",
      "Epoch 513, Loss: 0.27667778730392456\n",
      "Epoch 514, Loss: 0.27667778730392456\n",
      "Epoch 515, Loss: 0.27667778730392456\n",
      "Epoch 516, Loss: 0.27667778730392456\n",
      "Epoch 517, Loss: 0.27667778730392456\n",
      "Epoch 518, Loss: 0.27667778730392456\n",
      "Epoch 519, Loss: 0.27667778730392456\n",
      "Epoch 520, Loss: 0.27667778730392456\n",
      "Epoch 521, Loss: 0.27667778730392456\n",
      "Epoch 522, Loss: 0.27667778730392456\n",
      "Epoch 523, Loss: 0.27667778730392456\n",
      "Epoch 524, Loss: 0.2766779363155365\n",
      "Epoch 525, Loss: 0.27667778730392456\n",
      "Epoch 526, Loss: 0.27667778730392456\n",
      "Epoch 527, Loss: 0.27667778730392456\n",
      "Epoch 528, Loss: 0.2766779363155365\n",
      "Epoch 529, Loss: 0.27667778730392456\n",
      "Epoch 530, Loss: 0.27667778730392456\n",
      "Epoch 531, Loss: 0.27667778730392456\n",
      "Epoch 532, Loss: 0.27667778730392456\n",
      "Epoch 533, Loss: 0.27667778730392456\n",
      "Epoch 534, Loss: 0.27667778730392456\n",
      "Epoch 535, Loss: 0.2766781747341156\n",
      "Epoch 536, Loss: 0.27667850255966187\n",
      "Epoch 537, Loss: 0.27667778730392456\n",
      "Epoch 538, Loss: 0.2766782343387604\n",
      "Epoch 539, Loss: 0.27667778730392456\n",
      "Epoch 540, Loss: 0.27667778730392456\n",
      "Epoch 541, Loss: 0.27667778730392456\n",
      "Epoch 542, Loss: 0.27667778730392456\n",
      "Epoch 543, Loss: 0.27667778730392456\n",
      "Epoch 544, Loss: 0.27667778730392456\n",
      "Epoch 545, Loss: 0.2766778767108917\n",
      "Epoch 546, Loss: 0.27667778730392456\n",
      "Epoch 547, Loss: 0.27667778730392456\n",
      "Epoch 548, Loss: 0.27667784690856934\n",
      "Epoch 549, Loss: 0.27667778730392456\n",
      "Epoch 550, Loss: 0.27667778730392456\n",
      "Epoch 551, Loss: 0.27667832374572754\n",
      "Epoch 552, Loss: 0.27667778730392456\n",
      "Epoch 553, Loss: 0.27667778730392456\n",
      "Epoch 554, Loss: 0.27667850255966187\n",
      "Epoch 555, Loss: 0.27667778730392456\n",
      "Epoch 556, Loss: 0.27667778730392456\n",
      "Epoch 557, Loss: 0.2766779959201813\n",
      "Epoch 558, Loss: 0.27667778730392456\n",
      "Epoch 559, Loss: 0.27667778730392456\n",
      "Epoch 560, Loss: 0.27667853236198425\n",
      "Epoch 561, Loss: 0.27667778730392456\n",
      "Epoch 562, Loss: 0.27667778730392456\n",
      "Epoch 563, Loss: 0.27667778730392456\n",
      "Epoch 564, Loss: 0.2766779959201813\n",
      "Epoch 565, Loss: 0.2766779959201813\n",
      "Epoch 566, Loss: 0.27667778730392456\n",
      "Epoch 567, Loss: 0.27667778730392456\n",
      "Epoch 568, Loss: 0.27667778730392456\n",
      "Epoch 569, Loss: 0.27667778730392456\n",
      "Epoch 570, Loss: 0.27667778730392456\n",
      "Epoch 571, Loss: 0.27667778730392456\n",
      "Epoch 572, Loss: 0.27667778730392456\n",
      "Epoch 573, Loss: 0.27667778730392456\n",
      "Epoch 574, Loss: 0.27667778730392456\n",
      "Epoch 575, Loss: 0.27667778730392456\n",
      "Epoch 576, Loss: 0.27667778730392456\n",
      "Epoch 577, Loss: 0.27667778730392456\n",
      "Epoch 578, Loss: 0.27667778730392456\n",
      "Epoch 579, Loss: 0.2766778767108917\n",
      "Epoch 580, Loss: 0.27667778730392456\n",
      "Epoch 581, Loss: 0.27667778730392456\n",
      "Epoch 582, Loss: 0.27667778730392456\n",
      "Epoch 583, Loss: 0.27667778730392456\n",
      "Epoch 584, Loss: 0.27667778730392456\n",
      "Epoch 585, Loss: 0.27667778730392456\n",
      "Epoch 586, Loss: 0.27667778730392456\n",
      "Epoch 587, Loss: 0.27667778730392456\n",
      "Epoch 588, Loss: 0.27667805552482605\n",
      "Epoch 589, Loss: 0.27667778730392456\n",
      "Epoch 590, Loss: 0.27667880058288574\n",
      "Epoch 591, Loss: 0.2766779363155365\n",
      "Epoch 592, Loss: 0.27667778730392456\n",
      "Epoch 593, Loss: 0.27667778730392456\n",
      "Epoch 594, Loss: 0.27667778730392456\n",
      "Epoch 595, Loss: 0.2766779959201813\n",
      "Epoch 596, Loss: 0.27667829394340515\n",
      "Epoch 597, Loss: 0.2766779065132141\n",
      "Epoch 598, Loss: 0.27667778730392456\n",
      "Epoch 599, Loss: 0.27667778730392456\n",
      "Epoch 600, Loss: 0.27667778730392456\n",
      "Epoch 601, Loss: 0.27667778730392456\n",
      "Epoch 602, Loss: 0.27667778730392456\n",
      "Epoch 603, Loss: 0.276678204536438\n",
      "Epoch 604, Loss: 0.27667978405952454\n",
      "Epoch 605, Loss: 0.27667784690856934\n",
      "Epoch 606, Loss: 0.27667784690856934\n",
      "Epoch 607, Loss: 0.27667784690856934\n",
      "Epoch 608, Loss: 0.27667781710624695\n",
      "Epoch 609, Loss: 0.27667781710624695\n",
      "Epoch 610, Loss: 0.27667781710624695\n",
      "Epoch 611, Loss: 0.27667781710624695\n",
      "Epoch 612, Loss: 0.27667781710624695\n",
      "Epoch 613, Loss: 0.27667781710624695\n",
      "Epoch 614, Loss: 0.27667781710624695\n",
      "Epoch 615, Loss: 0.27667781710624695\n",
      "Epoch 616, Loss: 0.27667781710624695\n",
      "Epoch 617, Loss: 0.27667781710624695\n",
      "Epoch 618, Loss: 0.27667778730392456\n",
      "Epoch 619, Loss: 0.27667781710624695\n",
      "Epoch 620, Loss: 0.2766779959201813\n",
      "Epoch 621, Loss: 0.27667781710624695\n",
      "Epoch 622, Loss: 0.2766779065132141\n",
      "Epoch 623, Loss: 0.27667808532714844\n",
      "Epoch 624, Loss: 0.27667778730392456\n",
      "Epoch 625, Loss: 0.27667781710624695\n",
      "Epoch 626, Loss: 0.27667781710624695\n",
      "Epoch 627, Loss: 0.27667781710624695\n",
      "Epoch 628, Loss: 0.2766779959201813\n",
      "Epoch 629, Loss: 0.27667781710624695\n",
      "Epoch 630, Loss: 0.27667781710624695\n",
      "Epoch 631, Loss: 0.27667781710624695\n",
      "Epoch 632, Loss: 0.27667781710624695\n",
      "Epoch 633, Loss: 0.27667781710624695\n",
      "Epoch 634, Loss: 0.27667781710624695\n",
      "Epoch 635, Loss: 0.27667781710624695\n",
      "Epoch 636, Loss: 0.2766778767108917\n",
      "Epoch 637, Loss: 0.27667781710624695\n",
      "Epoch 638, Loss: 0.27667781710624695\n",
      "Epoch 639, Loss: 0.27667781710624695\n",
      "Epoch 640, Loss: 0.2766779661178589\n",
      "Epoch 641, Loss: 0.27667781710624695\n",
      "Epoch 642, Loss: 0.27667778730392456\n",
      "Epoch 643, Loss: 0.27667778730392456\n",
      "Epoch 644, Loss: 0.27667778730392456\n",
      "Epoch 645, Loss: 0.2766779959201813\n",
      "Epoch 646, Loss: 0.27667802572250366\n",
      "Epoch 647, Loss: 0.27667778730392456\n",
      "Epoch 648, Loss: 0.27667778730392456\n",
      "Epoch 649, Loss: 0.27667778730392456\n",
      "Epoch 650, Loss: 0.27667778730392456\n",
      "Epoch 651, Loss: 0.2766779363155365\n",
      "Epoch 652, Loss: 0.27667778730392456\n",
      "Epoch 653, Loss: 0.27667808532714844\n",
      "Epoch 654, Loss: 0.27667778730392456\n",
      "Epoch 655, Loss: 0.27667778730392456\n",
      "Epoch 656, Loss: 0.27667778730392456\n",
      "Epoch 657, Loss: 0.27667778730392456\n",
      "Epoch 658, Loss: 0.2766783833503723\n",
      "Epoch 659, Loss: 0.27667781710624695\n",
      "Epoch 660, Loss: 0.27667781710624695\n",
      "Epoch 661, Loss: 0.27667778730392456\n",
      "Epoch 662, Loss: 0.27667778730392456\n",
      "Epoch 663, Loss: 0.27667778730392456\n",
      "Epoch 664, Loss: 0.27667778730392456\n",
      "Epoch 665, Loss: 0.27667778730392456\n",
      "Epoch 666, Loss: 0.27667778730392456\n",
      "Epoch 667, Loss: 0.27667778730392456\n",
      "Epoch 668, Loss: 0.27667778730392456\n",
      "Epoch 669, Loss: 0.27667778730392456\n",
      "Epoch 670, Loss: 0.27667778730392456\n",
      "Epoch 671, Loss: 0.27667778730392456\n",
      "Epoch 672, Loss: 0.27667778730392456\n",
      "Epoch 673, Loss: 0.27667784690856934\n",
      "Epoch 674, Loss: 0.27667778730392456\n",
      "Epoch 675, Loss: 0.27667778730392456\n",
      "Epoch 676, Loss: 0.27667778730392456\n",
      "Epoch 677, Loss: 0.27667778730392456\n",
      "Epoch 678, Loss: 0.27667778730392456\n",
      "Epoch 679, Loss: 0.27667778730392456\n",
      "Epoch 680, Loss: 0.2766779065132141\n",
      "Epoch 681, Loss: 0.27667778730392456\n",
      "Epoch 682, Loss: 0.27667778730392456\n",
      "Epoch 683, Loss: 0.27667778730392456\n",
      "Epoch 684, Loss: 0.27667778730392456\n",
      "Epoch 685, Loss: 0.27667778730392456\n",
      "Epoch 686, Loss: 0.27667778730392456\n",
      "Epoch 687, Loss: 0.27667778730392456\n",
      "Epoch 688, Loss: 0.27667778730392456\n",
      "Epoch 689, Loss: 0.27667778730392456\n",
      "Epoch 690, Loss: 0.27667778730392456\n",
      "Epoch 691, Loss: 0.27667778730392456\n",
      "Epoch 692, Loss: 0.27667778730392456\n",
      "Epoch 693, Loss: 0.27667778730392456\n",
      "Epoch 694, Loss: 0.27667778730392456\n",
      "Epoch 695, Loss: 0.27667781710624695\n",
      "Epoch 696, Loss: 0.27667778730392456\n",
      "Epoch 697, Loss: 0.27667778730392456\n",
      "Epoch 698, Loss: 0.27667778730392456\n",
      "Epoch 699, Loss: 0.27667778730392456\n",
      "Epoch 700, Loss: 0.27667778730392456\n",
      "Epoch 701, Loss: 0.27667778730392456\n",
      "Epoch 702, Loss: 0.27667778730392456\n",
      "Epoch 703, Loss: 0.27667778730392456\n",
      "Epoch 704, Loss: 0.27667778730392456\n",
      "Epoch 705, Loss: 0.27667778730392456\n",
      "Epoch 706, Loss: 0.27667778730392456\n",
      "Epoch 707, Loss: 0.27667778730392456\n",
      "Epoch 708, Loss: 0.27667778730392456\n",
      "Epoch 709, Loss: 0.27667778730392456\n",
      "Epoch 710, Loss: 0.27667778730392456\n",
      "Epoch 711, Loss: 0.27667778730392456\n",
      "Epoch 712, Loss: 0.27667778730392456\n",
      "Epoch 713, Loss: 0.27667778730392456\n",
      "Epoch 714, Loss: 0.27667778730392456\n",
      "Epoch 715, Loss: 0.27667778730392456\n",
      "Epoch 716, Loss: 0.27667778730392456\n",
      "Epoch 717, Loss: 0.27667778730392456\n",
      "Epoch 718, Loss: 0.27667778730392456\n",
      "Epoch 719, Loss: 0.27667778730392456\n",
      "Epoch 720, Loss: 0.27667778730392456\n",
      "Epoch 721, Loss: 0.27667778730392456\n",
      "Epoch 722, Loss: 0.27667778730392456\n",
      "Epoch 723, Loss: 0.27667778730392456\n",
      "Epoch 724, Loss: 0.27667778730392456\n",
      "Epoch 725, Loss: 0.27667778730392456\n",
      "Epoch 726, Loss: 0.27667778730392456\n",
      "Epoch 727, Loss: 0.27667778730392456\n",
      "Epoch 728, Loss: 0.27667778730392456\n",
      "Epoch 729, Loss: 0.27667778730392456\n",
      "Epoch 730, Loss: 0.27667778730392456\n",
      "Epoch 731, Loss: 0.27667778730392456\n",
      "Epoch 732, Loss: 0.27667778730392456\n",
      "Epoch 733, Loss: 0.27667778730392456\n",
      "Epoch 734, Loss: 0.27667778730392456\n",
      "Epoch 735, Loss: 0.27667778730392456\n",
      "Epoch 736, Loss: 0.27667778730392456\n",
      "Epoch 737, Loss: 0.27667778730392456\n",
      "Epoch 738, Loss: 0.27667778730392456\n",
      "Epoch 739, Loss: 0.27667778730392456\n",
      "Epoch 740, Loss: 0.27667778730392456\n",
      "Epoch 741, Loss: 0.27667778730392456\n",
      "Epoch 742, Loss: 0.27667778730392456\n",
      "Epoch 743, Loss: 0.27667778730392456\n",
      "Epoch 744, Loss: 0.27667778730392456\n",
      "Epoch 745, Loss: 0.27667778730392456\n",
      "Epoch 746, Loss: 0.27667778730392456\n",
      "Epoch 747, Loss: 0.27667778730392456\n",
      "Epoch 748, Loss: 0.27667778730392456\n",
      "Epoch 749, Loss: 0.27667778730392456\n",
      "Epoch 750, Loss: 0.27667778730392456\n",
      "Epoch 751, Loss: 0.27667778730392456\n",
      "Epoch 752, Loss: 0.27667778730392456\n",
      "Epoch 753, Loss: 0.27667778730392456\n",
      "Epoch 754, Loss: 0.27667778730392456\n",
      "Epoch 755, Loss: 0.27667778730392456\n",
      "Epoch 756, Loss: 0.27667778730392456\n",
      "Epoch 757, Loss: 0.27667778730392456\n",
      "Epoch 758, Loss: 0.27667778730392456\n",
      "Epoch 759, Loss: 0.27667778730392456\n",
      "Epoch 760, Loss: 0.27667778730392456\n",
      "Epoch 761, Loss: 0.27667778730392456\n",
      "Epoch 762, Loss: 0.27667778730392456\n",
      "Epoch 763, Loss: 0.27667778730392456\n",
      "Epoch 764, Loss: 0.27667778730392456\n",
      "Epoch 765, Loss: 0.27667778730392456\n",
      "Epoch 766, Loss: 0.27667778730392456\n",
      "Epoch 767, Loss: 0.27667778730392456\n",
      "Epoch 768, Loss: 0.27667778730392456\n",
      "Epoch 769, Loss: 0.27667778730392456\n",
      "Epoch 770, Loss: 0.27667778730392456\n",
      "Epoch 771, Loss: 0.27667778730392456\n",
      "Epoch 772, Loss: 0.27667778730392456\n",
      "Epoch 773, Loss: 0.27667778730392456\n",
      "Epoch 774, Loss: 0.27667778730392456\n",
      "Epoch 775, Loss: 0.27667778730392456\n",
      "Epoch 776, Loss: 0.27667778730392456\n",
      "Epoch 777, Loss: 0.27667778730392456\n",
      "Epoch 778, Loss: 0.27667778730392456\n",
      "Epoch 779, Loss: 0.27667778730392456\n",
      "Epoch 780, Loss: 0.27667778730392456\n",
      "Epoch 781, Loss: 0.27667778730392456\n",
      "Epoch 782, Loss: 0.27667778730392456\n",
      "Epoch 783, Loss: 0.27667778730392456\n",
      "Epoch 784, Loss: 0.27667778730392456\n",
      "Epoch 785, Loss: 0.27667778730392456\n",
      "Epoch 786, Loss: 0.27667778730392456\n",
      "Epoch 787, Loss: 0.27667778730392456\n",
      "Epoch 788, Loss: 0.27667778730392456\n",
      "Epoch 789, Loss: 0.27667778730392456\n",
      "Epoch 790, Loss: 0.27667778730392456\n",
      "Epoch 791, Loss: 0.27667778730392456\n",
      "Epoch 792, Loss: 0.27667778730392456\n",
      "Epoch 793, Loss: 0.27667778730392456\n",
      "Epoch 794, Loss: 0.27667778730392456\n",
      "Epoch 795, Loss: 0.27667778730392456\n",
      "Epoch 796, Loss: 0.27667778730392456\n",
      "Epoch 797, Loss: 0.27667778730392456\n",
      "Epoch 798, Loss: 0.27667778730392456\n",
      "Epoch 799, Loss: 0.27667778730392456\n",
      "Epoch 800, Loss: 0.27667778730392456\n",
      "Epoch 801, Loss: 0.27667778730392456\n",
      "Epoch 802, Loss: 0.27667778730392456\n",
      "Epoch 803, Loss: 0.27667778730392456\n",
      "Epoch 804, Loss: 0.27667778730392456\n",
      "Epoch 805, Loss: 0.27667781710624695\n",
      "Epoch 806, Loss: 0.27667778730392456\n",
      "Epoch 807, Loss: 0.27667778730392456\n",
      "Epoch 808, Loss: 0.27667778730392456\n",
      "Epoch 809, Loss: 0.27667778730392456\n",
      "Epoch 810, Loss: 0.27667778730392456\n",
      "Epoch 811, Loss: 0.27667778730392456\n",
      "Epoch 812, Loss: 0.27667778730392456\n",
      "Epoch 813, Loss: 0.27667778730392456\n",
      "Epoch 814, Loss: 0.27667778730392456\n",
      "Epoch 815, Loss: 0.27667778730392456\n",
      "Epoch 816, Loss: 0.27667778730392456\n",
      "Epoch 817, Loss: 0.27667778730392456\n",
      "Epoch 818, Loss: 0.27667778730392456\n",
      "Epoch 819, Loss: 0.27667778730392456\n",
      "Epoch 820, Loss: 0.27667778730392456\n",
      "Epoch 821, Loss: 0.27667778730392456\n",
      "Epoch 822, Loss: 0.27667778730392456\n",
      "Epoch 823, Loss: 0.27667778730392456\n",
      "Epoch 824, Loss: 0.27667778730392456\n",
      "Epoch 825, Loss: 0.27667778730392456\n",
      "Epoch 826, Loss: 0.27667778730392456\n",
      "Epoch 827, Loss: 0.27667778730392456\n",
      "Epoch 828, Loss: 0.27667778730392456\n",
      "Epoch 829, Loss: 0.27667778730392456\n",
      "Epoch 830, Loss: 0.27667778730392456\n",
      "Epoch 831, Loss: 0.27667778730392456\n",
      "Epoch 832, Loss: 0.27667778730392456\n",
      "Epoch 833, Loss: 0.27667778730392456\n",
      "Epoch 834, Loss: 0.27667778730392456\n",
      "Epoch 835, Loss: 0.27667778730392456\n",
      "Epoch 836, Loss: 0.27667778730392456\n",
      "Epoch 837, Loss: 0.27667778730392456\n",
      "Epoch 838, Loss: 0.27667778730392456\n",
      "Epoch 839, Loss: 0.27667778730392456\n",
      "Epoch 840, Loss: 0.27667778730392456\n",
      "Epoch 841, Loss: 0.27667778730392456\n",
      "Epoch 842, Loss: 0.27667778730392456\n",
      "Epoch 843, Loss: 0.27667778730392456\n",
      "Epoch 844, Loss: 0.27667778730392456\n",
      "Epoch 845, Loss: 0.27667778730392456\n",
      "Epoch 846, Loss: 0.27667778730392456\n",
      "Epoch 847, Loss: 0.27667778730392456\n",
      "Epoch 848, Loss: 0.27667778730392456\n",
      "Epoch 849, Loss: 0.27667778730392456\n",
      "Epoch 850, Loss: 0.27667778730392456\n",
      "Epoch 851, Loss: 0.27667778730392456\n",
      "Epoch 852, Loss: 0.27667778730392456\n",
      "Epoch 853, Loss: 0.27667778730392456\n",
      "Epoch 854, Loss: 0.27667778730392456\n",
      "Epoch 855, Loss: 0.27667778730392456\n",
      "Epoch 856, Loss: 0.27667778730392456\n",
      "Epoch 857, Loss: 0.27667778730392456\n",
      "Epoch 858, Loss: 0.27667778730392456\n",
      "Epoch 859, Loss: 0.27667778730392456\n",
      "Epoch 860, Loss: 0.27667778730392456\n",
      "Epoch 861, Loss: 0.27667778730392456\n",
      "Epoch 862, Loss: 0.27667778730392456\n",
      "Epoch 863, Loss: 0.27667778730392456\n",
      "Epoch 864, Loss: 0.27667778730392456\n",
      "Epoch 865, Loss: 0.27667778730392456\n",
      "Epoch 866, Loss: 0.27667778730392456\n",
      "Epoch 867, Loss: 0.27667778730392456\n",
      "Epoch 868, Loss: 0.27667778730392456\n",
      "Epoch 869, Loss: 0.27667778730392456\n",
      "Epoch 870, Loss: 0.27667778730392456\n",
      "Epoch 871, Loss: 0.27667778730392456\n",
      "Epoch 872, Loss: 0.27667778730392456\n",
      "Epoch 873, Loss: 0.27667778730392456\n",
      "Epoch 874, Loss: 0.27667778730392456\n",
      "Epoch 875, Loss: 0.27667778730392456\n",
      "Epoch 876, Loss: 0.27667778730392456\n",
      "Epoch 877, Loss: 0.27667778730392456\n",
      "Epoch 878, Loss: 0.27667778730392456\n",
      "Epoch 879, Loss: 0.27667778730392456\n",
      "Epoch 880, Loss: 0.27667778730392456\n",
      "Epoch 881, Loss: 0.27667778730392456\n",
      "Epoch 882, Loss: 0.27667778730392456\n",
      "Epoch 883, Loss: 0.27667778730392456\n",
      "Epoch 884, Loss: 0.27667778730392456\n",
      "Epoch 885, Loss: 0.27667778730392456\n",
      "Epoch 886, Loss: 0.27667778730392456\n",
      "Epoch 887, Loss: 0.27667778730392456\n",
      "Epoch 888, Loss: 0.27667778730392456\n",
      "Epoch 889, Loss: 0.27667778730392456\n",
      "Epoch 890, Loss: 0.27667778730392456\n",
      "Epoch 891, Loss: 0.27667778730392456\n",
      "Epoch 892, Loss: 0.27667778730392456\n",
      "Epoch 893, Loss: 0.27667778730392456\n",
      "Epoch 894, Loss: 0.27667778730392456\n",
      "Epoch 895, Loss: 0.27667778730392456\n",
      "Epoch 896, Loss: 0.27667778730392456\n",
      "Epoch 897, Loss: 0.27667778730392456\n",
      "Epoch 898, Loss: 0.27667778730392456\n",
      "Epoch 899, Loss: 0.27667778730392456\n",
      "Epoch 900, Loss: 0.27667778730392456\n",
      "Epoch 901, Loss: 0.27667778730392456\n",
      "Epoch 902, Loss: 0.27667778730392456\n",
      "Epoch 903, Loss: 0.27667778730392456\n",
      "Epoch 904, Loss: 0.27667778730392456\n",
      "Epoch 905, Loss: 0.27667778730392456\n",
      "Epoch 906, Loss: 0.27667778730392456\n",
      "Epoch 907, Loss: 0.27667778730392456\n",
      "Epoch 908, Loss: 0.27667778730392456\n",
      "Epoch 909, Loss: 0.27667778730392456\n",
      "Epoch 910, Loss: 0.27667778730392456\n",
      "Epoch 911, Loss: 0.27667778730392456\n",
      "Epoch 912, Loss: 0.27667778730392456\n",
      "Epoch 913, Loss: 0.27667778730392456\n",
      "Epoch 914, Loss: 0.27667778730392456\n",
      "Epoch 915, Loss: 0.27667778730392456\n",
      "Epoch 916, Loss: 0.27667778730392456\n",
      "Epoch 917, Loss: 0.27667778730392456\n",
      "Epoch 918, Loss: 0.27667778730392456\n",
      "Epoch 919, Loss: 0.27667778730392456\n",
      "Epoch 920, Loss: 0.27667778730392456\n",
      "Epoch 921, Loss: 0.27667778730392456\n",
      "Epoch 922, Loss: 0.27667778730392456\n",
      "Epoch 923, Loss: 0.27667778730392456\n",
      "Epoch 924, Loss: 0.27667778730392456\n",
      "Epoch 925, Loss: 0.27667778730392456\n",
      "Epoch 926, Loss: 0.27667778730392456\n",
      "Epoch 927, Loss: 0.27667778730392456\n",
      "Epoch 928, Loss: 0.27667778730392456\n",
      "Epoch 929, Loss: 0.27667778730392456\n",
      "Epoch 930, Loss: 0.27667778730392456\n",
      "Epoch 931, Loss: 0.27667778730392456\n",
      "Epoch 932, Loss: 0.27667778730392456\n",
      "Epoch 933, Loss: 0.27667778730392456\n",
      "Epoch 934, Loss: 0.27667778730392456\n",
      "Epoch 935, Loss: 0.27667778730392456\n",
      "Epoch 936, Loss: 0.27667778730392456\n",
      "Epoch 937, Loss: 0.27667778730392456\n",
      "Epoch 938, Loss: 0.27667778730392456\n",
      "Epoch 939, Loss: 0.27667778730392456\n",
      "Epoch 940, Loss: 0.27667778730392456\n",
      "Epoch 941, Loss: 0.27667778730392456\n",
      "Epoch 942, Loss: 0.27667778730392456\n",
      "Epoch 943, Loss: 0.27667778730392456\n",
      "Epoch 944, Loss: 0.27667778730392456\n",
      "Epoch 945, Loss: 0.27667778730392456\n",
      "Epoch 946, Loss: 0.27667778730392456\n",
      "Epoch 947, Loss: 0.27667778730392456\n",
      "Epoch 948, Loss: 0.27667778730392456\n",
      "Epoch 949, Loss: 0.27667778730392456\n",
      "Epoch 950, Loss: 0.27667778730392456\n",
      "Epoch 951, Loss: 0.27667778730392456\n",
      "Epoch 952, Loss: 0.27667778730392456\n",
      "Epoch 953, Loss: 0.27667778730392456\n",
      "Epoch 954, Loss: 0.27667778730392456\n",
      "Epoch 955, Loss: 0.27667778730392456\n",
      "Epoch 956, Loss: 0.27667778730392456\n",
      "Epoch 957, Loss: 0.27667778730392456\n",
      "Epoch 958, Loss: 0.27667778730392456\n",
      "Epoch 959, Loss: 0.27667778730392456\n",
      "Epoch 960, Loss: 0.27667778730392456\n",
      "Epoch 961, Loss: 0.27667778730392456\n",
      "Epoch 962, Loss: 0.27667778730392456\n",
      "Epoch 963, Loss: 0.27667778730392456\n",
      "Epoch 964, Loss: 0.27667778730392456\n",
      "Epoch 965, Loss: 0.27667778730392456\n",
      "Epoch 966, Loss: 0.27667778730392456\n",
      "Epoch 967, Loss: 0.27667778730392456\n",
      "Epoch 968, Loss: 0.27667778730392456\n",
      "Epoch 969, Loss: 0.27667778730392456\n",
      "Epoch 970, Loss: 0.27667778730392456\n",
      "Epoch 971, Loss: 0.27667778730392456\n",
      "Epoch 972, Loss: 0.27667778730392456\n",
      "Epoch 973, Loss: 0.27667778730392456\n",
      "Epoch 974, Loss: 0.27667778730392456\n",
      "Epoch 975, Loss: 0.27667778730392456\n",
      "Epoch 976, Loss: 0.27667778730392456\n",
      "Epoch 977, Loss: 0.27667778730392456\n",
      "Epoch 978, Loss: 0.27667778730392456\n",
      "Epoch 979, Loss: 0.27667778730392456\n",
      "Epoch 980, Loss: 0.27667778730392456\n",
      "Epoch 981, Loss: 0.27667778730392456\n",
      "Epoch 982, Loss: 0.27667778730392456\n",
      "Epoch 983, Loss: 0.27667778730392456\n",
      "Epoch 984, Loss: 0.27667778730392456\n",
      "Epoch 985, Loss: 0.27667778730392456\n",
      "Epoch 986, Loss: 0.27667778730392456\n",
      "Epoch 987, Loss: 0.27667778730392456\n",
      "Epoch 988, Loss: 0.27667778730392456\n",
      "Epoch 989, Loss: 0.27667778730392456\n",
      "Epoch 990, Loss: 0.27667778730392456\n",
      "Epoch 991, Loss: 0.27667778730392456\n",
      "Epoch 992, Loss: 0.27667778730392456\n",
      "Epoch 993, Loss: 0.27667778730392456\n",
      "Epoch 994, Loss: 0.27667778730392456\n",
      "Epoch 995, Loss: 0.27667778730392456\n",
      "Epoch 996, Loss: 0.27667778730392456\n",
      "Epoch 997, Loss: 0.27667778730392456\n",
      "Epoch 998, Loss: 0.27667778730392456\n",
      "Epoch 999, Loss: 0.27667778730392456\n",
      "Epoch 1000, Loss: 0.27667778730392456\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    " def __init__(self):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    # 인코더 구성\n",
    "    self.encoder = nn.Sequential(\n",
    "    nn.Linear(1326, 663), # 입력 차원 55에서 낮은 차수의 중간 차원 32로 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Linear(663, 331), # 중간 차원 32에서 특징 차원 16으로 더 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Linear(331, 165), # 중간 차원 32에서 특징 차원 16으로 더 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Linear(165, 82), # 중간 차원 32에서 특징 차원 16으로 더 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Linear(82, 41), # 중간 차원 32에서 특징 차원 16으로 더 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Linear(41, 20), # 중간 차원 32에서 특징 차원 16으로 더 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Linear(20, 10), # 중간 차원 32에서 특징 차원 16으로 더 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    )\n",
    "    # 디코더 구성\n",
    "    self.decoder = nn.Sequential(\n",
    "    nn.Linear(10, 20), # 입력 차원 55에서 낮은 차수의 중간 차원 32로 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Linear(20, 41), # 중간 차원 32에서 특징 차원 16으로 더 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Linear(41, 82), # 중간 차원 32에서 특징 차원 16으로 더 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Linear(82, 165), # 중간 차원 32에서 특징 차원 16으로 더 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Linear(165, 331), # 중간 차원 32에서 특징 차원 16으로 더 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Linear(331, 663), # 중간 차원 32에서 특징 차원 16으로 더 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Linear(663, 1326), # 중간 차원 32에서 특징 차원 16으로 더 압축\n",
    "    nn.ReLU(), # 활성화 함수 ReLU 사용\n",
    "    nn.Sigmoid() # 출력을 0과 1 사이로 조정\n",
    "    )\n",
    " \n",
    " def forward(self, x):\n",
    "    x = self.encoder(x) # 인코딩\n",
    "    x = self.decoder(x) # 디코딩\n",
    "    return x\n",
    "\n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss() # MSE: Mean Squared Error \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) # Adam optimizer\n",
    "\n",
    "for epoch in range(1000):\n",
    " for data in train_loader:\n",
    "    inputs = data[0]\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, inputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이상치 인덱스: [  2   5   7  10  13  17  24  25  28  29  30  38  39  40  42  46  56  57\n",
      "  59  65  66  68  69  70  72  74  75  77  79  81  83  90  93  94  98 101\n",
      " 103 107 111 114 117 125 126 127 129 131 135 136 139 140 144 145 146 148\n",
      " 149 154 157 160 162 166 167 169 175 179 185 187 191 193 196 201 207 208\n",
      " 217 228 229 233 234 239 241 242 243 244 246 248 251 258 264 265 269 276\n",
      " 278 279 283 289 291 292 294 303 306 312 315 317 319 320 322 324 332 333\n",
      " 338 347 349 359 360 363 378 380 381 382 384 385 387 394 398 399 400 402\n",
      " 412 418 419 420 422 423 428 432 434 436 437 438 440 444 448 451 454 455\n",
      " 462 466 468 478 487 489 490 492 495 496 497 500 503 504 507 511 518 526\n",
      " 528 535 538 546 549 550 551 556 560 561 562 563 573 576 591 592 593 594\n",
      " 600 607 608 615 618 620 621 623 632 633 635 640 645 649 651 652 654 665\n",
      " 666 668 673 675 679 683 686 691 694 698 700 701 702 704 711 714 716 717\n",
      " 718 719 720 721 723 725 726 729 733 734 737 738 739]\n",
      "이상치 수: 229\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGwCAYAAABPSaTdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8JElEQVR4nO3dfVxUdd7/8ffInYCANwgDQmaJZmhmWia23otaZtqdZTdqaFpmeqm5uu6Vdm0rpYbaupqZQXmT7bVF+UszMW9K3Xa9LU3XWzRRiDICEQLF8/vDda5GUWEcmDmn1/PxOA+c7/nOmc85Q8y77/ecMzbDMAwBAABYVA1PFwAAAFCVCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSfD1dgDc4d+6cTpw4oZCQENlsNk+XAwAAKsAwDJ06dUrR0dGqUePy4zeEHUknTpxQbGysp8sAAAAuOHbsmGJiYi67nrAjKSQkRNL5gxUaGurhai7jppuk7GwpKkr69789XQ0AAB5XUFCg2NhYx+f45RB2JMfUVWhoqPeGnSlTpMJCqVYtyVtrBADAA652CgphxyyeftrTFQAAYEpcjQUAACyNsAMAACyNaSyzyM6WysokH5/zJykDAMpVVlamM2fOeLoMuIGfn598fHyueTuEHbO4/Xbp+HGpQQMpK8vT1QCA1zEMQzk5Ofr55589XQrcqHbt2rLb7dd0HzzCDgDAEi4EnYiICAUFBXGTWJMzDENFRUXKzc2VJEVdw6wGYQcAYHplZWWOoFOvXj1PlwM3CQwMlCTl5uYqIiLC5SktTlAGAJjehXN0goKCPFwJ3O3Ce3ot52ERdgAAlsHUlfW44z0l7AAAAEsj7AAAAEsj7AAA4MWOHDkim82mnTt3Vuvrrl+/Xjab7Zov5bfZbProo48uu7469o+wAwCAh9hstisugwYN8nSJlsCl5wAAeEh2drbj3++//75efPFF7du3z9EWGBiovLy8Sm+3rKxMNptNNWowpiExsmMen38u7d59/icAwBLsdrtjCQsLk81mu6TtgsOHD6tz584KCgpSy5Yt9Y9//MOxLi0tTbVr19Ynn3yim2++WQEBATp69KhKS0s1fvx4NWjQQMHBwWrbtq3Wr1/veN7Ro0d17733qk6dOgoODlZ8fLxWrlzpVOO2bdvUpk0bBQUFKSEhwSmMSdK8efN04403yt/fX02bNtWiRYuuuM//+te/1KpVK9WsWVNt2rTRjh07ruEIVgwjO17g+gkrrtrnyCv3VEMlAGBBKSnnl6u57TZp+XLntj59pO3br/7cMWPOL1Vo0qRJmjFjhuLi4jRp0iQ9+uijOnjwoHx9z3+UFxUVKTk5WW+99Zbq1auniIgIDR48WEeOHNGyZcsUHR2t9PR09ezZU7t27VJcXJxGjBih0tJSffHFFwoODtaePXtUq1atS173tddeU/369TV8+HA99dRT2rRpkyQpPT1do0aN0qxZs9StWzd98sknGjx4sGJiYtS5c+dL9uH06dPq3bu3unTposWLFyszM1OjRo2q0uMmEXYAAFZXUHD+uwWvJjb20rYffqjYcwsKKl9XJY0bN0733HP+f3xfeuklxcfH6+DBg7rpppsknb/p3ty5c9WyZUtJ0qFDh/Tee+8pKytL0dHRjm2sWrVKqampmjp1qr777js98MADatGihSTphhtuuOR1//znP6tjx46SpAkTJuiee+7RL7/8opo1a2rGjBkaNGiQnn32WUnSmDFj9NVXX2nGjBnlhp0lS5aorKxMb7/9toKCghQfH6+srCw988wzbj5azgg7AABrCw09/yXKV1O/fvltFXluaGjl66qkW265xfHvC98TlZub6wg7/v7+Tn22b98uwzDUpEkTp+2UlJQ4vlLj+eef1zPPPKPVq1erW7dueuCBB5y2caXXve6667R37149/fTTTv3bt2+v2bNnl7sPe/fuVcuWLZ3udN2uXbuKHYBrQNgxi6VLpaIiKShIGjDA09UAgHlcyxTTxdNaHuTn5+f494W7Cp87d87RFhgY6HS34XPnzsnHx0fbtm275DulLkxVDRkyRD169NCKFSu0evVqJScn67XXXtPIkSMr/LoX3+HYMIzL3vXYMIyK7aybcYKyWYwfLw0dev4nAABX0apVK5WVlSk3N1eNGzd2Wux2u6NfbGyshg8frg8//FBjx47VggULKvwazZo108aNG53aNm/erGbNmpXb/+abb9bXX3+t4uJiR9tXX31VyT2rPMIOAAAW1KRJEz322GN68skn9eGHHyozM1NbtmzRq6++6rjiavTo0frss8+UmZmp7du3a+3atZcNKuV54YUXlJaWpjfeeEMHDhxQSkqKPvzwQ40bN67c/gMGDFCNGjWUlJSkPXv2aOXKlZoxY4Zb9vdKCDsAAFhUamqqnnzySY0dO1ZNmzZVnz599M9//lOx/zkZu6ysTCNGjFCzZs3Us2dPNW3aVHPnzq3w9vv27avZs2dr+vTpio+P1/z585WamqpOnTqV279WrVr6f//v/2nPnj1q1aqVJk2apFdffdUdu3pFNsNTE2hepKCgQGFhYcrPz1doNZxkdrEKXXq+eNj5KwIaNJCysqqhKgAwj19++UWZmZlq1KiRatas6ely4EZXem8r+vnNyA4AALA0wg4AALA0wg4AALA0wg4AwDI4DdV63PGeEnYAAKZ34cZ3RUVFHq4E7nbhPf31zQ0rizsom8WFG0D96kZQAIDzfHx8VLt2beXm5kqSgoKCLnsXX5iDYRgqKipSbm6uateufcldoCuDsGMWW7d6ugIA8GoX7gp8IfDAGmrXru10x2dXEHYAAJZgs9kUFRWliIgInTlzxtPlwA38/PyuaUTnAsIOAMBSfHx83PIBCevgBGUAAGBpjOyYxbBh0k8/SXXrSvPne7oaAABMg7BjFitW/N93YwEAgApjGgsAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFiaR8POvHnzdMsttyg0NFShoaFq166dPv30U8d6wzA0ZcoURUdHKzAwUJ06ddK3337rtI2SkhKNHDlS4eHhCg4OVp8+fZSVlVXduwIAALyUR8NOTEyMXnnlFW3dulVbt25Vly5ddN999zkCzbRp05SSkqI5c+Zoy5Ytstvt6t69u06dOuXYxujRo5Wenq5ly5Zp48aNKiwsVO/evVVWVuap3aoajz4qJSWd/wkAACrMZhiG4ekifq1u3bqaPn26nnrqKUVHR2v06NH6/e9/L+n8KE5kZKReffVVDRs2TPn5+apfv74WLVqk/v37S5JOnDih2NhYrVy5Uj169KjQaxYUFCgsLEz5+fkKDQ2tsn27nOsnrLhqnyOv3FMNlQAAYB4V/fz2mnN2ysrKtGzZMp0+fVrt2rVTZmamcnJylJiY6OgTEBCgjh07avPmzZKkbdu26cyZM059oqOj1bx5c0ef8pSUlKigoMBpAQAA1uTxsLNr1y7VqlVLAQEBGj58uNLT03XzzTcrJydHkhQZGenUPzIy0rEuJydH/v7+qlOnzmX7lCc5OVlhYWGOJTY21s17BQAAvIXHw07Tpk21c+dOffXVV3rmmWc0cOBA7dmzx7HeZrM59TcM45K2i12tz8SJE5Wfn+9Yjh07dm07AQAAvJbHw46/v78aN26sNm3aKDk5WS1bttTs2bNlt9sl6ZIRmtzcXMdoj91uV2lpqfLy8i7bpzwBAQGOK8AuLF7vppuk0NDzPwEAQIV5POxczDAMlZSUqFGjRrLb7crIyHCsKy0t1YYNG5SQkCBJat26tfz8/Jz6ZGdna/fu3Y4+llFYKJ06df4nAACoMF9Pvvgf/vAH9erVS7GxsTp16pSWLVum9evXa9WqVbLZbBo9erSmTp2quLg4xcXFaerUqQoKCtKAAQMkSWFhYUpKStLYsWNVr1491a1bV+PGjVOLFi3UrVs3T+4aAADwEh4NO99//72eeOIJZWdnKywsTLfccotWrVql7t27S5LGjx+v4uJiPfvss8rLy1Pbtm21evVqhYSEOLYxc+ZM+fr66uGHH1ZxcbG6du2qtLQ0+fj4eGq3AACAF/G6++x4ginus7N4mHT8uNSggcQdogEAMN99dgAAAKoCYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFiaRy89RyW88YZUXCwFBnq6EgAATIWwYxa9e3u6AgAATIlpLAAAYGmEHQAAYGlMY5nFtm1Saank7y+1bu3pagAAMA3Cjlncdx9fFwEAgAuYxgIAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJbGHZTNYu9eyTAkm83TlQAAYCqEHbMICfF0BQAAmBLTWAAAwNIIOwAAwNKYxjKLlBSpoEAKDZXGjPF0NQAAmAZhxyxSUqTjx6UGDQg7AABUAtNYAADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0ripoFncdpsUGyvVr+/pSgAAMBXCjlksX+7pCgAAMCWmsQAAgKURdgAAgKURdgAAgKVxzo5Z9Okj/fDD+ROUOX8HAIAKI+yYxfbt0vHjUoMGnq4EAABTYRoLAABYGmEHAABYmkfDTnJysm6//XaFhIQoIiJCffv21b59+5z6DBo0SDabzWm58847nfqUlJRo5MiRCg8PV3BwsPr06aOsrKzq3BUAAOClPBp2NmzYoBEjRuirr75SRkaGzp49q8TERJ0+fdqpX8+ePZWdne1YVq5c6bR+9OjRSk9P17Jly7Rx40YVFhaqd+/eKisrq87dAQAAXsijJyivWrXK6XFqaqoiIiK0bds2dejQwdEeEBAgu91e7jby8/O1cOFCLVq0SN26dZMkLV68WLGxsVqzZo169OhxyXNKSkpUUlLieFxQUOCO3QEAAF7Iq87Zyc/PlyTVrVvXqX39+vWKiIhQkyZNNHToUOXm5jrWbdu2TWfOnFFiYqKjLTo6Ws2bN9fmzZvLfZ3k5GSFhYU5ltjY2CrYGwAA4A28JuwYhqExY8borrvuUvPmzR3tvXr10pIlS7R27Vq99tpr2rJli7p06eIYmcnJyZG/v7/q1KnjtL3IyEjl5OSU+1oTJ05Ufn6+Yzl27FjV7RgAAPAor7nPznPPPadvvvlGGzdudGrv37+/49/NmzdXmzZt1LBhQ61YsUL333//ZbdnGIZsNlu56wICAhQQEOCewgEAgFfzirAzcuRILV++XF988YViYmKu2DcqKkoNGzbUgQMHJEl2u12lpaXKy8tzGt3Jzc1VQkJCldZdrcaMkQoKpNBQT1cCAICpeDTsGIahkSNHKj09XevXr1ejRo2u+pyTJ0/q2LFjioqKkiS1bt1afn5+ysjI0MMPPyxJys7O1u7duzVt2rQqrb9ajRnj6QoAADAlj4adESNGaOnSpfr4448VEhLiOMcmLCxMgYGBKiws1JQpU/TAAw8oKipKR44c0R/+8AeFh4erX79+jr5JSUkaO3as6tWrp7p162rcuHFq0aKF4+osAADw2+XRsDNv3jxJUqdOnZzaU1NTNWjQIPn4+GjXrl1699139fPPPysqKkqdO3fW+++/r5CQEEf/mTNnytfXVw8//LCKi4vVtWtXpaWlycfHpzp3BwAAeCGbYRiGp4vwtIKCAoWFhSk/P1+hHjgn5voJK67a58ikDpJhSDab9KugBwDAb1VFP7+95tJzXEWzZlJY2PmfAACgwgg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0jz63ViohI8/lkpLJX9/T1cCAICpEHbMonVrT1cAAIApMY0FAAAsjbADAAAsjWkss/jkE6m4WAoMlHr39nQ1AACYBmHHLIYPl44flxo0kLKyPF0NAACmwTQWAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMKOWdSqJYWEnP8JAAAqjDsom8W//+3pCgAAMCVGdgAAgKURdgAAgKURdgAAgKVxzo5ZvPCClJcn1akjTZ/u6WoAADANwo5ZvPeedPy41KABYQcAgEpgGgsAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaNxU0i3vukX76Sapb19OVAABgKoQds5g/39MVAABgSkxjAQAAS/No2ElOTtbtt9+ukJAQRUREqG/fvtq3b59TH8MwNGXKFEVHRyswMFCdOnXSt99+69SnpKREI0eOVHh4uIKDg9WnTx9lZWVV564AAAAv5VLYyczMdMuLb9iwQSNGjNBXX32ljIwMnT17VomJiTp9+rSjz7Rp05SSkqI5c+Zoy5Ytstvt6t69u06dOuXoM3r0aKWnp2vZsmXauHGjCgsL1bt3b5WVlbmlTgAAYF42wzCMyj7Jx8dHHTp0UFJSkh588EHVrFnTLcX88MMPioiI0IYNG9ShQwcZhqHo6GiNHj1av//97yWdH8WJjIzUq6++qmHDhik/P1/169fXokWL1L9/f0nSiRMnFBsbq5UrV6pHjx5Xfd2CggKFhYUpPz9foaGhbtmXyrh+woqr9jmyZrKUkyPZ7dLWrdVQFQAA3q2in98ujex8/fXXatWqlcaOHSu73a5hw4bpX//6l8vFXpCfny9JqvufK44yMzOVk5OjxMRER5+AgAB17NhRmzdvliRt27ZNZ86cceoTHR2t5s2bO/pcrKSkRAUFBU6L18vJkY4fP/8TAABUmEthp3nz5kpJSdHx48eVmpqqnJwc3XXXXYqPj1dKSop++OGHSm/TMAyNGTNGd911l5o3by5JyvnPB3tkZKRT38jISMe6nJwc+fv7q06dOpftc7Hk5GSFhYU5ltjY2ErXCwAAzOGaTlD29fVVv3799Le//U2vvvqqDh06pHHjxikmJkZPPvmksrOzK7yt5557Tt98843ee++9S9bZbDanx4ZhXNJ2sSv1mThxovLz8x3LsWPHKlwnAAAwl2sKO1u3btWzzz6rqKgopaSkaNy4cTp06JDWrl2r48eP67777qvQdkaOHKnly5dr3bp1iomJcbTb7XZJumSEJjc31zHaY7fbVVpaqry8vMv2uVhAQIBCQ0OdFgAAYE0uhZ2UlBS1aNFCCQkJOnHihN59910dPXpUL7/8sho1aqT27dtr/vz52r59+xW3YxiGnnvuOX344Ydau3atGjVq5LS+UaNGstvtysjIcLSVlpZqw4YNSkhIkCS1bt1afn5+Tn2ys7O1e/duRx8AAPDb5dIdlOfNm6ennnpKgwcPdoy+XOy6667TwoULr7idESNGaOnSpfr4448VEhLiGMEJCwtTYGCgbDabRo8eralTpyouLk5xcXGaOnWqgoKCNGDAAEffpKQkjR07VvXq1VPdunU1btw4tWjRQt26dXNl9wAAgIW4FHYOHDhw1T7+/v4aOHDgFfvMmzdPktSpUyen9tTUVA0aNEiSNH78eBUXF+vZZ59VXl6e2rZtq9WrVyskJMTRf+bMmfL19dXDDz+s4uJide3aVWlpafLx8ancjgEAAMtx6T47qampqlWrlh566CGn9v/93/9VUVHRVUOOtzHFfXYWDzt/6XmDBhJ3hwYAoGrvs/PKK68oPDz8kvaIiAhNnTrVlU0CAABUCZemsY4ePXrJycSS1LBhQ3333XfXXBTKMW2aVFQkBQV5uhIAAEzFpbATERGhb775Rtdff71T+9dff6169eq5oy5c7D8nZAMAgMpxaRrrkUce0fPPP69169aprKxMZWVlWrt2rUaNGqVHHnnE3TUCAAC4zKWRnZdffllHjx5V165d5et7fhPnzp3Tk08+yTk7AADAq7gUdvz9/fX+++/rT3/6k77++msFBgaqRYsWatiwobvrwwX79klnz0q+vlLTpp6uBgAA03Ap7FzQpEkTNWnSxF214Eq6duXScwAAXOBS2CkrK1NaWpo+//xz5ebm6ty5c07r165d65biAAAArpVLYWfUqFFKS0vTPffco+bNm1/1G8gBAAA8xaWws2zZMv3tb3/T3Xff7e56AAAA3MqlS8/9/f3VuHFjd9cCAADgdi6FnbFjx2r27Nly4Wu1AAAAqpVL01gbN27UunXr9Omnnyo+Pl5+fn5O6z/88EO3FAcAAHCtXAo7tWvXVr9+/dxdCwAAgNu5FHZSU1PdXQcAAECVcOmcHUk6e/as1qxZo/nz5+vUqVOSpBMnTqiwsNBtxQEAAFwrl0Z2jh49qp49e+q7775TSUmJunfvrpCQEE2bNk2//PKL3njjDXfXiS1bpLIyycfH05UAAGAqLo3sjBo1Sm3atFFeXp4CAwMd7f369dPnn3/utuLwK1FRUkzM+Z8AAKDCXL4aa9OmTfL393dqb9iwoY4fP+6WwgAAANzBpZGdc+fOqays7JL2rKwshYSEXHNRAAAA7uJS2OnevbtmzZrleGyz2VRYWKjJkyfzFRJV5c03pZSU8z8BAECF2QwXboN84sQJde7cWT4+Pjpw4IDatGmjAwcOKDw8XF988YUiIiKqotYqU1BQoLCwMOXn5ys0NLTaX//6CSuu2ufI4mHS8eNSgwZSVlY1VAUAgHer6Oe3S+fsREdHa+fOnXrvvfe0fft2nTt3TklJSXrsscecTlgGAADwNJfCjiQFBgbqqaee0lNPPeXOegAAANzKpbDz7rvvXnH9k08+6VIxAAAA7uZS2Bk1apTT4zNnzqioqEj+/v4KCgoi7AAAAK/h0tVYeXl5TkthYaH27dunu+66S++99567awQAAHCZy9+NdbG4uDi98sorl4z6AAAAeJLLJyiXx8fHRydOnHDnJk2vIpeVAwCAquNS2Fm+fLnTY8MwlJ2drTlz5qh9+/ZuKQwAAMAdXAo7ffv2dXpss9lUv359denSRa+99po76sLFmjSRwsKkyEhPVwIAgKm4FHbOnTvn7jpwNWvXeroCAABMyW0nKAMAAHgjl0Z2xowZU+G+KSkprrwEAACAW7gUdnbs2KHt27fr7Nmzatq0qSRp//798vHx0W233eboZ7PZ3FMlAACAi1wKO/fee69CQkL0zjvvqE6dOpLO32hw8ODB+t3vfqexY8e6tUhIeuwx6ccfpfBwackST1cDAIBp2AzDMCr7pAYNGmj16tWKj493at+9e7cSExNNd6+din5FvCvcdZ+dI4uHScePSw0aSFlZbtkmAABmVtHPb5dOUC4oKND3339/SXtubq5OnTrlyiYBAACqhEthp1+/fho8eLD+/ve/KysrS1lZWfr73/+upKQk3X///e6uEQAAwGUunbPzxhtvaNy4cXr88cd15syZ8xvy9VVSUpKmT5/u1gIBAACuhUthJygoSHPnztX06dN16NAhGYahxo0bKzg42N31AQAAXJNruqlgdna2srOz1aRJEwUHB8uFc50BAACqlEth5+TJk+ratauaNGmiu+++W9nZ2ZKkIUOGcNk5AADwKi6Fnf/6r/+Sn5+fvvvuOwUFBTna+/fvr1WrVlV4O1988YXuvfdeRUdHy2az6aOPPnJaP2jQINlsNqflzjvvdOpTUlKikSNHKjw8XMHBwerTp4+yuDQbAAD8h0thZ/Xq1Xr11VcVExPj1B4XF6ejR49WeDunT59Wy5YtNWfOnMv26dmzp2O6LDs7WytXrnRaP3r0aKWnp2vZsmXauHGjCgsL1bt3b5WVlVVupwAAgCW5dILy6dOnnUZ0Lvjxxx8VEBBQ4e306tVLvXr1umKfgIAA2e32ctfl5+dr4cKFWrRokbp16yZJWrx4sWJjY7VmzRr16NGj3OeVlJSopKTE8bigoKDCNXvM0KFSfr4UFubpSgAAMBWXRnY6dOigd9991/HYZrPp3Llzmj59ujp37uy24iRp/fr1ioiIUJMmTTR06FDl5uY61m3btk1nzpxRYmKioy06OlrNmzfX5s2bL7vN5ORkhYWFOZbY2Fi31lwlJk+WUlLO/wQAABXm0sjO9OnT1alTJ23dulWlpaUaP368vv32W/3000/atGmT24rr1auXHnroITVs2FCZmZn67//+b3Xp0kXbtm1TQECAcnJy5O/v7/h+rgsiIyOVk5Nz2e1OnDjR6ZvbCwoKzBF4AABApbkUdm6++WZ98803mjdvnnx8fHT69Gndf//9GjFihKKiotxWXP/+/R3/bt68udq0aaOGDRtqxYoVV7xTs2EYV/zG9YCAgEpNtwEAAPOqdNi5MG00f/58vfTSS1VR02VFRUWpYcOGOnDggCTJbrertLRUeXl5TqM7ubm5SkhIqNbaAACAd6r0OTt+fn7avXv3FUdOqsrJkyd17Ngxx+hR69at5efnp4yMDEef7Oxs7d6923phJyZGstnO/wQAABXm0gnKTz75pBYuXHjNL15YWKidO3dq586dkqTMzEzt3LlT3333nQoLCzVu3Dj94x//0JEjR7R+/Xrde++9Cg8PV79+/SRJYWFhSkpK0tixY/X5559rx44devzxx9WiRQvH1VkAAOC3zaVzdkpLS/XWW28pIyNDbdq0ueQ7sVJSUiq0na1btzpdvXXhpOGBAwdq3rx52rVrl9599139/PPPioqKUufOnfX+++8rJCTE8ZyZM2fK19dXDz/8sIqLi9W1a1elpaXJx8fHlV0DAAAWYzMq8YVWhw8f1vXXX6+uXbtefoM2m9auXeuW4qpLQUGBwsLClJ+fr9DQULdu+/oJK9yynSOLh0nHj0sNGkjcIRoAgAp/fldqZCcuLk7Z2dlat26dpPNXS73++uuKjIy8tmoBAACqSKXO2bl4EOjTTz/V6dOn3VoQAACAO7l0gvIFlZgBAwAA8IhKhZ0L3zx+cRsAAIC3qtQ5O4ZhaNCgQY67D//yyy8aPnz4JVdjffjhh+6rEAAA4BpUKuwMHDjQ6fHjjz/u1mIAAADcrVJhJzU1tarqwNUsXiyVlEh8pxcAAJXi0k0F4QGdOnm6AgAATOmarsYCAADwdoQdAABgaUxjmcX69f93zg5TWgAAVBhhxywef5zvxgIAwAVMYwEAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEvjDspmwV2TAQBwCSM7AADA0gg7AADA0gg7AADA0jhnxyxeeknKz5fCwqTJkz1dDQAApkHYMYsFC6Tjx6UGDQg7AABUAtNYAADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0ripoFl07Cj9+KMUHu7pSgAAMBXCjlksWeLpCgAAMCWmsQAAgKURdgAAgKURdgAAgKURdsyiSxcpPv78TwAAUGGcoGwW+/dLx49L+fmergQAAFPx6MjOF198oXvvvVfR0dGy2Wz66KOPnNYbhqEpU6YoOjpagYGB6tSpk7799lunPiUlJRo5cqTCw8MVHBysPn36KCsrqxr3AgAAeDOPhp3Tp0+rZcuWmjNnTrnrp02bppSUFM2ZM0dbtmyR3W5X9+7dderUKUef0aNHKz09XcuWLdPGjRtVWFio3r17q6ysrLp2AwAAeDGPTmP16tVLvXr1KnedYRiaNWuWJk2apPvvv1+S9M477ygyMlJLly7VsGHDlJ+fr4ULF2rRokXq1q2bJGnx4sWKjY3VmjVr1KNHj2rbFwAA4J289gTlzMxM5eTkKDEx0dEWEBCgjh07avPmzZKkbdu26cyZM059oqOj1bx5c0ef8pSUlKigoMBpAQAA1uS1YScnJ0eSFBkZ6dQeGRnpWJeTkyN/f3/VqVPnsn3Kk5ycrLCwMMcSGxvr5uoBAIC38Nqwc4HNZnN6bBjGJW0Xu1qfiRMnKj8/37EcO3bMLbUCAADv47Vhx263S9IlIzS5ubmO0R673a7S0lLl5eVdtk95AgICFBoa6rQAAABr8tqw06hRI9ntdmVkZDjaSktLtWHDBiUkJEiSWrduLT8/P6c+2dnZ2r17t6MPAAD4bfPo1ViFhYU6ePCg43FmZqZ27typunXr6rrrrtPo0aM1depUxcXFKS4uTlOnTlVQUJAGDBggSQoLC1NSUpLGjh2revXqqW7duho3bpxatGjhuDrLMl58USoslGrV8nQlAACYikfDztatW9W5c2fH4zFjxkiSBg4cqLS0NI0fP17FxcV69tlnlZeXp7Zt22r16tUKCQlxPGfmzJny9fXVww8/rOLiYnXt2lVpaWny8fGp9v2pUk8/7ekKAAAwJZthGIani/C0goIChYWFKT8/3+3n71w/YYVbtnPklXvcsh0AAKyiop/fXnvODgAAgDvwRaBmkZ0tlZVJPj5SVJSnqwEAwDQY2TGL22+XYmPP/wQAABVG2AEAAJZG2AEAAJbGOTsmkZ3/i6L+87PdFa7w4qotAACcMbIDAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjUvPTeKxR/4sn3NlKqthsW9zBwCgihF2TOJwvRhPlwAAgCkxjQUAACyNsAMAACyNaSyT6LNnvQLPlKjYL0DLb+7k6XIAADANwo5JTFyXqqjCk8quVY+wAwBAJTCNBQAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2bCprED7XqOP0EAAAVQ9gxiT4DZ3m6BAAATIlpLAAAYGmEHQAAYGmEHQAAYGmcs2MSU1fNUdgvp5RfM0R/6Pmcp8sBAMA0CDsm0fnQFkUVnlR2rXqeLgUAAFNhGgsAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaNxU0ieU3d1TYL4XKr1nL06UAAGAqhB2TSO78lKdLAADAlLx6GmvKlCmy2WxOi91ud6w3DENTpkxRdHS0AgMD1alTJ3377bcerBgAAHgbrw47khQfH6/s7GzHsmvXLse6adOmKSUlRXPmzNGWLVtkt9vVvXt3nTp1yoMVAwAAb+L101i+vr5OozkXGIahWbNmadKkSbr//vslSe+8844iIyO1dOlSDRs27LLbLCkpUUlJieNxQUGB+wsHAABewetHdg4cOKDo6Gg1atRIjzzyiA4fPixJyszMVE5OjhITEx19AwIC1LFjR23evPmK20xOTlZYWJhjiY2NrdJ9cIfPFwzXrpkP6fMFwz1dCgAApuLVYadt27Z699139dlnn2nBggXKyclRQkKCTp48qZycHElSZGSk03MiIyMd6y5n4sSJys/PdyzHjh2rsn1wl6DSYoWUFiuotNjTpQAAYCpePY3Vq1cvx79btGihdu3a6cYbb9Q777yjO++8U5Jks9mcnmMYxiVtFwsICFBAQID7CwYAAF7Hq0d2LhYcHKwWLVrowIEDjvN4Lh7Fyc3NvWS0BwAA/HaZKuyUlJRo7969ioqKUqNGjWS325WRkeFYX1paqg0bNighIcGDVQIAAG/i1dNY48aN07333qvrrrtOubm5evnll1VQUKCBAwfKZrNp9OjRmjp1quLi4hQXF6epU6cqKChIAwYM8HTpAADAS3h12MnKytKjjz6qH3/8UfXr19edd96pr776Sg0bNpQkjR8/XsXFxXr22WeVl5entm3bavXq1QoJCfFw5QAAwFt4ddhZtmzZFdfbbDZNmTJFU6ZMqZ6CAACA6ZjqnB0AAIDKIuwAAABL8+ppLPyfST1GqObZUv3i6+/pUgAAMBXCjkmsbXyHp0sAAMCUmMYCAACWRtgBAACWxjSWSTTPOSj/sjMq9fHTbntjT5cDAIBpEHZMYsEHf1JU4Ull16qndiPe8XQ5AACYBtNYAADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0rj0HFXq+gkrrtrnyCv3VEMlAIDfKkZ2AACApRF2AACApTGNZRLdhsyTTZLh6UIAADAZwo5JnA4I8nQJAACYEtNYAADA0gg7AADA0pjGMomkf6UrpLRIp/yDtPCOfp4uBwAA0yDsmMSQLR8pqvCksmvVI+wAAFAJTGMBAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABL46aCJrHbfqOyi8J1MijM06UAAGAqhB2TGPrAi54u4RLXT1hRbds58so9bnktAMBvD9NYAADA0hjZQbncNWoDAICnEXYAFzD1BgDmQdgxiQUf/I/qFeXrZFCYV56/AwCAtyLsmETznEOKKjyp7Fr1PF2K12K0BQBQHsKOxfCBf+04XwkArIWwAwAWxv8AAYQdmASjLcCl+O8CqBjCDn5T+HAAgN8ey9xUcO7cuWrUqJFq1qyp1q1b68svv/R0SQAAwAtYYmTn/fff1+jRozV37ly1b99e8+fPV69evbRnzx5dd911ni4PAJxwHs1vU3W+7/yOObNE2ElJSVFSUpKGDBkiSZo1a5Y+++wzzZs3T8nJyR6uzvswlVM9rPqHjT+i1YP/Ts2F98u7mT7slJaWatu2bZowYYJTe2JiojZv3lzuc0pKSlRSUuJ4nJ+fL0kqKChwe33nSorcsp1TxjkF/+enu7YJz6vI71zzyZ+55bWu+6//dct2qvO1dr/Uwy3bqU7uer+qU3W+XxU5PtX5vnvb+2XVvwlV9Z5eOF6GYVy5o2Fyx48fNyQZmzZtcmr/85//bDRp0qTc50yePNmQxMLCwsLCwmKB5dixY1fMCqYf2bnAZrM5PTYM45K2CyZOnKgxY8Y4Hp87d04//fST6tWrd9nnmE1BQYFiY2N17NgxhYaGerocU+NYug/H0n04lu7DsXSf6j6WhmHo1KlTio6OvmI/04ed8PBw+fj4KCcnx6k9NzdXkZGR5T4nICBAAQEBTm21a9euqhI9KjQ0lP943YRj6T4cS/fhWLoPx9J9qvNYhoWFXbWP6S899/f3V+vWrZWRkeHUnpGRoYSEBA9VBQAAvIXpR3YkacyYMXriiSfUpk0btWvXTm+++aa+++47DR8+3NOlAQAAD7NE2Onfv79Onjyp//mf/1F2draaN2+ulStXqmHDhp4uzWMCAgI0efLkS6brUHkcS/fhWLoPx9J9OJbu463H0mYYV7teCwAAwLxMf84OAADAlRB2AACApRF2AACApRF2AACApRF2TGzu3Llq1KiRatasqdatW+vLL7+8bN+NGzeqffv2qlevngIDA3XTTTdp5syZ1Vitd6vMsfy1TZs2ydfXV7feemvVFmgilTmW69evl81mu2T597//XY0Ve6/K/l6WlJRo0qRJatiwoQICAnTjjTfq7bffrqZqvVtljuWgQYPK/b2Mj4+vxoq9V2V/L5csWaKWLVsqKChIUVFRGjx4sE6ePFlN1f6HW76gCtVu2bJlhp+fn7FgwQJjz549xqhRo4zg4GDj6NGj5fbfvn27sXTpUmP37t1GZmamsWjRIiMoKMiYP39+NVfufSp7LC/4+eefjRtuuMFITEw0WrZsWT3FernKHst169YZkox9+/YZ2dnZjuXs2bPVXLn3ceX3sk+fPkbbtm2NjIwMIzMz0/jnP/95yfcG/hZV9lj+/PPPTr+Px44dM+rWrWtMnjy5egv3QpU9ll9++aVRo0YNY/bs2cbhw4eNL7/80oiPjzf69u1brXUTdkzqjjvuMIYPH+7UdtNNNxkTJkyo8Db69etnPP744+4uzXRcPZb9+/c3/vjHPxqTJ08m7PxHZY/lhbCTl5dXDdWZS2WP5aeffmqEhYUZJ0+erI7yTOVa/16mp6cbNpvNOHLkSFWUZyqVPZbTp083brjhBqe2119/3YiJiamyGsvDNJYJlZaWatu2bUpMTHRqT0xM1ObNmyu0jR07dmjz5s3q2LFjVZRoGq4ey9TUVB06dEiTJ0+u6hJN41p+L1u1aqWoqCh17dpV69atq8oyTcGVY7l8+XK1adNG06ZNU4MGDdSkSRONGzdOxcXF1VGy13LH38uFCxeqW7duv+kb1UquHcuEhARlZWVp5cqVMgxD33//vf7+97/rnnvuqY6SHSxxB+Xfmh9//FFlZWWXfNFpZGTkJV+IerGYmBj98MMPOnv2rKZMmaIhQ4ZUZalez5VjeeDAAU2YMEFffvmlfH35T+gCV45lVFSU3nzzTbVu3VolJSVatGiRunbtqvXr16tDhw7VUbZXcuVYHj58WBs3blTNmjWVnp6uH3/8Uc8++6x++umn3/R5O9fy91KSsrOz9emnn2rp0qVVVaJpuHIsExIStGTJEvXv31+//PKLzp49qz59+ugvf/lLdZTswF9qE7PZbE6PDcO4pO1iX375pQoLC/XVV19pwoQJaty4sR599NGqLNMUKnosy8rKNGDAAL300ktq0qRJdZVnKpX5vWzatKmaNm3qeNyuXTsdO3ZMM2bM+E2HnQsqcyzPnTsnm82mJUuWOL4FOiUlRQ8++KD++te/KjAwsMrr9Wau/L2UpLS0NNWuXVt9+/atosrMpzLHcs+ePXr++ef14osvqkePHsrOztYLL7yg4cOHa+HChdVRriTCjimFh4fLx8fnkiSdm5t7SeK+WKNGjSRJLVq00Pfff68pU6b8psNOZY/lqVOntHXrVu3YsUPPPfecpPMfMoZhyNfXV6tXr1aXLl2qpXZvcy2/l7925513avHixe4uz1RcOZZRUVFq0KCBI+hIUrNmzWQYhrKyshQXF1elNXura/m9NAxDb7/9tp544gn5+/tXZZmm4MqxTE5OVvv27fXCCy9Ikm655RYFBwfrd7/7nV5++WVFRUVVed0Sl56bkr+/v1q3bq2MjAyn9oyMDCUkJFR4O4ZhqKSkxN3lmUplj2VoaKh27dqlnTt3Opbhw4eradOm2rlzp9q2bVtdpXsdd/1e7tixo9r+AHorV45l+/btdeLECRUWFjra9u/frxo1aigmJqZK6/Vm1/J7uWHDBh08eFBJSUlVWaJpuHIsi4qKVKOGc9Tw8fGRdP4zqNpU6+nQcJsLl/8tXLjQ2LNnjzF69GgjODjYcbXAhAkTjCeeeMLRf86cOcby5cuN/fv3G/v37zfefvttIzQ01Jg0aZKndsFrVPZYXoyrsf5PZY/lzJkzjfT0dGP//v3G7t27jQkTJhiSjA8++MBTu+A1KnssT506ZcTExBgPPvig8e233xobNmww4uLijCFDhnhqF7yGq/+NP/7440bbtm2ru1yvVtljmZqaavj6+hpz5841Dh06ZGzcuNFo06aNcccdd1Rr3YQdE/vrX/9qNGzY0PD39zduu+02Y8OGDY51AwcONDp27Oh4/Prrrxvx8fFGUFCQERoaarRq1cqYO3euUVZW5oHKvU9ljuXFCDvOKnMsX331VePGG280atasadSpU8e46667jBUrVnigau9U2d/LvXv3Gt26dTMCAwONmJgYY8yYMUZRUVE1V+2dKnssf/75ZyMwMNB48803q7lS71fZY/n6668bN998sxEYGGhERUUZjz32mJGVlVWtNdsMozrHkQAAAKoX5+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAQDlsNps++ugjT5cBwA0IO4CFDBo0SDabTTabTb6+vrruuuv0zDPPKC8vz9OlVdiRI0dks9m0c+fOanm9KVOm6NZbb72kPTs7W7169arS105LS3O8X79eatasWaWvC/zW+Hq6AADu1bNnT6Wmpurs2bPas2ePnnrqKf3888967733PF2aW5WWlsrf37/Ktm+326ts278WGhqqffv2ObXZbLbL9i9vvw3DUFlZmXx9K/cn3dXnAWbDyA5gMQEBAbLb7YqJiVFiYqL69++v1atXO/VJTU1Vs2bNVLNmTd10002aO3eu0/qsrCw98sgjqlu3roKDg9WmTRv985//dKyfN2+ebrzxRvn7+6tp06ZatGiR0/NtNpveeust9evXT0FBQYqLi9Py5csd6/Py8vTYY4+pfv36CgwMVFxcnFJTUyVJjRo1kiS1atVKNptNnTp1knR+1Kpv375KTk5WdHS0mjRp4niti6ebateurbS0tKvuT1paml566SV9/fXXjlGVC8+7eLu7du1Sly5dFBgYqHr16unpp59WYWGhY/2F+mbMmKGoqCjVq1dPI0aM0JkzZ674ftlsNtntdqclMjLSsb5Tp0567rnnNGbMGIWHh6t79+5av369bDabPvvsM7Vp00YBAQH68ssvVVJSoueff14RERGqWbOm7rrrLm3ZssWxrcs9D7A64jxgYYcPH9aqVavk5+fnaFuwYIEmT56sOXPmqFWrVtqxY4eGDh2q4OBgDRw4UIWFherYsaMaNGig5cuXy263a/v27Tp37pwkKT09XaNGjdKsWbPUrVs3ffLJJxo8eLBiYmLUuXNnx+u89NJLmjZtmqZPn66//OUveuyxx3T06FHVrVtX//3f/609e/bo008/VXh4uA4ePKji4mJJ0r/+9S/dcccdWrNmjeLj451GMT7//HOFhoYqIyNDFf0O4yvtT//+/bV7926tWrVKa9askSSFhYVdso2ioiL17NlTd955p7Zs2aLc3FwNGTJEzz33nFOoWrdunaKiorRu3TodPHhQ/fv316233qqhQ4dW/E0rxzvvvKNnnnlGmzZtkmEYysnJkSSNHz9eM2bM0A033KDatWtr/Pjx+uCDD/TOO++oYcOGmjZtmnr06KGDBw+qbt26ju1d/DzA8qr1O9YBVKmBAwcaPj4+RnBwsFGzZk1DkiHJSElJcfSJjY01li5d6vS8P/3pT0a7du0MwzCM+fPnGyEhIcbJkyfLfY2EhARj6NChTm0PPfSQcffddzseSzL++Mc/Oh4XFhYaNpvN+PTTTw3DMIx7773XGDx4cLnbz8zMNCQZO3bsuGTfIiMjjZKSEqd2SUZ6erpTW1hYmJGamlqh/Zk8ebLRsmXLS9p/vd0333zTqFOnjlFYWOhYv2LFCqNGjRpGTk6Oo76GDRsaZ8+edfR56KGHjP79+5f7uoZhGKmpqYYkIzg42Gnp3r27o0/Hjh2NW2+91el569atMyQZH330kaOtsLDQ8PPzM5YsWeJoKy0tNaKjo41p06Zd9nnAbwEjO4DFdO7cWfPmzVNRUZHeeust7d+/XyNHjpQk/fDDDzp27JiSkpKcRhvOnj3rGNHYuXOnWrVq5TQS8Gt79+7V008/7dTWvn17zZ4926ntlltucfw7ODhYISEhys3NlSQ988wzeuCBB7R9+3YlJiaqb9++SkhIuOq+tWjRotLn6Vxtfypi7969atmypYKDgx1t7du317lz57Rv3z7HtFN8fLx8fHwcfaKiorRr164rbjskJETbt293agsMDHR63KZNm3Kf++v2Q4cO6cyZM2rfvr2jzc/PT3fccYf27t1boe0BVkXYASwmODhYjRs3liS9/vrr6ty5s1566SX96U9/ckxFLViwQG3btnV63oUP6Ys/aMtz8Qm0hmFc0vbrqbMLz7nw+r169dLRo0e1YsUKrVmzRl27dtWIESM0Y8aMq+5bebUYF01p/fo8mYrsz9WUt3+/fv0LrrTPl1OjRg3H+3U55e33xe0XjkFF3pvLbQ+wKk5QBixu8uTJmjFjhk6cOKHIyEg1aNBAhw8fVuPGjZ2WCycG33LLLdq5c6d++umncrfXrFkzbdy40alt8+bNatasWaXqql+/vgYNGqTFixdr1qxZevPNNyXJMXJTVlZW4e1kZ2c7Hh84cEBFRUWOx1fbH39//6u+1s0336ydO3fq9OnTjrZNmzapRo0ajhOlPa1x48by9/d3em/OnDmjrVu3Vvq9AayGsANYXKdOnRQfH6+pU6dKOn9fmeTkZM2ePVv79+/Xrl27lJqaqpSUFEnSo48+Krvdrr59+2rTpk06fPiwPvjgA/3jH/+QJL3wwgtKS0vTG2+8oQMHDiglJUUffvihxo0bV+GaXnzxRX388cc6ePCgvv32W33yySeOD+SIiAgFBgZq1apV+v7775Wfn3/FbXXp0kVz5szR9u3btXXrVg0fPtxphOVq+3P99dcrMzNTO3fu1I8//qiSkpJLXuOxxx5TzZo1NXDgQO3evVvr1q3TyJEj9cQTTzhdOeUK4z8nHF+8XG1E6GLBwcF65pln9MILL2jVqlXas2ePhg4dqqKiIiUlJV1TjYDZEXaA34AxY8ZowYIFOnbsmIYMGaK33npLaWlpatGihTp27Ki0tDTHyI6/v79Wr16tiIgI3X333WrRooVeeeUVxzRX3759NXv2bE2fPl3x8fGaP3++UlNTHZeIV4S/v78mTpyoW265RR06dJCPj4+WLVsmSfL19dXrr7+u+fPnKzo6Wvfdd98Vt/Xaa68pNjZWHTp00IABAzRu3DgFBQU5vdaV9ueBBx5Qz5491blzZ9WvX7/c+xEFBQXps88+008//aTbb79dDz74oLp27ao5c+ZUeJ8vp6CgQFFRUZcsF85vqoxXXnlFDzzwgJ544gnddtttOnjwoD777DPVqVPnmusEzMxmXDzZDQAAYCGM7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEv7/wEVfrP0m8tgAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()  # 모델을 평가 모드로 전환\n",
    "with torch.no_grad():  # 기울기 계산 비활성화\n",
    "    reconstructed = model(X_test_tensor)  # 테스트 데이터 복원\n",
    "\n",
    "# 2. 재구성 오차 계산 (Mean Squared Error)\n",
    "reconstruction_errors = torch.mean((X_test_tensor - reconstructed) ** 2, dim=1)\n",
    "\n",
    "# 3. NumPy 배열로 변환\n",
    "errors = reconstruction_errors.detach().cpu().numpy()\n",
    "\n",
    "# 4. 임계값 설정 (평균 + 3 * 표준편차 사용)\n",
    "threshold = np.mean(errors) -0.024 #+ 10 #- 0.001 * np.std(errors)\n",
    "\n",
    "# 5. 이상치 탐지: 오차가 임계값보다 큰 데이터 찾기\n",
    "outliers = np.where(errors > threshold)[0]\n",
    "\n",
    "# 6. 결과 출력\n",
    "print(f\"이상치 인덱스: {outliers}\")\n",
    "print(f\"이상치 수: {len(outliers)}\")\n",
    "\n",
    "# 7. 재구성 오차 히스토그램 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(errors, bins=50)\n",
    "plt.axvline(threshold, color='r', linestyle='dashed', linewidth=2, label='Threshold')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27889016, 0.26944947, 0.2863557 , 0.27725947, 0.27344394,\n",
       "       0.34381536, 0.2703378 , 0.34506616, 0.2779378 , 0.27991346,\n",
       "       0.40837985, 0.27276793, 0.27708542, 0.29199442, 0.2832894 ,\n",
       "       0.27960405, 0.28358898, 0.30388847, 0.2709495 , 0.277924  ,\n",
       "       0.27991068, 0.28338113, 0.2829683 , 0.26441386, 0.7535025 ,\n",
       "       0.28889987, 0.27092078, 0.2848168 , 0.38679978, 0.37483102,\n",
       "       0.35330433, 0.27932698, 0.28090075, 0.270511  , 0.27068564,\n",
       "       0.27553833, 0.27735472, 0.28011596, 0.29440594, 0.29622784,\n",
       "       0.48519176, 0.27483946, 0.3250225 , 0.27353206, 0.2767649 ,\n",
       "       0.282243  , 0.28560716, 0.27745456, 0.27341706, 0.26788023,\n",
       "       0.27107903, 0.27656874, 0.27553934, 0.28386664, 0.26731098,\n",
       "       0.27929813, 0.6138813 , 0.6032356 , 0.27674666, 0.2921795 ,\n",
       "       0.2817529 , 0.2680391 , 0.28497416, 0.27004007, 0.2805503 ,\n",
       "       0.7680831 , 0.287219  , 0.27916452, 0.2931224 , 0.2944475 ,\n",
       "       0.2881938 , 0.27816486, 0.38090706, 0.28391567, 0.34928152,\n",
       "       0.59484625, 0.27137014, 0.29089418, 0.28037867, 0.28635398,\n",
       "       0.27812478, 0.31216815, 0.28109497, 0.32406574, 0.27407065,\n",
       "       0.26920956, 0.27091804, 0.2770413 , 0.27372032, 0.2807721 ,\n",
       "       0.36479047, 0.28128034, 0.26904336, 0.3942828 , 0.332162  ,\n",
       "       0.27896821, 0.27771163, 0.279422  , 0.2898457 , 0.26797035,\n",
       "       0.27481306, 0.2903242 , 0.2748428 , 0.40385434, 0.27308875,\n",
       "       0.26748508, 0.274755  , 0.48164353, 0.27287167, 0.28219754,\n",
       "       0.27069563, 0.29319978, 0.2744438 , 0.28027692, 0.33901912,\n",
       "       0.26708812, 0.2701631 , 0.2868051 , 0.277914  , 0.27939865,\n",
       "       0.2688589 , 0.2785933 , 0.2679627 , 0.27427664, 0.27324742,\n",
       "       0.4469946 , 0.63815486, 0.32297406, 0.27779582, 0.34136042,\n",
       "       0.26656637, 0.34428358, 0.27672866, 0.2717116 , 0.2699892 ,\n",
       "       0.28608546, 0.5034131 , 0.27465746, 0.2727327 , 0.3508409 ,\n",
       "       0.7794769 , 0.27885127, 0.27325043, 0.2690683 , 0.33859792,\n",
       "       0.40424368, 0.28660917, 0.2803202 , 0.29448724, 0.6096043 ,\n",
       "       0.28312778, 0.27643436, 0.27951944, 0.28402632, 0.2874427 ,\n",
       "       0.28087774, 0.27683854, 0.36248088, 0.27722207, 0.27212203,\n",
       "       0.3240107 , 0.2743587 , 0.28724268, 0.2692316 , 0.27217072,\n",
       "       0.2748736 , 0.35164705, 0.2916539 , 0.2746678 , 0.32888117,\n",
       "       0.28252703, 0.27280277, 0.27202806, 0.2844949 , 0.28450757,\n",
       "       0.29114696, 0.27456513, 0.28064457, 0.28061256, 0.3964004 ,\n",
       "       0.28059977, 0.2748415 , 0.28071344, 0.2697675 , 0.26986685,\n",
       "       0.28537244, 0.278488  , 0.4001569 , 0.2729442 , 0.27849996,\n",
       "       0.27332836, 0.3420805 , 0.270583  , 0.61422783, 0.27523583,\n",
       "       0.28312415, 0.28921652, 0.26698503, 0.27265003, 0.2791578 ,\n",
       "       0.27574295, 0.3949087 , 0.2673189 , 0.2838778 , 0.26567692,\n",
       "       0.27606165, 0.2723758 , 0.46528563, 0.35102263, 0.27797022,\n",
       "       0.2805516 , 0.2697579 , 0.27632684, 0.2772664 , 0.2804814 ,\n",
       "       0.28043458, 0.28094557, 0.36162743, 0.27182135, 0.27320525,\n",
       "       0.28215784, 0.2750225 , 0.27489614, 0.27290452, 0.28459767,\n",
       "       0.27944812, 0.27696487, 0.2747671 , 0.5902451 , 0.7320589 ,\n",
       "       0.27884796, 0.27557066, 0.2741176 , 0.41102704, 0.29152712,\n",
       "       0.28458554, 0.27992004, 0.2809495 , 0.27808997, 0.29179552,\n",
       "       0.28078294, 0.3793238 , 0.7148005 , 0.41281447, 0.6000428 ,\n",
       "       0.27001908, 0.28532648, 0.27983534, 0.38596097, 0.27195054,\n",
       "       0.28352332, 0.34725377, 0.27929717, 0.27640358, 0.28333324,\n",
       "       0.2781465 , 0.2766501 , 0.2791709 , 0.33495495, 0.2742668 ,\n",
       "       0.28040886, 0.276669  , 0.2698625 , 0.2722167 , 0.28666455,\n",
       "       0.29337344, 0.27333224, 0.26731908, 0.28425482, 0.4082277 ,\n",
       "       0.282924  , 0.27121568, 0.28005323, 0.2821553 , 0.28436947,\n",
       "       0.27889904, 0.2860219 , 0.28493497, 0.3643383 , 0.35740194,\n",
       "       0.27754012, 0.276927  , 0.2698355 , 0.3359966 , 0.2775255 ,\n",
       "       0.27708176, 0.27847907, 0.2810137 , 0.26941693, 0.34824878,\n",
       "       0.26674002, 0.29030788, 0.34359664, 0.2783246 , 0.6280365 ,\n",
       "       0.2696715 , 0.28252694, 0.28183386, 0.26362395, 0.26774424,\n",
       "       0.2651435 , 0.2750461 , 0.28047946, 0.30475977, 0.27663395,\n",
       "       0.2735447 , 0.5812943 , 0.27417192, 0.26913887, 0.27976927,\n",
       "       0.2736955 , 0.27652755, 0.28739712, 0.279436  , 0.279121  ,\n",
       "       0.29025257, 0.27200082, 0.34644085, 0.28082323, 0.357134  ,\n",
       "       0.29331678, 0.26837862, 0.39367312, 0.28060666, 0.28576005,\n",
       "       0.27875772, 0.28171945, 0.27846166, 0.26807153, 0.2662614 ,\n",
       "       0.27936643, 0.27623966, 0.2868674 , 0.28604183, 0.27229977,\n",
       "       0.27356377, 0.27825484, 0.28065065, 0.790167  , 0.27958286,\n",
       "       0.2712891 , 0.26534232, 0.27673203, 0.28037035, 0.28104275,\n",
       "       0.27278548, 0.27806365, 0.3467529 , 0.27848846, 0.32587323,\n",
       "       0.27164346, 0.26697809, 0.28397685, 0.28029644, 0.27514914,\n",
       "       0.27145106, 0.270033  , 0.28408584, 0.27314138, 0.3887331 ,\n",
       "       0.28633553, 0.26841104, 0.27614745, 0.34648737, 0.274816  ,\n",
       "       0.2689988 , 0.26855475, 0.27063406, 0.26837176, 0.28038073,\n",
       "       0.2756016 , 0.27859372, 0.2747213 , 0.27019662, 0.27153045,\n",
       "       0.27931982, 0.27558276, 0.28201562, 0.34074718, 0.2817896 ,\n",
       "       0.3271516 , 0.28592458, 0.34491938, 0.26981196, 0.7309954 ,\n",
       "       0.3269415 , 0.2831936 , 0.28950188, 0.2722967 , 0.27814287,\n",
       "       0.27365482, 0.26613423, 0.2781061 , 0.2835302 , 0.29992148,\n",
       "       0.2702767 , 0.27581033, 0.2832814 , 0.3345417 , 0.5787228 ,\n",
       "       0.47168788, 0.28324935, 0.3462383 , 0.2748319 , 0.2713667 ,\n",
       "       0.28202137, 0.2703504 , 0.28044298, 0.28016418, 0.2801284 ,\n",
       "       0.28181112, 0.28016913, 0.61752427, 0.2775309 , 0.28180563,\n",
       "       0.27496853, 0.28485453, 0.28227797, 0.28614202, 0.65643084,\n",
       "       0.35665345, 0.2739946 , 0.29473323, 0.29463634, 0.27810076,\n",
       "       0.27886912, 0.2791075 , 0.28020024, 0.34082562, 0.27527076,\n",
       "       0.27556795, 0.2794043 , 0.705571  , 0.27958986, 0.28856406,\n",
       "       0.2708662 , 0.3477752 , 0.62056667, 0.57600766, 0.28085518,\n",
       "       0.44349426, 0.26823825, 0.27010933, 0.27930492, 0.35033724,\n",
       "       0.27668738, 0.27685472, 0.27869436, 0.29658943, 0.27979988,\n",
       "       0.27568033, 0.28849247, 0.27515152, 0.27703482, 0.35562238,\n",
       "       0.33241108, 0.27384946, 0.27596086, 0.2820586 , 0.27739817,\n",
       "       0.28099778, 0.2746991 , 0.34621674, 0.27981043, 0.28043464,\n",
       "       0.26915985, 0.35690528, 0.27676398, 0.2875195 , 0.26317814,\n",
       "       0.27663472, 0.28248927, 0.27973714, 0.28332058, 0.27748328,\n",
       "       0.28359282, 0.2749822 , 0.2805014 , 0.3203031 , 0.2764128 ,\n",
       "       0.27324194, 0.27174783, 0.27967438, 0.27543697, 0.27640447,\n",
       "       0.28445366, 0.28136456, 0.34390974, 0.27809536, 0.6210888 ,\n",
       "       0.7366495 , 0.27505913, 0.7274195 , 0.26963046, 0.26431912,\n",
       "       0.7639102 , 0.6035509 , 0.33449954, 0.27479288, 0.28413337,\n",
       "       0.6044395 , 0.2836661 , 0.271639  , 0.28817314, 0.35249272,\n",
       "       0.28034192, 0.27253032, 0.28819954, 0.28035703, 0.28218463,\n",
       "       0.27357024, 0.3305833 , 0.28050074, 0.26736113, 0.28186142,\n",
       "       0.2832875 , 0.2731802 , 0.2801102 , 0.348499  , 0.2722117 ,\n",
       "       0.27423537, 0.26816964, 0.27685815, 0.27305943, 0.2670787 ,\n",
       "       0.28250206, 0.34334356, 0.2837448 , 0.28706306, 0.2789289 ,\n",
       "       0.2686742 , 0.26802033, 0.27266034, 0.2741602 , 0.27673855,\n",
       "       0.29555985, 0.27252933, 0.28465652, 0.28712958, 0.26929557,\n",
       "       0.2689393 , 0.27481794, 0.27896327, 0.28270844, 0.2758582 ,\n",
       "       0.27818862, 0.29061395, 0.27558446, 0.2642397 , 0.36432683,\n",
       "       0.37717113, 0.34554425, 0.26822445, 0.28205556, 0.26928613,\n",
       "       0.27979037, 0.3113588 , 0.27211198, 0.2722735 , 0.28120866,\n",
       "       0.29192632, 0.32616997, 0.28573152, 0.44852117, 0.27834848,\n",
       "       0.27531806, 0.27545184, 0.28277004, 0.27442327, 0.2732344 ,\n",
       "       0.26831165, 0.26967162, 0.26778483, 0.2921793 , 0.26735783,\n",
       "       0.27162367, 0.42275298, 0.27025688, 0.2689464 , 0.27662703,\n",
       "       0.28299552, 0.2813687 , 0.2828575 , 0.27478236, 0.26922402,\n",
       "       0.27932853, 0.2730394 , 0.27850378, 0.276111  , 0.28119823,\n",
       "       0.2757607 , 0.32406706, 0.40508094, 0.37362638, 0.2946086 ,\n",
       "       0.274304  , 0.27530116, 0.26805314, 0.271941  , 0.2667317 ,\n",
       "       0.29066023, 0.2749683 , 0.28012002, 0.27801278, 0.27848974,\n",
       "       0.27408642, 0.27235898, 0.8073506 , 0.34392607, 0.27785394,\n",
       "       0.27356902, 0.27414125, 0.2637159 , 0.27578768, 0.27985743,\n",
       "       0.4109153 , 0.27025777, 0.27902064, 0.5864824 , 0.28392354,\n",
       "       0.2899123 , 0.29497072, 0.26584053, 0.28885326, 0.27261594,\n",
       "       0.2800627 , 0.26841068, 0.2717094 , 0.2785824 , 0.28427425,\n",
       "       0.272306  , 0.28201127, 0.3328745 , 0.37622577, 0.27825418,\n",
       "       0.35594538, 0.27343878, 0.27241564, 0.27599093, 0.26946396,\n",
       "       0.34807402, 0.2792175 , 0.2714191 , 0.2737314 , 0.27283135,\n",
       "       0.33956715, 0.28136063, 0.27104715, 0.28089803, 0.41730088,\n",
       "       0.27216643, 0.3318753 , 0.28538784, 0.27428082, 0.28558683,\n",
       "       0.2763286 , 0.2812992 , 0.27660584, 0.2677926 , 0.27663708,\n",
       "       0.28143895, 0.26698735, 0.27307847, 0.26636466, 0.27653718,\n",
       "       0.3309045 , 0.28971434, 0.27028015, 0.28948992, 0.27937508,\n",
       "       0.2739101 , 0.2783965 , 0.28124326, 0.2863886 , 0.27670607,\n",
       "       0.2890827 , 0.2803683 , 0.26705244, 0.26567668, 0.28999016,\n",
       "       0.2765008 , 0.28447562, 0.2777961 , 0.3317108 , 0.2776553 ,\n",
       "       0.26901263, 0.35015368, 0.2791513 , 0.2783415 , 0.275911  ,\n",
       "       0.2699866 , 0.28721493, 0.28076273, 0.2820842 , 0.40360537,\n",
       "       0.27087417, 0.27871317, 0.27255273, 0.39393958, 0.27700055,\n",
       "       0.76397336, 0.3313747 , 0.34458718, 0.28348058, 0.29128703,\n",
       "       0.27038392, 0.26824334, 0.2783663 , 0.269911  , 0.2733    ,\n",
       "       0.27787563, 0.37428534, 0.27467597, 0.2799274 , 0.28879946,\n",
       "       0.27724373, 0.2856544 , 0.6147263 , 0.33973798, 0.49994606,\n",
       "       0.28772718, 0.3816066 , 0.28003532, 0.3831431 , 0.2772121 ,\n",
       "       0.3245527 , 0.29049844, 0.28499928, 0.2848956 , 0.7384518 ,\n",
       "       0.2770719 , 0.28464192, 0.26519856, 0.29785836, 0.4133328 ,\n",
       "       0.278495  , 0.26764408, 0.34274325, 0.393492  , 0.28799716],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a['faultNumber'] = test_a['simulationRun'].apply(lambda x: 1 if x in outliers else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a['faultNumber'] = test_a['simulationRun'].apply(lambda x: 1 if x in outliers else 0)\n",
    "\n",
    "prit = test_a[['faultNumber']] \n",
    "prit.to_csv(\"corr_auto.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faultNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710395</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710396</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710397</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710398</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710399</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710400 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        faultNumber\n",
       "0                 0\n",
       "1                 0\n",
       "2                 0\n",
       "3                 0\n",
       "4                 0\n",
       "...             ...\n",
       "710395            1\n",
       "710396            1\n",
       "710397            1\n",
       "710398            1\n",
       "710399            1\n",
       "\n",
       "[710400 rows x 1 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 빈도가 높은 구간: 0.2744102478027344 ~ 0.28526976704597473\n",
      "이상치 인덱스: [  2   5   7  10  13  17  24  25  27  28  29  30  38  39  40  42  46  56\n",
      "  57  59  62  65  66  68  69  70  72  74  75  77  79  81  83  90  93  94\n",
      "  98 101 103 107 111 114 117 125 126 127 129 131 135 136 139 140 144 145\n",
      " 146 148 149 154 157 160 162 166 167 169 173 174 175 179 185 187 191 193\n",
      " 196 201 207 208 217 228 229 233 234 235 239 241 242 243 244 246 248 251\n",
      " 258 264 265 268 269 274 276 277 278 279 283 289 291 292 294 303 306 312\n",
      " 315 317 319 320 322 324 332 333 338 347 349 359 360 363 378 380 381 382\n",
      " 384 385 387 394 398 399 400 402 412 416 418 419 420 422 423 428 432 434\n",
      " 436 437 438 440 444 448 451 454 455 462 466 468 478 485 487 489 490 492\n",
      " 495 496 497 500 503 504 507 511 518 526 528 535 537 538 546 549 550 551\n",
      " 556 560 561 562 563 573 576 591 592 593 594 600 607 608 615 618 620 621\n",
      " 623 629 632 633 635 640 645 649 651 652 654 665 666 668 673 675 679 683\n",
      " 686 691 694 698 700 701 702 704 711 714 716 717 718 719 720 721 723 725\n",
      " 726 727 728 729 731 733 734 737 738 739]\n",
      "이상치 수: 244\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. 히스토그램 그리기 및 데이터 얻기\n",
    "counts, bins, patches = plt.hist(errors, bins=50)\n",
    "\n",
    "# 2. 가장 빈도가 높은 막대기의 인덱스 찾기\n",
    "max_count_index = np.argmax(counts)\n",
    "\n",
    "# 3. 가장 빈도가 높은 구간 (정상 데이터 구간)\n",
    "max_bin_start = bins[max_count_index]\n",
    "max_bin_end = bins[max_count_index + 1]\n",
    "\n",
    "# 4. 정상 데이터 범위를 제외한 나머지 부분을 이상치로 지정\n",
    "#outliers = np.where((errors < max_bin_start) | (errors > max_bin_end))[0]\n",
    "outliers = np.where((errors > max_bin_end))[0]\n",
    "# 5. 결과 출력\n",
    "print(f\"가장 빈도가 높은 구간: {max_bin_start} ~ {max_bin_end}\")\n",
    "print(f\"이상치 인덱스: {outliers}\")\n",
    "print(f\"이상치 수: {len(outliers)}\")\n",
    "\n",
    "# 6. 히스토그램 및 가장 빈도가 높은 구간 강조\n",
    "plt.axvspan(max_bin_start, max_bin_end, color='y', alpha=0.3, label='Max Frequency Bin')  # 가장 높은 막대기 강조\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a['faultNumber'] = test_a['simulationRun'].apply(lambda x: 1 if x in outliers else 0)\n",
    "\n",
    "prit = test_a[['faultNumber']] \n",
    "prit.to_csv(\"corr_auto2.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
