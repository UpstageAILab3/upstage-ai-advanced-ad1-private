{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "DATA_PATH = Path(\"../data\")\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATA_PATH / \"train.csv\")\n",
    "test_data = pd.read_csv(DATA_PATH / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.060641 0.13718\n",
      "51.564 55.773\n",
      "41.768 45.979\n",
      "20.752 29.855\n",
      "25.951 27.818\n",
      "41.394 43.257\n"
     ]
    }
   ],
   "source": [
    "print(train_data['xmeas_39'].min(),train_data['xmeas_39'].max()) #0.06~0.14\n",
    "print(train_data['xmeas_40'].min(),train_data['xmeas_40'].max()) #51~56\n",
    "print(train_data['xmeas_41'].min(),train_data['xmeas_41'].max()) #41~56\n",
    "print(train_data['xmeas_14'].min(),train_data['xmeas_14'].max()) #20~30\n",
    "print(train_data['xmeas_5'].min(),train_data['xmeas_5'].max()) #25~28\n",
    "print(train_data['xmeas_6'].min(),train_data['xmeas_6'].max()) #41~44\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['faultNumber', 'simulationRun', 'sample', 'xmeas_1', 'xmeas_2',\n",
       "       'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8',\n",
       "       'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14',\n",
       "       'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20',\n",
       "       'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', 'xmeas_26',\n",
       "       'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32',\n",
       "       'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38',\n",
       "       'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4',\n",
       "       'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df) -> pd.DataFrame:\n",
    "    numeric_cols = [\n",
    "       'xmeas_1', 'xmeas_2',\n",
    "       'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8',\n",
    "       'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14',\n",
    "       'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20',\n",
    "       'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', 'xmeas_26',\n",
    "       'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32',\n",
    "       'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38',\n",
    "       'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4',\n",
    "       'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11'\n",
    "    ]\n",
    "    return df[numeric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = process_data(train_data)\n",
    "test_df = process_data(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridAnomalyDetector:\n",
    "    def __init__(self):\n",
    "        self.iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "        self.sgd_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_stats = {}  # 범위 기준 저장\n",
    "\n",
    "    def fit(self, data: pd.DataFrame):\n",
    "        # 1. 상관계수 계산\n",
    "        corr_matrix = data.corr()\n",
    "        high_corr_features = [col for col in data.columns if corr_matrix[col].abs().max() > 0.8]\n",
    "\n",
    "        # 2. 범위 기반 이상 탐지 기준 저장 (평균 ± 3표준편차)\n",
    "        for feature in high_corr_features:\n",
    "            mean = data[feature].mean()\n",
    "            std = data[feature].std()\n",
    "            self.feature_stats[feature] = (mean, std)\n",
    "\n",
    "        # 3. 범위 기반 피처 제거 후 나머지로 모델 학습\n",
    "        data_for_model = data.drop(columns=high_corr_features)\n",
    "        scaled_data = self.scaler.fit_transform(data_for_model)\n",
    "\n",
    "        # IsolationForest와 SGDOneClassSVM 학습\n",
    "        self.iso_forest.fit(scaled_data)\n",
    "        self.sgd_svm.fit(scaled_data)\n",
    "\n",
    "    def predict(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # 1. 범위 기반 예측 수행\n",
    "        range_outliers = pd.DataFrame(index=data.index)\n",
    "        for feature, (mean, std) in self.feature_stats.items():\n",
    "            lower_bound = mean - 3 * std\n",
    "            upper_bound = mean + 3 * std\n",
    "            range_outliers[feature + '_Outlier'] = (~data[feature].between(lower_bound, upper_bound)).astype(int)\n",
    "\n",
    "        # 2. 나머지 피처에 대해 IsolationForest와 SGDOneClassSVM 적용\n",
    "        data_for_model = data.drop(columns=self.feature_stats.keys(), errors='ignore')\n",
    "        scaled_data = self.scaler.transform(data_for_model)\n",
    "\n",
    "        iso_labels = (self.iso_forest.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "        svm_labels = (self.sgd_svm.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "\n",
    "        # 3. 결과 통합 (다수결 방식)\n",
    "        results = pd.DataFrame({\n",
    "            'IsolationForest': iso_labels,\n",
    "            'SGDOneClassSVM': svm_labels\n",
    "        }, index=data.index)\n",
    "\n",
    "        results = pd.concat([results, range_outliers], axis=1)\n",
    "        results['Final_Label'] = (results.mean(axis=1) >= 0.5).astype(int)  # 다수결\n",
    "\n",
    "        return results[['Final_Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class HybridAnomalyDetector:\n",
    "    def __init__(self):\n",
    "        self.iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "        self.sgd_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_stats = {}  # 범위 기준 저장\n",
    "        self.removed_features = []  # 제거된 피처 저장\n",
    "\n",
    "    def fit(self, data: pd.DataFrame):\n",
    "        # 1. 상관계수 계산\n",
    "        corr_matrix = data.corr().abs()  # 상관계수 절대값 사용\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1) > 0)  # 상삼각 행렬 추출\n",
    "\n",
    "        # 2. 상관계수가 0.9 이상인 피처 중 하나를 제거\n",
    "        self.removed_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "        print(f\"제거된 피처: {self.removed_features}\")\n",
    "\n",
    "        # 3. 범위 기반 이상 탐지 기준 저장 (평균 ± 3표준편차)\n",
    "        for feature in self.removed_features:\n",
    "            mean = data[feature].mean()\n",
    "            std = data[feature].std()\n",
    "            self.feature_stats[feature] = (mean, std)\n",
    "\n",
    "        # 4. 제거된 피처를 제외하고 나머지 데이터로 모델 학습\n",
    "        data_for_model = data.drop(columns=self.removed_features, errors='ignore')\n",
    "        scaled_data = self.scaler.fit_transform(data_for_model)\n",
    "\n",
    "        # IsolationForest와 One-Class SVM 학습\n",
    "        self.iso_forest.fit(scaled_data)\n",
    "        self.sgd_svm.fit(scaled_data)\n",
    "\n",
    "    def predict(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # 1. 범위 기반 예측 수행\n",
    "        range_outliers = pd.DataFrame(index=data.index)\n",
    "        for feature, (mean, std) in self.feature_stats.items():\n",
    "            lower_bound = mean - 3 * std\n",
    "            upper_bound = mean + 3 * std\n",
    "            range_outliers[feature + '_Outlier'] = (~data[feature].between(lower_bound, upper_bound)).astype(int)\n",
    "\n",
    "        # 2. 나머지 피처에 대해 IsolationForest와 SGDOneClassSVM 적용\n",
    "        data_for_model = data.drop(columns=self.removed_features, errors='ignore')\n",
    "        scaled_data = self.scaler.transform(data_for_model)\n",
    "\n",
    "        iso_labels = (self.iso_forest.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "        svm_labels = (self.sgd_svm.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "\n",
    "        # 3. 결과 통합 (다수결 방식)\n",
    "        results = pd.DataFrame({\n",
    "            'IsolationForest': iso_labels,\n",
    "            'SGDOneClassSVM': svm_labels\n",
    "        }, index=data.index)\n",
    "\n",
    "        results = pd.concat([results, range_outliers], axis=1)\n",
    "        results['Final_Label'] = (results.mean(axis=1) >= 0.5).astype(int)  # 다수결\n",
    "\n",
    "        return results[['Final_Label']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class HybridAnomalyDetector:\n",
    "    def __init__(self):\n",
    "        self.iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "        self.sgd_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_stats = {}  # 범위 기준 저장\n",
    "        self.removed_features = []  # 제거된 피처 저장\n",
    "\n",
    "    def fit(self, data: pd.DataFrame):\n",
    "        # 1. 상관계수 계산\n",
    "        corr_matrix = data.corr().abs()  # 상관계수 절대값 사용\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1) > 0)  # 상삼각 행렬 추출\n",
    "\n",
    "        # 2. 상관계수가 0.9 이상인 모든 피처 찾기\n",
    "        high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "        # 3. 제거된 피처 저장\n",
    "        self.removed_features = high_corr_features\n",
    "        print(f\"제거된 피처: {self.removed_features}\")\n",
    "\n",
    "        # 4. 범위 기반 기준 저장 (평균 ± 3표준편차)\n",
    "        for feature in self.removed_features:\n",
    "            mean = data[feature].mean()\n",
    "            std = data[feature].std()\n",
    "            self.feature_stats[feature] = (mean, std)\n",
    "\n",
    "        # 5. 제거된 피처를 제외하고 나머지 데이터로 모델 학습\n",
    "        data_for_model = data.drop(columns=self.removed_features, errors='ignore')\n",
    "        scaled_data = self.scaler.fit_transform(data_for_model)\n",
    "\n",
    "        # IsolationForest와 One-Class SVM 학습\n",
    "        self.iso_forest.fit(scaled_data)\n",
    "        self.sgd_svm.fit(scaled_data)\n",
    "\n",
    "    def predict(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # 1. 범위 기반 예측 수행\n",
    "        range_outliers = pd.DataFrame(index=data.index)\n",
    "        for feature, (mean, std) in self.feature_stats.items():\n",
    "            lower_bound = mean - 3 * std\n",
    "            upper_bound = mean + 3 * std\n",
    "            range_outliers[feature + '_Outlier'] = (~data[feature].between(lower_bound, upper_bound)).astype(int)\n",
    "\n",
    "        # 2. 나머지 피처에 대해 IsolationForest와 SGDOneClassSVM 적용\n",
    "        data_for_model = data.drop(columns=self.removed_features, errors='ignore')\n",
    "        scaled_data = self.scaler.transform(data_for_model)\n",
    "\n",
    "        iso_labels = (self.iso_forest.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "        svm_labels = (self.sgd_svm.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "\n",
    "        # 3. 결과 통합 (다수결 방식)\n",
    "        results = pd.DataFrame({\n",
    "            'IsolationForest': iso_labels,\n",
    "            'SGDOneClassSVM': svm_labels\n",
    "        }, index=data.index)\n",
    "\n",
    "        results = pd.concat([results, range_outliers], axis=1)\n",
    "        results['Final_Label'] = (results.mean(axis=1) >= 0.5).astype(int)  # 다수결\n",
    "\n",
    "        return results[['Final_Label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class HybridAnomalyDetector:\n",
    "    def __init__(self):\n",
    "        self.iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "        self.sgd_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_stats = {}  # 범위 기준 저장\n",
    "        self.removed_features = []  # 제거된 피처 목록\n",
    "\n",
    "    def fit(self, data: pd.DataFrame):\n",
    "        # 1. 상관계수 계산\n",
    "        corr_matrix = data.corr().abs()  # 상관계수 절대값 사용\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1) > 0)  # 상삼각 행렬\n",
    "\n",
    "        # 2. 상관계수가 0.9 이상인 모든 피처 찾기\n",
    "        self.removed_features = [col for col in upper_tri.columns if any(upper_tri[col] > 0.9)]\n",
    "        print(f\"제거된 피처: {self.removed_features}\")\n",
    "\n",
    "        # 3. 범위 기반 기준 저장 (제거된 피처만 사용, 평균 ± 3표준편차)\n",
    "        for feature in self.removed_features:\n",
    "            mean = data[feature].mean()\n",
    "            std = data[feature].std()\n",
    "            self.feature_stats[feature] = (mean, std)\n",
    "\n",
    "        # 4. 제거된 피처를 제외한 데이터로 모델 학습\n",
    "        data_for_model = data.drop(columns=self.removed_features, errors='ignore')\n",
    "        scaled_data = self.scaler.fit_transform(data_for_model)\n",
    "\n",
    "        # IsolationForest와 One-Class SVM 학습\n",
    "        self.iso_forest.fit(scaled_data)\n",
    "        self.sgd_svm.fit(scaled_data)\n",
    "\n",
    "    def predict(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # 1. 범위 기반 예측 수행 (제거된 피처만 사용)\n",
    "        range_outliers = pd.DataFrame(index=data.index)\n",
    "        for feature, (mean, std) in self.feature_stats.items():\n",
    "            lower_bound = mean - 3 * std\n",
    "            upper_bound = mean + 3 * std\n",
    "            range_outliers[feature + '_Outlier'] = (~data[feature].between(lower_bound, upper_bound)).astype(int)\n",
    "\n",
    "        # 2. 나머지 피처에 대해 IsolationForest와 SGDOneClassSVM 적용\n",
    "        data_for_model = data.drop(columns=self.removed_features, errors='ignore')\n",
    "        scaled_data = self.scaler.transform(data_for_model)\n",
    "\n",
    "        iso_labels = (self.iso_forest.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "        svm_labels = (self.sgd_svm.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "\n",
    "        # 3. 결과 통합 (다수결 방식)\n",
    "        results = pd.DataFrame({\n",
    "            'IsolationForest': iso_labels,\n",
    "            'SGDOneClassSVM': svm_labels\n",
    "        }, index=data.index)\n",
    "\n",
    "        results = pd.concat([results, range_outliers], axis=1)\n",
    "        results['Final_Label'] = (results.mean(axis=1) >= 0.5).astype(int)  # 다수결\n",
    "\n",
    "        return results[['Final_Label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class HybridAnomalyDetector:\n",
    "    def __init__(self):\n",
    "        self.iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "        self.sgd_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_stats = {}  # 범위 기준 저장\n",
    "        self.removed_features = []  # 상관관계 높은 피처 목록 저장\n",
    "        self.high_corr_groups = []  # 상관관계 높은 피처들 간 그룹 저장\n",
    "        self.pca_models = {}  # 상관관계 높은 피처 그룹별 PCA 모델 저장\n",
    "\n",
    "    def fit(self, data: pd.DataFrame):\n",
    "        # 1. 상관계수 계산\n",
    "        corr_matrix = data.corr().abs()  # 상관계수 절대값 사용\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1) > 0)  # 상삼각 행렬\n",
    "\n",
    "        # 2. 상관계수가 0.9 이상인 모든 피처 그룹 추출\n",
    "        self.removed_features = [col for col in upper_tri.columns if any(upper_tri[col] > 0.9)]\n",
    "        print(f\"제거된 피처: {self.removed_features}\")\n",
    "\n",
    "        # 상관관계가 높은 피처들끼리 그룹핑\n",
    "        for col in self.removed_features:\n",
    "            correlated_features = list(upper_tri.index[upper_tri[col] > 0.9])\n",
    "            if correlated_features:\n",
    "                self.high_corr_groups.append(correlated_features)\n",
    "\n",
    "        print(f\"상관관계 높은 피처 그룹: {self.high_corr_groups}\")\n",
    "\n",
    "        # 3. 각 상관관계 그룹에 대해 범위 기반 기준 저장 (평균 ± 3표준편차)\n",
    "        for group in self.high_corr_groups:\n",
    "            for feature in group:\n",
    "                mean = data[feature].mean()\n",
    "                std = data[feature].std()\n",
    "                self.feature_stats[feature] = (mean, std)\n",
    "\n",
    "            # PCA 모델을 각 그룹에 대해 생성하고, 첫 번째 주성분만 사용할 수 있도록 맞춤\n",
    "            pca = PCA(n_components=1)\n",
    "            pca.fit(data[group])\n",
    "            self.pca_models[tuple(group)] = pca\n",
    "\n",
    "        # 4. 상관관계 높은 피처들을 제외하고 나머지 데이터로 모델 학습\n",
    "        data_for_model = data.drop(columns=self.removed_features, errors='ignore')\n",
    "        scaled_data = self.scaler.fit_transform(data_for_model)\n",
    "\n",
    "        # IsolationForest와 One-Class SVM 학습\n",
    "        self.iso_forest.fit(scaled_data)\n",
    "        self.sgd_svm.fit(scaled_data)\n",
    "\n",
    "    def predict(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # 1. 상관관계 높은 피처 그룹에 대해 범위 기반 이상 탐지 수행\n",
    "        range_outliers = pd.DataFrame(index=data.index)\n",
    "        for feature, (mean, std) in self.feature_stats.items():\n",
    "            lower_bound = mean - 3 * std\n",
    "            upper_bound = mean + 3 * std\n",
    "            range_outliers[feature + '_Outlier'] = (~data[feature].between(lower_bound, upper_bound)).astype(int)\n",
    "\n",
    "        # PCA 기반 이상 탐지 수행\n",
    "        pca_outliers = pd.DataFrame(index=data.index)\n",
    "        for group, pca in self.pca_models.items():\n",
    "            group_data = data[list(group)]\n",
    "            pca_transformed = pca.transform(group_data)\n",
    "            outlier_label = (np.abs(pca_transformed) > 3).astype(int).ravel()  # 3 표준편차 이상이면 이상치로 설정\n",
    "            pca_outliers[str(group) + '_PCA_Outlier'] = outlier_label\n",
    "\n",
    "        # 2. 나머지 피처에 대해 IsolationForest와 One-Class SVM 적용\n",
    "        data_for_model = data.drop(columns=self.removed_features, errors='ignore')\n",
    "        scaled_data = self.scaler.transform(data_for_model)\n",
    "\n",
    "        iso_labels = (self.iso_forest.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "        svm_labels = (self.sgd_svm.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "\n",
    "        # 3. 결과 통합 (다수결 방식)\n",
    "        results = pd.DataFrame({\n",
    "            'IsolationForest': iso_labels,\n",
    "            'SGDOneClassSVM': svm_labels\n",
    "        }, index=data.index)\n",
    "\n",
    "        # 상관관계 기반 이상 탐지 결과 추가\n",
    "        results = pd.concat([results, range_outliers, pca_outliers], axis=1)\n",
    "\n",
    "        # 최종 다수결: 모든 탐지 결과의 평균이 0.5 이상이면 이상치로 판별\n",
    "        results['Final_Label'] = (results.mean(axis=1) >= 0.5).astype(int)\n",
    "\n",
    "        return results[['Final_Label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class HybridAnomalyDetector:\n",
    "    def __init__(self):\n",
    "        self.iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "        self.sgd_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_stats = {}  # 범위 기준 저장\n",
    "        self.removed_features = []  # 상관관계 높은 피처 목록 저장\n",
    "        self.high_corr_groups = []  # 상관관계 높은 피처들 간 그룹 저장\n",
    "        self.low_corr_features = []  # 상관관계가 낮은 피처 저장 (상관계수 < 0.2)\n",
    "        self.pca_models = {}  # 상관관계 높은 피처 그룹별 PCA 모델 저장\n",
    "\n",
    "    def fit(self, data: pd.DataFrame):\n",
    "        # 1. 상관계수 계산\n",
    "        corr_matrix = data.corr().abs()  # 상관계수 절대값 사용\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1) > 0)  # 상삼각 행렬\n",
    "\n",
    "        # 2. 상관계수가 0.9 이상인 모든 피처 그룹 추출\n",
    "        self.removed_features = [col for col in upper_tri.columns if any(upper_tri[col] > 0.9)]\n",
    "        print(f\"제거된 피처: {self.removed_features}\")\n",
    "\n",
    "        # 상관관계가 높은 피처들끼리 그룹핑\n",
    "        for col in self.removed_features:\n",
    "            correlated_features = list(upper_tri.index[upper_tri[col] > 0.9])\n",
    "            if correlated_features:\n",
    "                self.high_corr_groups.append(correlated_features)\n",
    "\n",
    "        print(f\"상관관계 높은 피처 그룹: {self.high_corr_groups}\")\n",
    "\n",
    "        # 3. 상관계수가 0.2 이하인 피처 목록 추출\n",
    "        for col in data.columns:\n",
    "            if all(upper_tri[col] < 0.2):  # 상관계수 0.2 미만인 피처들\n",
    "                self.low_corr_features.append(col)\n",
    "\n",
    "        print(f\"상관관계가 낮은 피처: {self.low_corr_features}\")\n",
    "\n",
    "        # 4. 상관관계가 낮은 피처들에 대해 범위 기준 저장 (평균 ± 3표준편차)\n",
    "        for feature in self.low_corr_features:\n",
    "            mean = data[feature].mean()\n",
    "            std = data[feature].std()\n",
    "            self.feature_stats[feature] = (mean, std)\n",
    "\n",
    "        # 5. 각 상관관계 그룹에 대해 PCA 적용 및 모델 저장\n",
    "        for group in self.high_corr_groups:\n",
    "            pca = PCA(n_components=1)\n",
    "            pca.fit(data[group])\n",
    "            self.pca_models[tuple(group)] = pca\n",
    "\n",
    "        # 6. 상관관계 높은 피처들과 상관관계 낮은 피처들을 제외하고 나머지 데이터로 IsolationForest와 SVM 학습\n",
    "        features_to_remove = self.removed_features + self.low_corr_features\n",
    "        data_for_model = data.drop(columns=features_to_remove, errors='ignore')\n",
    "        scaled_data = self.scaler.fit_transform(data_for_model)\n",
    "\n",
    "        # IsolationForest와 One-Class SVM 학습\n",
    "        self.iso_forest.fit(scaled_data)\n",
    "        self.sgd_svm.fit(scaled_data)\n",
    "\n",
    "    def predict(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # 1. 상관관계 높은 피처 그룹에 대해 PCA 기반 이상 탐지 수행\n",
    "        pca_outliers = pd.DataFrame(index=data.index)\n",
    "        for group, pca in self.pca_models.items():\n",
    "            group_data = data[list(group)]\n",
    "            pca_transformed = pca.transform(group_data)\n",
    "            outlier_label = (np.abs(pca_transformed) > 3).astype(int).ravel()  # 3 표준편차 이상이면 이상치로 설정\n",
    "            pca_outliers[str(group) + '_PCA_Outlier'] = outlier_label\n",
    "\n",
    "        # 2. 상관관계가 낮은 피처들에 대해 범위 기반 이상 탐지 수행\n",
    "        range_outliers = pd.DataFrame(index=data.index)\n",
    "        for feature, (mean, std) in self.feature_stats.items():\n",
    "            lower_bound = mean - 3 * std\n",
    "            upper_bound = mean + 3 * std\n",
    "            range_outliers[feature + '_Outlier'] = (~data[feature].between(lower_bound, upper_bound)).astype(int)\n",
    "\n",
    "        # 3. 상관관계가 높은 피처들과 상관관계가 낮은 피처를 제외한 나머지 데이터에 대해 IsolationForest와 One-Class SVM 적용\n",
    "        features_to_remove = self.removed_features + self.low_corr_features\n",
    "        data_for_model = data.drop(columns=features_to_remove, errors='ignore')\n",
    "        scaled_data = self.scaler.transform(scaled_data)\n",
    "\n",
    "        iso_labels = (self.iso_forest.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "        svm_labels = (self.sgd_svm.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "\n",
    "        # 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class HybridAnomalyDetector:\n",
    "    def __init__(self):\n",
    "        self.iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "        self.sgd_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_stats = {}  # 범위 기준 저장\n",
    "        self.removed_features = []  # 상관관계 높은 피처 목록 저장\n",
    "        self.high_corr_groups = []  # 상관관계 높은 피처들 간 그룹 저장 (상관계수 > 0.9)\n",
    "        self.low_corr_features = []  # 상관관계가 낮은 피처 저장 (상관계수 < 0.2)\n",
    "        self.pca_models = {}  # 상관관계 높은 피처 그룹별 PCA 모델 저장\n",
    "\n",
    "    def fit(self, data: pd.DataFrame):\n",
    "        # 1. 상관계수 계산\n",
    "        corr_matrix = data.corr().abs()  # 상관계수 절대값 사용\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1) > 0)  # 상삼각 행렬\n",
    "\n",
    "        # 2. 상관계수가 0.9 이상인 모든 피처 그룹 추출\n",
    "        self.removed_features = [col for col in upper_tri.columns if any(upper_tri[col] > 0.9)]\n",
    "        print(f\"제거된 피처: {self.removed_features}\")\n",
    "\n",
    "        # 상관관계가 높은 피처들끼리 그룹핑\n",
    "        for col in self.removed_features:\n",
    "            correlated_features = list(upper_tri.index[upper_tri[col] > 0.9])\n",
    "            if correlated_features:\n",
    "                self.high_corr_groups.append(correlated_features)\n",
    "\n",
    "        print(f\"상관관계 높은 피처 그룹: {self.high_corr_groups}\")\n",
    "\n",
    "        # 3. 상관계수가 0.2 이하인 피처 목록 추출\n",
    "        # 모든 피처를 탐색하면서 다른 피처들과의 상관계수가 0.2 이하인 경우에만 추가\n",
    "        for col in data.columns:\n",
    "            # col 자신을 제외한 다른 피처들과의 상관관계가 모두 0.2 미만인지 확인\n",
    "            other_corrs = corr_matrix[col].drop(col)  # 자신을 제외\n",
    "            if all(other_corrs < 0.2):  # 모든 다른 피처와의 상관계수가 0.2 미만일 때만 추가\n",
    "                self.low_corr_features.append(col)\n",
    "\n",
    "        print(f\"상관관계가 낮은 피처: {self.low_corr_features}\")\n",
    "\n",
    "        # 4. 상관관계가 낮은 피처들에 대해 범위 기준 저장 (평균 ± 3표준편차)\n",
    "        for feature in self.low_corr_features:\n",
    "            mean = data[feature].mean()\n",
    "            std = data[feature].std()\n",
    "            self.feature_stats[feature] = (mean, std)\n",
    "\n",
    "        # 5. 각 상관관계 그룹에 대해 PCA 적용 및 모델 저장\n",
    "        for group in self.high_corr_groups:\n",
    "            pca = PCA(n_components=1)\n",
    "            pca.fit(data[group])\n",
    "            self.pca_models[tuple(group)] = pca\n",
    "\n",
    "        # 6. 상관관계 높은 피처들과 상관관계 낮은 피처들을 제외하고 나머지 데이터로 IsolationForest와 SVM 학습\n",
    "        features_to_remove = self.removed_features + self.low_corr_features\n",
    "        data_for_model = data.drop(columns=features_to_remove, errors='ignore')\n",
    "        scaled_data = self.scaler.fit_transform(data_for_model)\n",
    "\n",
    "        # IsolationForest와 One-Class SVM 학습\n",
    "        self.iso_forest.fit(scaled_data)\n",
    "        self.sgd_svm.fit(scaled_data)\n",
    "\n",
    "    def predict(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # 1. 상관관계 높은 피처 그룹에 대해 PCA 기반 이상 탐지 수행\n",
    "        pca_outliers = pd.DataFrame(index=data.index)\n",
    "        for group, pca in self.pca_models.items():\n",
    "            group_data = data[list(group)]\n",
    "            pca_transformed = pca.transform(group_data)\n",
    "            outlier_label = (np.abs(pca_transformed) > 3).astype(int).ravel()  # 3 표준편차 이상이면 이상치로 설정\n",
    "            pca_outliers[str(group) + '_PCA_Outlier'] = outlier_label\n",
    "\n",
    "        # 2. 상관관계가 낮은 피처들에 대해 범위 기반 이상 탐지 수행\n",
    "        range_outliers = pd.DataFrame(index=data.index)\n",
    "        for feature, (mean, std) in self.feature_stats.items():\n",
    "            lower_bound = mean - 3 * std\n",
    "            upper_bound = mean + 3 * std\n",
    "            range_outliers[feature + '_Outlier'] = (~data[feature].between(lower_bound, upper_bound)).astype(int)\n",
    "\n",
    "        # 3. 상관관계가 높은 피처들과 상관관계가 낮은 피처를 제외한 나머지 데이터에 대해 IsolationForest와 One-Class SVM 적용\n",
    "        features_to_remove = self.removed_features + self.low_corr_features\n",
    "        data_for_model = data.drop(columns=features_to_remove, errors='ignore')\n",
    "        scaled_data = self.scaler.transform(data_for_model)\n",
    "\n",
    "        iso_labels = (self.iso_forest.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "        svm_labels = (self.sgd_svm.predict(scaled_data) == -1).astype(int)  # 1: 이상, 0: 정상\n",
    "\n",
    "        # 4. 결과 통합 (다수결 방식)\n",
    "        results = pd.DataFrame({\n",
    "            'IsolationForest': iso_labels,\n",
    "            'SGDOneClassSVM': svm_labels\n",
    "        }, index=data.index)\n",
    "\n",
    "        # 상관관계 기반 이상 탐지 결과 추가\n",
    "        results = pd.concat([results, pca_outliers, range_outliers], axis=1)\n",
    "\n",
    "        # 최종 다수결: 모든 탐지 결과의 평균이 0.5 이상이면 이상치로 판별\n",
    "        results['faultNumber'] = (results.mean(axis=1) >= 0.5).astype(int)\n",
    "\n",
    "        return results[['faultNumber']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제거된 피처: ['xmeas_13', 'xmeas_16', 'xmeas_19', 'xmv_3', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_11']\n",
      "상관관계 높은 피처 그룹: [['xmeas_7'], ['xmeas_7', 'xmeas_13'], ['xmeas_18'], ['xmeas_1'], ['xmeas_10'], ['xmeas_12'], ['xmeas_15'], ['xmeas_18', 'xmeas_19'], ['xmeas_17']]\n",
      "상관관계가 낮은 피처: ['xmeas_5', 'xmeas_6', 'xmeas_14', 'xmeas_24', 'xmeas_26', 'xmeas_28', 'xmeas_32', 'xmeas_37', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_4']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 학습 데이터 준비\n",
    "    np.random.seed(42)\n",
    "    train_data = train_df\n",
    "\n",
    "    # 모델 생성 및 학습\n",
    "    model = HybridAnomalyDetector()\n",
    "    model.fit(train_data)\n",
    "\n",
    "    # 새로운 데이터에 대한 예측\n",
    "    new_data = test_df\n",
    "    predictions = model.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710395</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710396</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710397</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710398</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710399</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710400 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Final_Label\n",
       "0                 0\n",
       "1                 0\n",
       "2                 0\n",
       "3                 0\n",
       "4                 0\n",
       "...             ...\n",
       "710395            0\n",
       "710396            0\n",
       "710397            0\n",
       "710398            0\n",
       "710399            0\n",
       "\n",
       "[710400 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('multi_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
